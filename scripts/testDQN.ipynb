{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e583960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T09:18:22.868004Z",
     "start_time": "2023-11-20T09:18:22.864275Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/tmp-data/zhx/DriverOrderOfflineRL/cage-challenge-1/CybORG')\n",
    "sys.path.append('/tmp-data/zhx/DriverOrderOfflineRL/tianshou')\n",
    "sys.path.append('/tmp-data/zhx/DriverOrderOfflineRL/tianshou/examples/atari')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34c271ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T09:18:23.378946Z",
     "start_time": "2023-11-20T09:18:23.340741Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import deque\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import envpool\n",
    "except ImportError:\n",
    "    envpool = None\n",
    "\n",
    "from tianshou.env import ShmemVectorEnv\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "    No-op is assumed to be action 0.\n",
    "\n",
    "    :param gym.Env env: the environment to wrap.\n",
    "    :param int noop_max: the maximum value of no-ops to run.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        super().__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        noops = np.random.default_rng().randint(1, self.noop_max + 1)\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"Return only every `skip`-th frame (frameskipping) using most recent raw\n",
    "    observations (for max pooling across time steps)\n",
    "\n",
    "    :param gym.Env env: the environment to wrap.\n",
    "    :param int skip: number of `skip`-th frame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Step the environment with the given action. Repeat action, sum\n",
    "        reward, and max over last observations.\n",
    "        \"\"\"\n",
    "        obs_list, total_reward, done = [], 0., False\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            obs_list.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(obs_list[-2:], axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    \"\"\"Make end-of-life == end-of-episode, but only reset on true game over. It\n",
    "    helps the value estimation.\n",
    "\n",
    "    :param gym.Env env: the environment to wrap.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal, then update lives to\n",
    "        # handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if 0 < lives < self.lives:\n",
    "            # for Qbert sometimes we stay in lives == 0 condition for a few\n",
    "            # frames, so its important to keep lives > 0, so that we only reset\n",
    "            # once the environment is actually done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Calls the Gym environment reset, only when lives are exhausted. This\n",
    "        way all states are still reachable even though lives are episodic, and\n",
    "        the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset()\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs = self.env.step(0)[0]\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    \"\"\"Take action on reset for environments that are fixed until firing.\n",
    "    Related discussion: https://github.com/openai/baselines/issues/240\n",
    "\n",
    "    :param gym.Env env: the environment to wrap.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        return self.env.step(1)[0]\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\n",
    "\n",
    "    :param gym.Env env: the environment to wrap.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.size = 84\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.min(env.observation_space.low),\n",
    "            high=np.max(env.observation_space.high),\n",
    "            shape=(self.size, self.size),\n",
    "            dtype=env.observation_space.dtype\n",
    "        )\n",
    "\n",
    "    def observation(self, frame):\n",
    "        \"\"\"returns the current observation from a frame\"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        return cv2.resize(frame, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    \"\"\"Normalize observations to 0~1.\n",
    "\n",
    "    :param gym.Env env: the environment to wrap.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        low = np.min(env.observation_space.low)\n",
    "        high = np.max(env.observation_space.high)\n",
    "        self.bias = low\n",
    "        self.scale = high - low\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0., high=1., shape=env.observation_space.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return (observation - self.bias) / self.scale\n",
    "\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    \"\"\"clips the reward to {+1, 0, -1} by its sign.\n",
    "\n",
    "    :param gym.Env env: the environment to wrap.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.reward_range = (-1, 1)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign. Note: np.sign(0) == 0.\"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    \"\"\"Stack n_frames last frames.\n",
    "\n",
    "    :param gym.Env env: the environment to wrap.\n",
    "    :param int n_frames: the number of frames to stack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, n_frames):\n",
    "        super().__init__(env)\n",
    "        self.n_frames = n_frames\n",
    "        self.frames = deque([], maxlen=n_frames)\n",
    "        shape = (n_frames, ) + env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.min(env.observation_space.low),\n",
    "            high=np.max(env.observation_space.high),\n",
    "            shape=shape,\n",
    "            dtype=env.observation_space.dtype\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        for _ in range(self.n_frames):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        # the original wrapper use `LazyFrames` but since we use np buffer,\n",
    "        # it has no effect\n",
    "        return np.stack(self.frames, axis=0)\n",
    "\n",
    "\n",
    "def wrap_deepmind(\n",
    "    env_id,\n",
    "    episode_life=True,\n",
    "    clip_rewards=True,\n",
    "    frame_stack=4,\n",
    "    scale=False,\n",
    "    warp_frame=True\n",
    "):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari. The observation is\n",
    "    channel-first: (c, h, w) instead of (h, w, c).\n",
    "\n",
    "    :param str env_id: the atari environment id.\n",
    "    :param bool episode_life: wrap the episode life wrapper.\n",
    "    :param bool clip_rewards: wrap the reward clipping wrapper.\n",
    "    :param int frame_stack: wrap the frame stacking wrapper.\n",
    "    :param bool scale: wrap the scaling observation wrapper.\n",
    "    :param bool warp_frame: wrap the grayscale + resize observation wrapper.\n",
    "    :return: the wrapped atari environment.\n",
    "    \"\"\"\n",
    "    assert 'NoFrameskip' in env_id\n",
    "    env = gym.make(env_id)\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    if episode_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    if warp_frame:\n",
    "        env = WarpFrame(env)\n",
    "    if scale:\n",
    "        env = ScaledFloatFrame(env)\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "    if frame_stack:\n",
    "        env = FrameStack(env, frame_stack)\n",
    "    return env\n",
    "\n",
    "\n",
    "def make_atari_env(task, seed, training_num, test_num, **kwargs):\n",
    "    \"\"\"Wrapper function for Atari env.\n",
    "\n",
    "    If EnvPool is installed, it will automatically switch to EnvPool's Atari env.\n",
    "\n",
    "    :return: a tuple of (single env, training envs, test envs).\n",
    "    \"\"\"\n",
    "    if envpool is not None:\n",
    "        if kwargs.get(\"scale\", 0):\n",
    "            warnings.warn(\n",
    "                \"EnvPool does not include ScaledFloatFrame wrapper, \"\n",
    "                \"please set `x = x / 255.0` inside CNN network's forward function.\"\n",
    "            )\n",
    "        # parameters convertion\n",
    "        train_envs = env = envpool.make_gym(\n",
    "            task.replace(\"NoFrameskip-v4\", \"-v5\"),\n",
    "            num_envs=training_num,\n",
    "            seed=seed,\n",
    "            episodic_life=True,\n",
    "            reward_clip=True,\n",
    "            stack_num=kwargs.get(\"frame_stack\", 4),\n",
    "        )\n",
    "        test_envs = envpool.make_gym(\n",
    "            task.replace(\"NoFrameskip-v4\", \"-v5\"),\n",
    "            num_envs=training_num,\n",
    "            seed=seed,\n",
    "            episodic_life=False,\n",
    "            reward_clip=False,\n",
    "            stack_num=kwargs.get(\"frame_stack\", 4),\n",
    "        )\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            \"Recommend using envpool (pip install envpool) \"\n",
    "            \"to run Atari games more efficiently.\"\n",
    "        )\n",
    "        env = wrap_deepmind(task, **kwargs)\n",
    "        train_envs = ShmemVectorEnv(\n",
    "            [\n",
    "                lambda:\n",
    "                wrap_deepmind(task, episode_life=True, clip_rewards=True, **kwargs)\n",
    "                for _ in range(training_num)\n",
    "            ]\n",
    "        )\n",
    "        test_envs = ShmemVectorEnv(\n",
    "            [\n",
    "                lambda:\n",
    "                wrap_deepmind(task, episode_life=False, clip_rewards=False, **kwargs)\n",
    "                for _ in range(test_num)\n",
    "            ]\n",
    "        )\n",
    "        env.seed(seed)\n",
    "        train_envs.seed(seed)\n",
    "        test_envs.seed(seed)\n",
    "    return env, train_envs, test_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13141b5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T09:18:25.251017Z",
     "start_time": "2023-11-20T09:18:25.211428Z"
    },
    "code_folding": [
     18,
     81
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from atari_network import DQN\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.policy import DQNPolicy\n",
    "from tianshou.policy.modelbased.icm import ICMPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger, WandbLogger\n",
    "from tianshou.utils.net.discrete import IntrinsicCuriosityModule\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--task\", type=str, default=\"PongNoFrameskip-v4\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    parser.add_argument(\"--scale-obs\", type=int, default=0)\n",
    "    parser.add_argument(\"--eps-test\", type=float, default=0.005)\n",
    "    parser.add_argument(\"--eps-train\", type=float, default=1.)\n",
    "    parser.add_argument(\"--eps-train-final\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--buffer-size\", type=int, default=100000)\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0001)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--n-step\", type=int, default=3)\n",
    "    parser.add_argument(\"--target-update-freq\", type=int, default=500)\n",
    "    parser.add_argument(\"--epoch\", type=int, default=100)\n",
    "    parser.add_argument(\"--step-per-epoch\", type=int, default=100000)\n",
    "    parser.add_argument(\"--step-per-collect\", type=int, default=10)\n",
    "    parser.add_argument(\"--update-per-step\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=32)\n",
    "    parser.add_argument(\"--training-num\", type=int, default=10)\n",
    "    parser.add_argument(\"--test-num\", type=int, default=10)\n",
    "    parser.add_argument(\"--logdir\", type=str, default=\"log\")\n",
    "    parser.add_argument(\"--render\", type=float, default=0.)\n",
    "    parser.add_argument(\n",
    "        \"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    parser.add_argument(\"--frames-stack\", type=int, default=4)\n",
    "    parser.add_argument(\"--resume-path\", type=str, default=None)\n",
    "    parser.add_argument(\"--resume-id\", type=str, default=None)\n",
    "    parser.add_argument(\n",
    "        \"--logger\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        choices=[\"tensorboard\", \"wandb\"],\n",
    "    )\n",
    "    parser.add_argument(\"--wandb-project\", type=str, default=\"atari.benchmark\")\n",
    "    parser.add_argument(\n",
    "        \"--watch\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"watch the play of pre-trained policy only\"\n",
    "    )\n",
    "    parser.add_argument(\"--save-buffer-name\", type=str, default=None)\n",
    "    parser.add_argument(\n",
    "        \"--icm-lr-scale\",\n",
    "        type=float,\n",
    "        default=0.,\n",
    "        help=\"use intrinsic curiosity module with this lr scale\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--icm-reward-scale\",\n",
    "        type=float,\n",
    "        default=0.01,\n",
    "        help=\"scaling factor for intrinsic curiosity reward\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--icm-forward-loss-weight\",\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help=\"weight for the forward model loss in ICM\"\n",
    "    )\n",
    "    return parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "def test_dqn(args=get_args()):\n",
    "    env, train_envs, test_envs = make_atari_env(\n",
    "        args.task,\n",
    "        args.seed,\n",
    "        args.training_num,\n",
    "        args.test_num,\n",
    "        scale=args.scale_obs,\n",
    "        frame_stack=args.frames_stack,\n",
    "    )\n",
    "    args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "    # should be N_FRAMES x H x W\n",
    "    print(\"Observations shape:\", args.state_shape)\n",
    "    print(\"Actions shape:\", args.action_shape)\n",
    "    # seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    # define model\n",
    "    net = DQN(*args.state_shape, args.action_shape, args.device).to(args.device)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "    # define policy\n",
    "    policy = DQNPolicy(\n",
    "        net,\n",
    "        optim,\n",
    "        args.gamma,\n",
    "        args.n_step,\n",
    "        target_update_freq=args.target_update_freq\n",
    "    )\n",
    "    if args.icm_lr_scale > 0:\n",
    "        feature_net = DQN(\n",
    "            *args.state_shape, args.action_shape, args.device, features_only=True\n",
    "        )\n",
    "        action_dim = np.prod(args.action_shape)\n",
    "        feature_dim = feature_net.output_dim\n",
    "        icm_net = IntrinsicCuriosityModule(\n",
    "            feature_net.net,\n",
    "            feature_dim,\n",
    "            action_dim,\n",
    "            hidden_sizes=[512],\n",
    "            device=args.device\n",
    "        )\n",
    "        icm_optim = torch.optim.Adam(icm_net.parameters(), lr=args.lr)\n",
    "        policy = ICMPolicy(\n",
    "            policy, icm_net, icm_optim, args.icm_lr_scale, args.icm_reward_scale,\n",
    "            args.icm_forward_loss_weight\n",
    "        ).to(args.device)\n",
    "    # load a previous policy\n",
    "    if args.resume_path:\n",
    "        policy.load_state_dict(torch.load(args.resume_path, map_location=args.device))\n",
    "        print(\"Loaded agent from: \", args.resume_path)\n",
    "    # replay buffer: `save_last_obs` and `stack_num` can be removed together\n",
    "    # when you have enough RAM\n",
    "    buffer = VectorReplayBuffer(\n",
    "        args.buffer_size,\n",
    "        buffer_num=len(train_envs),\n",
    "        ignore_obs_next=True,\n",
    "        save_only_last_obs=True,\n",
    "        stack_num=args.frames_stack\n",
    "    )\n",
    "    # collector\n",
    "    train_collector = Collector(policy, train_envs, buffer, exploration_noise=True)\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "\n",
    "    # log\n",
    "    now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "    args.algo_name = \"dqn_icm\" if args.icm_lr_scale > 0 else \"dqn\"\n",
    "    log_name = os.path.join(args.task, args.algo_name, str(args.seed), now)\n",
    "    log_path = os.path.join(args.logdir, log_name)\n",
    "\n",
    "    # logger\n",
    "    if args.logger == \"wandb\":\n",
    "        logger = WandbLogger(\n",
    "            save_interval=1,\n",
    "            name=log_name.replace(os.path.sep, \"__\"),\n",
    "            run_id=args.resume_id,\n",
    "            config=args,\n",
    "            project=args.wandb_project,\n",
    "        )\n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    if args.logger == \"tensorboard\":\n",
    "        logger = TensorboardLogger(writer)\n",
    "    else:  # wandb\n",
    "        logger.load(writer)\n",
    "\n",
    "    def save_best_fn(policy):\n",
    "        torch.save(policy.state_dict(), os.path.join(log_path, \"policy.pth\"))\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        if env.spec.reward_threshold:\n",
    "            return mean_rewards >= env.spec.reward_threshold\n",
    "        elif \"Pong\" in args.task:\n",
    "            return mean_rewards >= 20\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        # nature DQN setting, linear decay in the first 1M steps\n",
    "        if env_step <= 1e6:\n",
    "            eps = args.eps_train - env_step / 1e6 * \\\n",
    "                (args.eps_train - args.eps_train_final)\n",
    "        else:\n",
    "            eps = args.eps_train_final\n",
    "        policy.set_eps(eps)\n",
    "        if env_step % 1000 == 0:\n",
    "            logger.write(\"train/env_step\", env_step, {\"train/eps\": eps})\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        policy.set_eps(args.eps_test)\n",
    "\n",
    "    def save_checkpoint_fn(epoch, env_step, gradient_step):\n",
    "        # see also: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        ckpt_path = os.path.join(log_path, \"checkpoint.pth\")\n",
    "        torch.save({\"model\": policy.state_dict()}, ckpt_path)\n",
    "        return ckpt_path\n",
    "\n",
    "    # watch agent's performance\n",
    "    def watch():\n",
    "        print(\"Setup test envs ...\")\n",
    "        policy.eval()\n",
    "        policy.set_eps(args.eps_test)\n",
    "        test_envs.seed(args.seed)\n",
    "        if args.save_buffer_name:\n",
    "            print(f\"Generate buffer with size {args.buffer_size}\")\n",
    "            buffer = VectorReplayBuffer(\n",
    "                args.buffer_size,\n",
    "                buffer_num=len(test_envs),\n",
    "                ignore_obs_next=True,\n",
    "                save_only_last_obs=True,\n",
    "                stack_num=args.frames_stack\n",
    "            )\n",
    "            collector = Collector(policy, test_envs, buffer, exploration_noise=True)\n",
    "            result = collector.collect(n_step=args.buffer_size)\n",
    "            print(f\"Save buffer into {args.save_buffer_name}\")\n",
    "            # Unfortunately, pickle will cause oom with 1M buffer size\n",
    "            buffer.save_hdf5(args.save_buffer_name)\n",
    "        else:\n",
    "            print(\"Testing agent ...\")\n",
    "            test_collector.reset()\n",
    "            result = test_collector.collect(\n",
    "                n_episode=args.test_num, render=args.render\n",
    "            )\n",
    "        rew = result[\"rews\"].mean()\n",
    "        print(f\"Mean reward (over {result['n/ep']} episodes): {rew}\")\n",
    "\n",
    "    if args.watch:\n",
    "        watch()\n",
    "        exit(0)\n",
    "\n",
    "    # test train_collector and start filling replay buffer\n",
    "    train_collector.collect(n_step=args.batch_size * args.training_num)\n",
    "    # trainer\n",
    "    result = offpolicy_trainer(\n",
    "        policy,\n",
    "        train_collector,\n",
    "        test_collector,\n",
    "        args.epoch,\n",
    "        args.step_per_epoch,\n",
    "        args.step_per_collect,\n",
    "        args.test_num,\n",
    "        args.batch_size,\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        logger=logger,\n",
    "        update_per_step=args.update_per_step,\n",
    "        test_in_train=False,\n",
    "        resume_from_log=args.resume_id is not None,\n",
    "        save_checkpoint_fn=save_checkpoint_fn,\n",
    "    )\n",
    "\n",
    "    pprint.pprint(result)\n",
    "    watch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38014d40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T09:18:28.158824Z",
     "start_time": "2023-11-20T09:18:26.394584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations shape: (4, 84, 84)\n",
      "Actions shape: 6\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Batch does not support heterogeneous list/tuple of tensors as unique value yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/tmp-data/zhx/DriverOrderOfflineRL/tianshou/tianshou/data/batch.py:150\u001b[0m, in \u001b[0;36m_parse_value\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43m_to_array_with_correct_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m/tmp-data/zhx/DriverOrderOfflineRL/tianshou/tianshou/data/batch.py:59\u001b[0m, in \u001b[0;36m_to_array_with_correct_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# convert the value to np.ndarray\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# convert to object obj type if neither bool nor number\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# raises an exception if array's elements are tensors themselves\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m obj_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(obj_array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (np\u001b[38;5;241m.\u001b[39mbool_, np\u001b[38;5;241m.\u001b[39mnumber)):\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 142\u001b[0m, in \u001b[0;36mtest_dqn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    134\u001b[0m buffer \u001b[38;5;241m=\u001b[39m VectorReplayBuffer(\n\u001b[1;32m    135\u001b[0m     args\u001b[38;5;241m.\u001b[39mbuffer_size,\n\u001b[1;32m    136\u001b[0m     buffer_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_envs),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     stack_num\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mframes_stack\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# collector\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m train_collector \u001b[38;5;241m=\u001b[39m \u001b[43mCollector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexploration_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m test_collector \u001b[38;5;241m=\u001b[39m Collector(policy, test_envs, exploration_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# log\u001b[39;00m\n",
      "File \u001b[0;32m/tmp-data/zhx/DriverOrderOfflineRL/tianshou/tianshou/data/collector.py:77\u001b[0m, in \u001b[0;36mCollector.__init__\u001b[0;34m(self, policy, env, buffer, preprocess_fn, exploration_noise)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# avoid creating attribute outside __init__\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp-data/zhx/DriverOrderOfflineRL/tianshou/tianshou/data/collector.py:114\u001b[0m, in \u001b[0;36mCollector.reset\u001b[0;34m(self, reset_buffer)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# use empty Batch for \"state\" so that self.data supports slicing\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# convert empty Batch to None when passing data to policy\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m Batch(\n\u001b[1;32m    112\u001b[0m     obs\u001b[38;5;241m=\u001b[39m{}, act\u001b[38;5;241m=\u001b[39m{}, rew\u001b[38;5;241m=\u001b[39m{}, done\u001b[38;5;241m=\u001b[39m{}, obs_next\u001b[38;5;241m=\u001b[39m{}, info\u001b[38;5;241m=\u001b[39m{}, policy\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m    113\u001b[0m )\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_buffer:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_buffer()\n",
      "File \u001b[0;32m/tmp-data/zhx/DriverOrderOfflineRL/tianshou/tianshou/data/collector.py:133\u001b[0m, in \u001b[0;36mCollector.reset_env\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_fn:\n\u001b[1;32m    131\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_fn(obs\u001b[38;5;241m=\u001b[39mobs,\n\u001b[1;32m    132\u001b[0m                              env_id\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_num))\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, obs)\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m/tmp-data/zhx/DriverOrderOfflineRL/tianshou/tianshou/data/batch.py:204\u001b[0m, in \u001b[0;36mBatch.__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, value: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set self.key = value.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[key] \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp-data/zhx/DriverOrderOfflineRL/tianshou/tianshou/data/batch.py:152\u001b[0m, in \u001b[0;36m_parse_value\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    150\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _to_array_with_correct_type(obj)\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch does not support heterogeneous list/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuple of tensors as unique value yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexception\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mTypeError\u001b[0m: Batch does not support heterogeneous list/tuple of tensors as unique value yet."
     ]
    }
   ],
   "source": [
    "test_dqn(get_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef0234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_test",
   "language": "python",
   "name": "rl_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
