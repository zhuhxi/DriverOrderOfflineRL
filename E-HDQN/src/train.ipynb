{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd021d2",
   "metadata": {},
   "source": [
    "## model DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74364934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T08:48:36.216692Z",
     "start_time": "2023-12-04T08:48:35.319683Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from rl.model import DDQN_Model, ICM_Model\n",
    "from rl.memory import Memory\n",
    "from rl.per import PERMemory\n",
    "from torch.nn.functional import mse_loss, cross_entropy, smooth_l1_loss, softmax\n",
    "import settings as sett\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_dim, tau, action_dim, gamma, hidd_ch, lam, lr,\n",
    "                 eps_sub, eps_sub_decay, beta, bs, target_interval, train_steps, max_memory,\n",
    "                 conv, reward_rescale, n_proc, per=False, norm_input=True, logger=None):\n",
    "        \"\"\"\n",
    "        :param state_dim: Shape of the state\n",
    "        :param float tau: Weight for agent loss\n",
    "        :param int action_dim: Number of actions\n",
    "        :param float gamma: Discount for sub controller\n",
    "        :param int hidd_ch: Number of hidden channels\n",
    "        :param float lam: Scaler for ICM reward\n",
    "        :param float lr: Learning rate\n",
    "        :param float eps_sub: Eps greedy change for sub policies\n",
    "        :param float eps_sub_decay: Epsilon decay for sub policy computed as eps * (1 - eps_decay) each step\n",
    "        :param float beta: Weight for loss of fwd net vs inv net\n",
    "        :param int bs: Batch size\n",
    "        :param int target_interval: Number of train steps between target updates\n",
    "        :param int train_steps: Number of training iterations for each call\n",
    "        :param int max_memory: Max memory\n",
    "        :param bool conv: Use or not convolutional networks\n",
    "        :param bool per: Use or not prioritized experience replay\n",
    "        \"\"\"\n",
    "\n",
    "        # Parameters\n",
    "        self.logger = logger\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.target_interval = target_interval\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        # policy parameters\n",
    "        self.tau = tau\n",
    "        self.eps_sub = eps_sub\n",
    "        self.eps_sub_decay = 1 - eps_sub_decay\n",
    "        self.gamma = gamma\n",
    "        # ICM parameters\n",
    "        self.beta = beta\n",
    "        self.lam = lam\n",
    "\n",
    "        self.n_proc = n_proc\n",
    "        self.train_steps = train_steps\n",
    "        self.reward_rescale = reward_rescale\n",
    "        self.norm_input = norm_input\n",
    "        self.per = per\n",
    "        self.target_count = 0\n",
    "\n",
    "        if self.per:\n",
    "            memory = PERMemory\n",
    "        else:\n",
    "            memory = Memory\n",
    "\n",
    "        # Create Policies / ICM modules / Memories\n",
    "        self.agent = DDQN_Model(self.state_dim, self.action_dim, hidd_ch)\n",
    "        self.agent_target = DDQN_Model(self.state_dim, self.action_dim, hidd_ch)\n",
    "        self.agent_target.update_target(self.agent)\n",
    "        self.agent_memory = memory(max_memory)\n",
    "        self.agent_opt = torch.optim.Adam(self.agent.parameters(), lr=self.lr)\n",
    "        self.icm = ICM_Model(self.state_dim, self.action_dim, conv).to(sett.device)\n",
    "        self.icm_opt = torch.optim.Adam(self.icm.parameters(), lr=1e-3)\n",
    "\n",
    "        # Send macro to correct device\n",
    "        self.agent = self.agent.to(sett.device)\n",
    "        self.agent_target = self.agent_target.to(sett.device)\n",
    "\n",
    "    def act(self, obs, deterministic=False):\n",
    "        x = torch.from_numpy(obs).float().to(sett.device)\n",
    "        if self.norm_input:\n",
    "            x /= 255\n",
    "\n",
    "        eps = max(0.01, self.eps_sub) if not deterministic else 0.01\n",
    "        actions = self.agent.act(x, eps=eps)\n",
    "        return actions\n",
    "\n",
    "    def set_mode(self, training=False):\n",
    "        self.agent.train(training)\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        # Rescale reward if a scaling is provided\n",
    "        if self.reward_rescale != 0:\n",
    "            if self.reward_rescale == 1:\n",
    "                reward = np.sign(reward)\n",
    "            elif self.reward_rescale == 2:\n",
    "                reward = np.clip(reward, -1, 1)\n",
    "            else:\n",
    "                reward *= self.reward_rescale\n",
    "        return reward\n",
    "\n",
    "    def save(self, i):\n",
    "        if not os.path.isdir(sett.SAVEPATH):\n",
    "            os.makedirs(sett.SAVEPATH)\n",
    "        torch.save(self.agent.state_dict(), os.path.join(sett.SAVEPATH, 'agent_%s.pth' % i))\n",
    "\n",
    "    def load(self, path, i):\n",
    "        self.agent.load_state_dict(torch.load(os.path.join(path, 'agent_%s.pth' % i), map_location=sett.device))\n",
    "\n",
    "    def store_transition(self, s, s1, a, reward, is_terminal):\n",
    "        reward = self.process_reward(reward)\n",
    "        self.agent_memory.store_transition(s, s1, a, reward, is_terminal)\n",
    "#         for i in range(len(s)):\n",
    "#             self.agent_memory.store_transition(s[i], s1[i], a[i], reward[i], is_terminal[i])\n",
    "\n",
    "    def update(self):\n",
    "        for i in range(self.train_steps):\n",
    "            self._update()\n",
    "            if self.logger is not None:\n",
    "                self.logger.step += 1\n",
    "\n",
    "    def _update(self):\n",
    "        # First train each sub policy\n",
    "        i = 0\n",
    "        memory = self.agent_memory\n",
    "        if len(memory) < self.bs * 10:\n",
    "            return\n",
    "\n",
    "        policy = self.agent\n",
    "        target = self.agent_target\n",
    "        icm = self.icm\n",
    "        policy_opt = self.agent_opt\n",
    "        icm_opt = self.icm_opt\n",
    "\n",
    "        if self.per:\n",
    "            state, new_state, action, reward, is_terminal, idxs, w_is = memory.sample(self.bs)\n",
    "            reduction = 'none'\n",
    "            self.logger.log_scalar(tag='Beta PER %i' % i, value=memory.beta)\n",
    "        else:\n",
    "            state, new_state, action, reward, is_terminal = memory.sample(self.bs)\n",
    "            reduction = 'mean'\n",
    "\n",
    "        if self.norm_input:\n",
    "            state = np.array(state, dtype=np.float) / 255\n",
    "            new_state = np.array(new_state, dtype=np.float) / 255\n",
    "\n",
    "        state = torch.tensor(state, dtype=torch.float).detach().to(sett.device)\n",
    "        new_state = torch.tensor(new_state, dtype=torch.float).detach().to(sett.device)\n",
    "        action = torch.tensor(action).detach().to(sett.device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float).detach().to(sett.device)\n",
    "        is_terminal = 1. - torch.tensor(is_terminal, dtype=torch.float).detach().to(sett.device)\n",
    "\n",
    "        # Augment rewards with curiosity\n",
    "        curiosity_rewards = icm.curiosity_rew(state, new_state, action)\n",
    "        reward = (1 - 0.01) * reward + 0.01 * self.lam * curiosity_rewards\n",
    "\n",
    "        # Policy loss\n",
    "        q = policy.forward(state)[torch.arange(self.bs), action]\n",
    "        max_action = torch.argmax(policy.forward(new_state), dim=1)\n",
    "        y = reward + self.gamma * target.forward(new_state)[torch.arange(self.bs), max_action] * is_terminal\n",
    "        policy_loss = smooth_l1_loss(input=q, target=y.detach(), reduction=reduction).mean(-1)\n",
    "\n",
    "        # ICM Loss\n",
    "        phi_hat = icm.forward(state, action)\n",
    "        phi_true = icm.phi_state(new_state)\n",
    "        fwd_loss = mse_loss(input=phi_hat, target=phi_true.detach(), reduction=reduction).mean(-1)\n",
    "        a_hat = icm.inverse_pred(state, new_state)\n",
    "        inv_loss = cross_entropy(input=a_hat, target=action.detach(), reduction=reduction)\n",
    "\n",
    "        # Total loss\n",
    "        inv_loss = (1 - self.beta) * inv_loss\n",
    "        fwd_loss = self.beta * fwd_loss * 288\n",
    "        loss = self.tau * policy_loss + inv_loss + fwd_loss\n",
    "\n",
    "        if self.per:\n",
    "            error = np.clip((torch.abs(q - y)).cpu().data.numpy(), 0, 0.8) # TODO\n",
    "            inv_prob = (1 - softmax(a_hat, dim=1)[torch.arange(self.bs), action]) / 5\n",
    "            curiosity_error = torch.abs(inv_prob).cpu().data.numpy()\n",
    "            total_error = error + curiosity_error\n",
    "\n",
    "            # update priorities\n",
    "            for k in range(self.bs):\n",
    "                memory.update(idxs[k], total_error[k])\n",
    "\n",
    "            loss = (loss * torch.FloatTensor(w_is).to(sett.device)).mean()\n",
    "\n",
    "        policy_opt.zero_grad()\n",
    "        icm_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in policy.parameters():\n",
    "            param.grad.data.clamp(-1, 1)\n",
    "        policy_opt.step()\n",
    "        icm_opt.step()\n",
    "\n",
    "        self.target_count += 1\n",
    "        if self.target_count == self.target_interval:\n",
    "            self.target_count = 0\n",
    "            self.agent_target.update_target(self.agent)\n",
    "\n",
    "        print('Policy Loss %i' % i, policy_loss.mean().cpu().data.numpy())\n",
    "#         print('ICM Fwd Loss %i' % i, fwd_loss.mean().cpu().data.numpy())\n",
    "#         print('ICM Inv Loss %i' % i, inv_loss.mean().cpu().data.numpy())\n",
    "#         print('Total Policy Loss %i' % i, loss.mean().cpu().data.numpy())\n",
    "#         print('Mean Curiosity Reward %i' % i, curiosity_rewards.mean().cpu().data.numpy())\n",
    "#         print('Q values %i' % i, q.mean().cpu().data.numpy())\n",
    "#         print('Target Boltz %i' % i, y.mean().cpu().data.numpy())\n",
    "\n",
    "        # Reduce sub eps\n",
    "        self.eps_sub = self.eps_sub * self.eps_sub_decay\n",
    "\n",
    "        \n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import settings as sett\n",
    "\n",
    "\n",
    "class DDQN_Model(nn.Module):\n",
    "    def __init__(self, state_size, action_size, conv, macro=None, hidd_ch=256, conv_ch=32):\n",
    "        super(DDQN_Model, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        self.hidd_ch = hidd_ch\n",
    "        state_size = np.array(state_size)[[3, 1, 2, 0]]\n",
    "        if macro is None:\n",
    "            self.backbone = nn.Sequential(nn.Conv2d(state_size[0], conv_ch, kernel_size=6, stride=4, padding=1),\n",
    "                                          nn.ELU(),\n",
    "                                          nn.Conv2d(conv_ch, conv_ch, kernel_size=3, stride=2, padding=1),\n",
    "                                          nn.ELU()\n",
    "                                          )\n",
    "\n",
    "        if conv:\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(conv_ch, conv_ch, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ELU(),\n",
    "                nn.Conv2d(conv_ch, conv_ch, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ELU()\n",
    "            )\n",
    "        else:\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Linear(state_size, hidd_ch),\n",
    "                nn.ReLU(),\n",
    "                #nn.Linear(hidd_ch, hidd_ch),\n",
    "                #nn.ReLU()\n",
    "            )\n",
    "\n",
    "        if macro is None:\n",
    "            out_shape = self.features(self.backbone(torch.randn(*((1,) + tuple(state_size[:-1]))))).view(-1).size().numel()\n",
    "        else:\n",
    "            out_shape = self.features(macro.backbone(torch.randn(*((1,) + tuple(state_size[:-1]))))).view(-1).size().numel()\n",
    "        self.lstm_out = nn.LSTM(out_shape, hidd_ch, 1, batch_first=True)\n",
    "\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(hidd_ch, self.action_size)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidd_ch, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, macro=None):\n",
    "        if obs.ndimension() == 4:\n",
    "            obs = obs[None]\n",
    "        obs = obs.float().transpose(2, 4)\n",
    "        stack = obs.shape[1]\n",
    "        backbone = self.backbone if macro is None else macro.backbone\n",
    "        x = torch.cat([self.features(backbone(obs[:, i]))[:, None] for i in range(stack)], dim=1)\n",
    "        x = x.view(x.size(0), stack, -1)\n",
    "\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidd_ch).to(sett.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidd_ch).to(sett.device)\n",
    "\n",
    "        lstm_out, (hn, cn) = self.lstm_out(x, (h0.detach(), c0.detach()))\n",
    "        x = lstm_out[:, -1]\n",
    "        adv = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        return value + (adv - adv.mean(-1, keepdim=True))\n",
    "\n",
    "    def act(self, state, eps, backbone=None):\n",
    "        if np.random.random() > eps:\n",
    "            q = self.forward(state, backbone)\n",
    "            action = torch.argmax(q, dim=-1).cpu().data.numpy()\n",
    "        else:\n",
    "            action = np.random.randint(self.action_size, size=1)\n",
    "        return action.item() if action.shape == (1,) else list(action.astype(np.int64))\n",
    "\n",
    "    def update_target(self, model):\n",
    "        self.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "class ICM_Model(nn.Module):\n",
    "    def __init__(self, state_size, action_size, conv):\n",
    "        super(ICM_Model, self).__init__()\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.state_size = np.array(state_size)[[3, 1, 2, 0]]\n",
    "\n",
    "        # Projection\n",
    "        if conv:\n",
    "            self.phi = nn.Sequential(\n",
    "                nn.Conv2d(self.state_size[0], 32, kernel_size=6, stride=4, padding=1),\n",
    "                nn.ELU(),\n",
    "                nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ELU(),\n",
    "                nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ELU(),\n",
    "                nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ELU()\n",
    "            )\n",
    "        else:\n",
    "            self.phi = nn.Sequential(\n",
    "                nn.Linear(self.state_size, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.Relu()\n",
    "            )\n",
    "\n",
    "        out_shape = self.phi(torch.randn(*((1,) + tuple(self.state_size[:-1])))).view(-1).size().numel()\n",
    "\n",
    "        # Forward Model\n",
    "        self.fwd = nn.Sequential(\n",
    "            nn.Linear(out_shape + 1, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_shape)\n",
    "        )\n",
    "\n",
    "        # Inverse Model\n",
    "        self.inv = nn.Sequential(\n",
    "            nn.Linear(out_shape * 2, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, *input):\n",
    "        obs, action = input\n",
    "        action = action.view(-1, 1)\n",
    "        phi = self.phi_state(obs)\n",
    "        x = torch.cat((phi, action.float()), -1)\n",
    "        phi_hat = self.fwd(x)\n",
    "        return phi_hat\n",
    "\n",
    "    def phi_state(self, s):\n",
    "        s = s[:, -1]\n",
    "        x = s.float().transpose(1, 3)\n",
    "        x = self.phi(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def inverse_pred(self, s, s1):\n",
    "        s = self.phi_state(s.float())\n",
    "        s1 = self.phi_state(s1.float())\n",
    "        x = torch.cat((s, s1), -1)\n",
    "        return self.inv(x)\n",
    "\n",
    "    def curiosity_rew(self, s, s1, a):\n",
    "        phi_hat = self.forward(s, a)\n",
    "        phi_s1 = self.phi_state(s1)\n",
    "        cur_rew = 1 / 2 * (torch.norm(phi_hat - phi_s1, p=2, dim=-1) ** 2)\n",
    "        return cur_rew\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aae136",
   "metadata": {},
   "source": [
    "## wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e24846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T08:48:36.513705Z",
     "start_time": "2023-12-04T08:48:36.219839Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Wrapper\n",
    "from gym.spaces import Box\n",
    "\n",
    "\n",
    "class TimeLimit(Wrapper):\n",
    "    def __init__(self, env, max_episode_steps=None):\n",
    "        super(TimeLimit, self).__init__(env)\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self._max_episode_steps = max_episode_steps\n",
    "        self._elapsed_steps = None\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        self._elapsed_steps += 1\n",
    "        if self._elapsed_steps >= self._max_episode_steps:\n",
    "            done = True\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self._elapsed_steps = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "from gym.spaces import Box\n",
    "from gym import ObservationWrapper\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class LazyFrames(object):\n",
    "    r\"\"\"Ensures common frames are only stored once to optimize memory use. \n",
    "\n",
    "    To further reduce the memory use, it is optionally to turn on lz4 to \n",
    "    compress the observations.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        This object should only be converted to numpy array just before forward pass. \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, frames, lz4_compress=False):\n",
    "        if lz4_compress:\n",
    "            from lz4.block import compress\n",
    "            self.shape = frames[0].shape\n",
    "            self.dtype = frames[0].dtype\n",
    "            frames = [compress(frame) for frame in frames]\n",
    "        self._frames = frames\n",
    "        self.lz4_compress = lz4_compress\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        if self.lz4_compress:\n",
    "            from lz4.block import decompress\n",
    "            frames = [np.frombuffer(decompress(frame), dtype=self.dtype).reshape(self.shape) for frame in self._frames]\n",
    "        else:\n",
    "            frames = self._frames\n",
    "        out = np.stack(frames, axis=0)\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__array__())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.__array__()[i]\n",
    "\n",
    "\n",
    "class FrameStack(ObservationWrapper):\n",
    "    r\"\"\"Observation wrapper that stacks the observations in a rolling manner. \n",
    "\n",
    "    For example, if the number of stacks is 4, then the returned observation contains\n",
    "    the most recent 4 observations. For environment 'Pendulum-v0', the original observation\n",
    "    is an array with shape [3], so if we stack 4 observations, the processed observation\n",
    "    has shape [3, 4]. \n",
    "\n",
    "    .. note::\n",
    "\n",
    "        To be memory efficient, the stacked observations are wrapped by :class:`LazyFrame`.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        The observation space must be `Box` type. If one uses `Dict`\n",
    "        as observation space, it should apply `FlattenDictWrapper` at first. \n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> import gym\n",
    "        >>> env = gym.make('PongNoFrameskip-v0')\n",
    "        >>> env = FrameStack(env, 4)\n",
    "        >>> env.observation_space\n",
    "        Box(4, 210, 160, 3)\n",
    "\n",
    "    Args:\n",
    "        env (Env): environment object\n",
    "        num_stack (int): number of stacks\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, env, num_stack, lz4_compress=False):\n",
    "        super(FrameStack, self).__init__(env)\n",
    "        self.num_stack = num_stack\n",
    "        self.lz4_compress = lz4_compress\n",
    "\n",
    "        self.frames = deque(maxlen=num_stack)\n",
    "\n",
    "        low = np.repeat(self.observation_space.low[np.newaxis, ...], num_stack, axis=0)\n",
    "        high = np.repeat(self.observation_space.high[np.newaxis, ...], num_stack, axis=0)\n",
    "        self.observation_space = Box(low=low, high=high, dtype=self.observation_space.dtype)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        assert len(self.frames) == self.num_stack, (len(self.frames), self.num_stack)\n",
    "        return LazyFrames(list(self.frames), self.lz4_compress)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(observation)\n",
    "        return self._get_observation(), reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        [self.frames.append(observation) for _ in range(self.num_stack)]\n",
    "        return self._get_observation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91744ccb",
   "metadata": {},
   "source": [
    "## env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0aceb07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T08:48:36.536869Z",
     "start_time": "2023-12-04T08:48:36.516055Z"
    }
   },
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "import numpy as np\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DimGridEnvironment(Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, size, hard):\n",
    "        super(DimGridEnvironment, self).__init__()\n",
    "        self.size = size\n",
    "        self.observation_space = spaces.Box(0, 1, [self.size, self.size, 1])\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        self.alternate_dim = False\n",
    "        self.dim0 = None\n",
    "        self.dim1 = None\n",
    "        self.dict = {'0': {'empty': 0, 'wall': 0.33, 'player': 0.66, 'goal': 1},\n",
    "                     '1': {'empty': 1, 'wall': 0.66, 'player': 0.33, 'goal': 0}}\n",
    "        self.action_dict = {0: [0, -1], 1: [-1, 0], 2: [0, 1], 3: [1, 0], 4: 4}\n",
    "        self.goal = None\n",
    "        self.player_pos = None\n",
    "        self.walls = None\n",
    "        self.hard = hard\n",
    "        self.seed = self._seed()\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return seed\n",
    "\n",
    "    def _compute_action(self, action):\n",
    "        action = self.action_dict[action]\n",
    "        reward = 0.\n",
    "\n",
    "        if action == 4: # Change dimension\n",
    "            self.alternate_dim = not self.alternate_dim\n",
    "        else:\n",
    "            new_pos = tuple(np.array(self.player_pos) + action)\n",
    "            if any(np.array(new_pos) == self.size) or any(np.array(new_pos) < 0):\n",
    "                reward = -1. if not self.hard else 0.\n",
    "                return reward, True\n",
    "            self.player_pos = new_pos\n",
    "            touched_wall = self.walls[self.player_pos]\n",
    "            if self.alternate_dim:\n",
    "                reward += -0.1\n",
    "            else:\n",
    "                if touched_wall:\n",
    "                    reward += -0.5\n",
    "                    if self.hard:\n",
    "                        return 0., True\n",
    "\n",
    "        terminal = True if self.player_pos == self.goal else False\n",
    "        reward += int(terminal)\n",
    "        return reward, terminal\n",
    "\n",
    "    def step(self, action):\n",
    "        reward, terminal = self._compute_action(action)\n",
    "        obs = self._get_obs()\n",
    "        return obs, reward, terminal, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self.alternate_dim:\n",
    "            self.dim1[self.walls] = self.dict['1']['wall']\n",
    "            self.dim1[self.player_pos] = self.dict['1']['player']\n",
    "            return self.dim1\n",
    "        else:\n",
    "            self.dim0[self.walls] = self.dict['0']['wall']\n",
    "            self.dim0[self.player_pos] = self.dict['0']['player']\n",
    "            return self.dim0\n",
    "\n",
    "    def reset(self):\n",
    "        y = np.random.choice(self.size, 2, replace=False)\n",
    "        x = np.random.choice(self.size, 2, replace=False)\n",
    "        self.player_pos = tuple([x[0], y[0]])\n",
    "        self.goal = tuple([x[1], y[1]])\n",
    "\n",
    "        # Fill dim0\n",
    "        self.dim0 = np.zeros(self.observation_space.shape)\n",
    "        self.walls = np.random.rand(*self.observation_space.shape) > 0.5\n",
    "        self.dim0[self.walls] = self.dict['0']['wall']\n",
    "        self.dim0[self.goal] = self.dict['0']['goal']\n",
    "        self.dim0[self.player_pos] = self.dict['0']['player']\n",
    "\n",
    "        # Fill dim1\n",
    "        self.dim1 = np.ones(self.observation_space.shape)\n",
    "        self.dim1[self.walls] = self.dict['1']['wall']\n",
    "        self.dim1[self.goal] = self.dict['1']['goal']\n",
    "        self.dim1[self.player_pos] = self.dict['1']['player']\n",
    "\n",
    "        self.walls[self.goal] = False\n",
    "        self.walls[self.player_pos] = False\n",
    "        self.alternate_dim = False\n",
    "        return self._get_obs()\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        logger.info('\\n Dim0: %s \\n Dim1:  %s' % (self.dim0, self.dim1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ea5313",
   "metadata": {},
   "source": [
    "## core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c4b8b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T08:48:36.543812Z",
     "start_time": "2023-12-04T08:48:36.539326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup env\n",
    "env = DimGridEnvironment(size=4, hard=False)\n",
    "env = TimeLimit(env, max_episode_steps=4 * 4)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cbc23a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T08:48:36.553376Z",
     "start_time": "2023-12-04T08:48:36.547063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_actions 5\n",
      "n_state (4, 4, 4, 1)\n",
      "conv True\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "# Setup Model\n",
    "n_actions = env.action_space.n if env.action_space.shape == () else env.action_space.shape[0]\n",
    "n_state = env.observation_space.n if env.observation_space.shape == () else env.observation_space.shape\n",
    "print('n_actions', n_actions)\n",
    "print('n_state', n_state)\n",
    "\n",
    "conv = True if isinstance(n_state, tuple) else False\n",
    "print('conv', conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26ad0e09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T08:48:39.671165Z",
     "start_time": "2023-12-04T08:48:36.555940Z"
    }
   },
   "outputs": [],
   "source": [
    "dqn = DQN(state_dim=n_state,\n",
    "          tau=0.001,\n",
    "          action_dim=n_actions,\n",
    "          gamma=0.95,\n",
    "          hidd_ch=256,\n",
    "          lam=0.1,\n",
    "          lr=1e-4,\n",
    "          eps_sub=0.3,\n",
    "          eps_sub_decay=0.99,\n",
    "          beta=0.9,\n",
    "          bs=128,\n",
    "          target_interval=500,\n",
    "          train_steps=100,\n",
    "          max_memory=1_000_000,\n",
    "          conv=conv,\n",
    "          per=None,\n",
    "          n_proc=1,\n",
    "          reward_rescale=False,\n",
    "          logger=None,\n",
    "          norm_input=False\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5912a53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T08:56:13.949644Z",
     "start_time": "2023-12-04T08:56:13.941455Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "action = dqn.act(np.array(obs))\n",
    "obs_new, r, is_terminal, info = env.step(action)\n",
    "dqn.store_transition(np.array(obs), np.array(obs_new), action, r, is_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63d7b204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T08:57:35.927052Z",
     "start_time": "2023-12-04T08:57:30.658772Z"
    }
   },
   "outputs": [],
   "source": [
    "dqn.set_mode(training=False)\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action = dqn.act(np.array(obs))\n",
    "    obs_new, r, is_terminal, info = env.step(action)\n",
    "    dqn.store_transition(np.array(obs), np.array(obs_new), action, r, is_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ad77a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T08:57:44.434194Z",
     "start_time": "2023-12-04T08:57:36.970930Z"
    }
   },
   "outputs": [],
   "source": [
    "dqn.set_mode(training=True)\n",
    "dqn.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e67faab6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T08:57:51.273885Z",
     "start_time": "2023-12-04T08:57:46.092175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward -535.5\n"
     ]
    }
   ],
   "source": [
    "tot_reward = 0\n",
    "dqn.set_mode(training=False)\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action = dqn.act(np.array(obs), deterministic=True)\n",
    "    obs_new, r, is_terminal, info = env.step(action)\n",
    "\n",
    "    tot_reward += r\n",
    "    obs = obs_new\n",
    "    if is_terminal:\n",
    "        obs = env.reset()\n",
    "print('total_reward', tot_reward)\n",
    "\n",
    "dqn.set_mode(training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ded21",
   "metadata": {},
   "source": [
    "Training Progress\n",
    "total_reward -535.8000000000001\n",
    "total_reward -3.3000000000000003\n",
    "total_reward -2.3\n",
    "total_reward -3.8000000000000003\n",
    "total_reward -1.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianshou",
   "language": "python",
   "name": "tianshou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
