{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/zhx/word/DriverOrderOfflineRL/cage-challenge-1/CybORG')\n",
    "\n",
    "import inspect\n",
    "from pprint import pprint\n",
    "from CybORG import CybORG\n",
    "from CybORG.Shared.Actions import *\n",
    "from CybORG.Agents import RedMeanderAgent, B_lineAgent\n",
    "from CybORG.Agents.Wrappers import *\n",
    "\n",
    "path = str(inspect.getfile(CybORG))\n",
    "path = path[:-10] + '/Shared/Scenarios/Scenario1b.yaml'\n",
    "\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from typing import Tuple, Union, List\n",
    "from torch import Tensor\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import numpy as np\n",
    "from multiprocessing import Pipe, Process\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, agent_class=None, env_class=None, env_args=None):\n",
    "        self.num_envs = None\n",
    "        self.agent_class = agent_class  # agent = agent_class(...)\n",
    "        self.if_off_policy = self.get_if_off_policy()  # whether off-policy or on-policy of DRL algorithm\n",
    "\n",
    "        '''Argument of environment'''\n",
    "        self.env_class = env_class  # env = env_class(**env_args)\n",
    "        self.env_args = env_args  # env = env_class(**env_args)\n",
    "        if env_args is None:  # dummy env_args\n",
    "            env_args = {'env_name': None,\n",
    "                        'num_envs': 1,\n",
    "                        'max_step': 12345,\n",
    "                        'state_dim': None,\n",
    "                        'action_dim': None,\n",
    "                        'if_discrete': None, }\n",
    "        env_args.setdefault('num_envs', 1)  # `num_envs=1` in default in single env.\n",
    "        env_args.setdefault('max_step', 12345)  # `max_step=12345` in default, which is a large enough value.\n",
    "        self.env_name = env_args['env_name']  # the name of environment. Be used to set 'cwd'.\n",
    "        self.num_envs = env_args['num_envs']  # the number of sub envs in vectorized env. `num_envs=1` in single env.\n",
    "        self.max_step = env_args['max_step']  # the max step number of an episode. 'set as 12345 in default.\n",
    "        self.state_dim = env_args['state_dim']  # vector dimension (feature number) of state\n",
    "        self.action_dim = env_args['action_dim']  # vector dimension (feature number) of action\n",
    "        self.if_discrete = env_args['if_discrete']  # discrete or continuous action space\n",
    "\n",
    "        '''Arguments for reward shaping'''\n",
    "        self.gamma = 0.99  # discount factor of future rewards\n",
    "        self.reward_scale = 2 ** 0  # an approximate target reward usually be closed to 256\n",
    "\n",
    "        '''Arguments for training'''\n",
    "        self.net_dims = (64, 32)  # the middle layer dimension of MLP (MultiLayer Perceptron)\n",
    "        self.learning_rate = 6e-5  # the learning rate for network updating\n",
    "        self.clip_grad_norm = 3.0  # 0.1 ~ 4.0, clip the gradient after normalization\n",
    "        self.state_value_tau = 0  # the tau of normalize for value and state `std = (1-std)*std + tau*std`\n",
    "        self.soft_update_tau = 5e-3  # 2 ** -8 ~= 5e-3. the tau of soft target update `net = (1-tau)*net + tau*net1`\n",
    "        if self.if_off_policy:  # off-policy\n",
    "            self.batch_size = int(64)  # num of transitions sampled from replay buffer.\n",
    "            self.horizon_len = int(512)  # collect horizon_len step while exploring, then update networks\n",
    "            self.buffer_size = int(1e6)  # ReplayBuffer size. First in first out for off-policy.\n",
    "            self.repeat_times = 1.0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
    "            self.if_use_per = False  # use PER (Prioritized Experience Replay) for sparse reward\n",
    "        else:  # on-policy\n",
    "            self.batch_size = int(128)  # num of transitions sampled from replay buffer.\n",
    "            self.horizon_len = int(2048)  # collect horizon_len step while exploring, then update network\n",
    "            self.buffer_size = None  # ReplayBuffer size. Empty the ReplayBuffer for on-policy.\n",
    "            self.repeat_times = 8.0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
    "            self.if_use_vtrace = False  # use V-trace + GAE (Generalized Advantage Estimation) for sparse reward\n",
    "\n",
    "        '''Arguments for device'''\n",
    "        self.gpu_id = int(0)  # `int` means the ID of single GPU, -1 means CPU\n",
    "        self.num_workers = 2  # rollout workers number pre GPU (adjust it to get high GPU usage)\n",
    "        self.num_threads = 8  # cpu_num for pytorch, `torch.set_num_threads(self.num_threads)`\n",
    "        self.random_seed = 0  # initialize random seed in self.init_before_training()\n",
    "        self.learner_gpus = 0  # `int` means the ID of single GPU, -1 means CPU\n",
    "\n",
    "        '''Arguments for evaluate'''\n",
    "        self.cwd = None  # current working directory to save model. None means set automatically\n",
    "        self.if_remove = True  # remove the cwd folder? (True, False, None:ask me)\n",
    "        self.break_step = np.inf  # break training if 'total_step > break_step'\n",
    "        self.break_score = np.inf  # break training if `cumulative_rewards > break_score`\n",
    "        self.if_keep_save = True  # keeping save the checkpoint. False means save until stop training.\n",
    "        self.if_over_write = False  # overwrite the best policy network. `self.cwd/actor.pth`\n",
    "        self.if_save_buffer = False  # if save the replay buffer for continuous training after stop training\n",
    "\n",
    "        self.save_gap = int(8)  # save actor f\"{cwd}/actor_*.pth\" for learning curve.\n",
    "        self.eval_times = int(3)  # number of times that get the average episodic cumulative return\n",
    "        self.eval_per_step = int(2e4)  # evaluate the agent per training steps\n",
    "        self.eval_env_class = None  # eval_env = eval_env_class(*eval_env_args)\n",
    "        self.eval_env_args = None  # eval_env = eval_env_class(*eval_env_args)\n",
    "\n",
    "    def init_before_training(self):\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        torch.set_num_threads(self.num_threads)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "\n",
    "        '''set cwd (current working directory) for saving model'''\n",
    "        if self.cwd is None:  # set cwd (current working directory) for saving model\n",
    "            self.cwd = f'./{self.env_name}_{self.agent_class.__name__[5:]}_{self.random_seed}'\n",
    "\n",
    "        '''remove history'''\n",
    "        if self.if_remove is None:\n",
    "            self.if_remove = bool(input(f\"| Arguments PRESS 'y' to REMOVE: {self.cwd}? \") == 'y')\n",
    "        if self.if_remove:\n",
    "            import shutil\n",
    "            shutil.rmtree(self.cwd, ignore_errors=True)\n",
    "            print(f\"| Arguments Remove cwd: {self.cwd}\")\n",
    "        else:\n",
    "            print(f\"| Arguments Keep cwd: {self.cwd}\")\n",
    "        os.makedirs(self.cwd, exist_ok=True)\n",
    "\n",
    "    def get_if_off_policy(self) -> bool:\n",
    "        agent_name = self.agent_class.__name__ if self.agent_class else ''\n",
    "        on_policy_names = ('SARSA', 'VPG', 'A2C', 'A3C', 'TRPO', 'PPO', 'MPO')\n",
    "        return all([agent_name.find(s) == -1 for s in on_policy_names])\n",
    "\n",
    "    def print(self):\n",
    "        from pprint import pprint\n",
    "        pprint(vars(self))  # prints out args in a neat, readable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:  # for off-policy\n",
    "    def __init__(self,\n",
    "                 max_size: int,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 gpu_id: int = 0,\n",
    "                 num_seqs: int = 1,\n",
    "                 if_use_per: bool = False,\n",
    "                 args: Config = Config()):\n",
    "        self.p = 0  # pointer\n",
    "        self.if_full = False\n",
    "        self.cur_size = 0\n",
    "        self.add_size = 0\n",
    "        self.add_item = None\n",
    "        self.max_size = max_size\n",
    "        self.num_seqs = num_seqs\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "\n",
    "        \"\"\"The struction of ReplayBuffer (for example, num_seqs = num_workers * num_envs == 2*4 = 8\n",
    "        ReplayBuffer:\n",
    "        worker0 for env0:   sequence of sub_env0.0  self.states  = Tensor[s, s, ..., s, ..., s]     \n",
    "                                                    self.actions = Tensor[a, a, ..., a, ..., a]   \n",
    "                                                    self.rewards = Tensor[r, r, ..., r, ..., r]   \n",
    "                                                    self.undones = Tensor[d, d, ..., d, ..., d]\n",
    "                                                                          <-----max_size----->\n",
    "                                                                          <-cur_size->\n",
    "                                                                                     ↑ pointer\n",
    "                            sequence of sub_env0.1  s, s, ..., s    a, a, ..., a    r, r, ..., r    d, d, ..., d\n",
    "                            sequence of sub_env0.2  s, s, ..., s    a, a, ..., a    r, r, ..., r    d, d, ..., d\n",
    "                            sequence of sub_env0.3  s, s, ..., s    a, a, ..., a    r, r, ..., r    d, d, ..., d\n",
    "        worker1 for env1:   sequence of sub_env1.0  s, s, ..., s    a, a, ..., a    r, r, ..., r    d, d, ..., d\n",
    "                            sequence of sub_env1.1  s, s, ..., s    a, a, ..., a    r, r, ..., r    d, d, ..., d\n",
    "                            sequence of sub_env1.2  s, s, ..., s    a, a, ..., a    r, r, ..., r    d, d, ..., d\n",
    "                            sequence of sub_env1.3  s, s, ..., s    a, a, ..., a    r, r, ..., r    d, d, ..., d\n",
    "        \n",
    "        D: done=True\n",
    "        d: done=False\n",
    "        sequence of transition: s-a-r-d, s-a-r-d, s-a-r-D  s-a-r-d, s-a-r-d, s-a-r-d, s-a-r-d, s-a-r-D  s-a-r-d, ...\n",
    "                                <------trajectory------->  <----------trajectory--------------------->  <-----------\n",
    "        \"\"\"\n",
    "        self.states = torch.empty((max_size, num_seqs, state_dim), dtype=torch.float32, device=self.device)\n",
    "        self.actions = torch.empty((max_size, num_seqs, action_dim), dtype=torch.float32, device=self.device)\n",
    "        self.rewards = torch.empty((max_size, num_seqs), dtype=torch.float32, device=self.device)\n",
    "        self.undones = torch.empty((max_size, num_seqs), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.if_use_per = if_use_per\n",
    "        if if_use_per:\n",
    "            self.sum_trees = [SumTree(buf_len=max_size) for _ in range(num_seqs)]\n",
    "            self.per_alpha = getattr(args, 'per_alpha', 0.6)  # alpha = (Uniform:0, Greedy:1)\n",
    "            self.per_beta = getattr(args, 'per_beta', 0.4)  # alpha = (Uniform:0, Greedy:1)\n",
    "            \"\"\"PER.  Prioritized Experience Replay. Section 4\n",
    "            alpha, beta = 0.7, 0.5 for rank-based variant\n",
    "            alpha, beta = 0.6, 0.4 for proportional variant\n",
    "            \"\"\"\n",
    "        else:\n",
    "            self.sum_trees = None\n",
    "            self.per_alpha = None\n",
    "            self.per_beta = None\n",
    "\n",
    "    def update(self, items: Tuple[Tensor, ...]):\n",
    "        self.add_item = items\n",
    "        states, actions, rewards, undones = items\n",
    "        # assert states.shape[1:] == (env_num, state_dim)\n",
    "        # assert actions.shape[1:] == (env_num, action_dim)\n",
    "        # assert rewards.shape[1:] == (env_num,)\n",
    "        # assert undones.shape[1:] == (env_num,)\n",
    "        self.add_size = rewards.shape[0]\n",
    "\n",
    "        p = self.p + self.add_size  # pointer\n",
    "        if p > self.max_size:\n",
    "            self.if_full = True\n",
    "            p0 = self.p\n",
    "            p1 = self.max_size\n",
    "            p2 = self.max_size - self.p\n",
    "            p = p - self.max_size\n",
    "\n",
    "            self.states[p0:p1], self.states[0:p] = states[:p2], states[-p:]\n",
    "            self.actions[p0:p1], self.actions[0:p] = actions[:p2], actions[-p:]\n",
    "            self.rewards[p0:p1], self.rewards[0:p] = rewards[:p2], rewards[-p:]\n",
    "            self.undones[p0:p1], self.undones[0:p] = undones[:p2], undones[-p:]\n",
    "        else:\n",
    "            self.states[self.p:p] = states\n",
    "            self.actions[self.p:p] = actions\n",
    "            self.rewards[self.p:p] = rewards\n",
    "            self.undones[self.p:p] = undones\n",
    "\n",
    "        if self.if_use_per:\n",
    "            '''data_ids for single env'''\n",
    "            data_ids = torch.arange(self.p, p, dtype=torch.long, device=self.device)\n",
    "            if p > self.max_size:\n",
    "                data_ids = torch.fmod(data_ids, self.max_size)\n",
    "\n",
    "            '''apply data_ids for vectorized env'''\n",
    "            for sum_tree in self.sum_trees:\n",
    "                sum_tree.update_ids(data_ids=data_ids.cpu(), prob=10.)\n",
    "\n",
    "        self.p = p\n",
    "        self.cur_size = self.max_size if self.if_full else self.p\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "        sample_len = self.cur_size - 1\n",
    "\n",
    "        ids = torch.randint(sample_len * self.num_seqs, size=(batch_size,), requires_grad=False)\n",
    "        ids0 = torch.fmod(ids, sample_len)  # ids % sample_len\n",
    "        ids1 = torch.div(ids, sample_len, rounding_mode='floor')  # ids // sample_len\n",
    "\n",
    "        return (self.states[ids0, ids1],\n",
    "                self.actions[ids0, ids1],\n",
    "                self.rewards[ids0, ids1],\n",
    "                self.undones[ids0, ids1],\n",
    "                self.states[ids0 + 1, ids1],)  # next_state\n",
    "\n",
    "    def sample_for_per(self, batch_size: int) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "        beg = -self.max_size\n",
    "        end = (self.cur_size - self.max_size) if (self.cur_size < self.max_size) else -1\n",
    "\n",
    "        '''get is_indices, is_weights'''\n",
    "        is_indices: list = []\n",
    "        is_weights: list = []\n",
    "\n",
    "        assert batch_size % self.num_seqs == 0\n",
    "        sub_batch_size = batch_size // self.num_seqs\n",
    "        for env_i in range(self.num_seqs):\n",
    "            sum_tree = self.sum_trees[env_i]\n",
    "            _is_indices, _is_weights = sum_tree.important_sampling(batch_size, beg, end, self.per_beta)\n",
    "            is_indices.append(_is_indices + sub_batch_size * env_i)\n",
    "            is_weights.append(_is_weights)\n",
    "\n",
    "        is_indices: Tensor = torch.hstack(is_indices).to(self.device)\n",
    "        is_weights: Tensor = torch.hstack(is_weights).to(self.device)\n",
    "\n",
    "        ids0 = torch.fmod(is_indices, self.cur_size)  # is_indices % sample_len\n",
    "        ids1 = torch.div(is_indices, self.cur_size, rounding_mode='floor')  # is_indices // sample_len\n",
    "        return (\n",
    "            self.states[ids0, ids1],\n",
    "            self.actions[ids0, ids1],\n",
    "            self.rewards[ids0, ids1],\n",
    "            self.undones[ids0, ids1],\n",
    "            self.states[ids0 + 1, ids1],  # next_state\n",
    "            is_weights,  # important sampling weights\n",
    "            is_indices,  # important sampling indices\n",
    "        )\n",
    "\n",
    "    def td_error_update_for_per(self, is_indices: Tensor, td_error: Tensor):  # td_error = (q-q).detach_().abs()\n",
    "        prob = td_error.clamp(1e-8, 10).pow(self.per_alpha)\n",
    "\n",
    "        # self.sum_tree.update_ids(is_indices.cpu(), prob.cpu())\n",
    "        batch_size = td_error.shape[0]\n",
    "        sub_batch_size = batch_size // self.num_seqs\n",
    "        for env_i in range(self.num_seqs):\n",
    "            sum_tree = self.sum_trees[env_i]\n",
    "            slice_i = env_i * sub_batch_size\n",
    "            slice_j = slice_i + sub_batch_size\n",
    "\n",
    "            sum_tree.update_ids(is_indices[slice_i:slice_j].cpu(), prob[slice_i:slice_j].cpu())\n",
    "\n",
    "    def save_or_load_history(self, cwd: str, if_save: bool):\n",
    "        item_names = (\n",
    "            (self.states, \"states\"),\n",
    "            (self.actions, \"actions\"),\n",
    "            (self.rewards, \"rewards\"),\n",
    "            (self.undones, \"undones\"),\n",
    "        )\n",
    "\n",
    "        if if_save:\n",
    "            for item, name in item_names:\n",
    "                if self.cur_size == self.p:\n",
    "                    buf_item = item[:self.cur_size]\n",
    "                else:\n",
    "                    buf_item = torch.vstack((item[self.p:self.cur_size], item[0:self.p]))\n",
    "                file_path = f\"{cwd}/replay_buffer_{name}.pth\"\n",
    "                print(f\"| buffer.save_or_load_history(): Save {file_path}\")\n",
    "                torch.save(buf_item, file_path)\n",
    "\n",
    "        elif all([os.path.isfile(f\"{cwd}/replay_buffer_{name}.pth\") for item, name in item_names]):\n",
    "            max_sizes = []\n",
    "            for item, name in item_names:\n",
    "                file_path = f\"{cwd}/replay_buffer_{name}.pth\"\n",
    "                print(f\"| buffer.save_or_load_history(): Load {file_path}\")\n",
    "                buf_item = torch.load(file_path)\n",
    "\n",
    "                max_size = buf_item.shape[0]\n",
    "                item[:max_size] = buf_item\n",
    "                max_sizes.append(max_size)\n",
    "            assert all([max_size == max_sizes[0] for max_size in max_sizes])\n",
    "            self.cur_size = self.p = max_sizes[0]\n",
    "            self.if_full = self.cur_size == self.max_size\n",
    "\n",
    "\n",
    "class SumTree:\n",
    "    \"\"\" BinarySearchTree for PER (SumTree)\n",
    "    Contributor: Github GyChou, Github mississippiu\n",
    "    Reference: https://github.com/kaixindelele/DRLib/tree/main/algos/pytorch/td3_sp\n",
    "    Reference: https://github.com/jaromiru/AI-blog/blob/master/SumTree.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buf_len: int):\n",
    "        self.buf_len = buf_len  # replay buffer len\n",
    "        self.max_len = (buf_len - 1) + buf_len  # parent_nodes_num + leaf_nodes_num\n",
    "        self.depth = math.ceil(math.log2(self.max_len))\n",
    "\n",
    "        self.tree = torch.zeros(self.max_len, dtype=torch.float32)\n",
    "\n",
    "    def update_id(self, data_id: int, prob=10):  # 10 is max_prob\n",
    "        tree_id = data_id + self.buf_len - 1\n",
    "\n",
    "        delta = prob - self.tree[tree_id]\n",
    "        self.tree[tree_id] = prob\n",
    "\n",
    "        for depth in range(self.depth - 2):  # propagate the change through tree\n",
    "            tree_id = (tree_id - 1) // 2  # faster than the recursive loop\n",
    "            self.tree[tree_id] += delta\n",
    "\n",
    "    def update_ids(self, data_ids: Tensor, prob: Tensor = 10.):  # 10 is max_prob\n",
    "        l_ids = data_ids + self.buf_len - 1\n",
    "\n",
    "        self.tree[l_ids] = prob\n",
    "        for depth in range(self.depth - 2):  # propagate the change through tree\n",
    "            p_ids = ((l_ids - 1) // 2).unique()  # parent indices\n",
    "            l_ids = p_ids * 2 + 1  # left children indices\n",
    "            r_ids = l_ids + 1  # right children indices\n",
    "            self.tree[p_ids] = self.tree[l_ids] + self.tree[r_ids]\n",
    "\n",
    "            l_ids = p_ids\n",
    "\n",
    "    def get_leaf_id_and_value(self, v) -> Tuple[int, float]:\n",
    "        \"\"\"Tree structure and array storage:\n",
    "        Tree index:\n",
    "              0       -> storing priority sum\n",
    "            |  |\n",
    "          1     2\n",
    "         | |   | |\n",
    "        3  4  5  6    -> storing priority for transitions\n",
    "        Array type for storing: [0, 1, 2, 3, 4, 5, 6]\n",
    "        \"\"\"\n",
    "        p_id = 0  # the leaf's parent node\n",
    "\n",
    "        for depth in range(self.depth - 2):  # propagate the change through tree\n",
    "            l_id = min(2 * p_id + 1, self.max_len - 1)  # the leaf's left node\n",
    "            r_id = l_id + 1  # the leaf's right node\n",
    "            if v <= self.tree[l_id]:\n",
    "                p_id = l_id\n",
    "            else:\n",
    "                v -= self.tree[l_id]\n",
    "                p_id = r_id\n",
    "        return p_id, self.tree[p_id]  # leaf_id and leaf_value\n",
    "\n",
    "    def important_sampling(self, batch_size: int, beg: int, end: int, per_beta: float) -> Tuple[Tensor, Tensor]:\n",
    "        # get random values for searching indices with proportional prioritization\n",
    "        values = (torch.arange(batch_size) + torch.rand(batch_size)) * (self.tree[0] / batch_size)\n",
    "\n",
    "        # get proportional prioritization\n",
    "        leaf_ids, leaf_values = list(zip(*[self.get_leaf_id_and_value(v) for v in values]))\n",
    "        leaf_ids = torch.tensor(leaf_ids, dtype=torch.long)\n",
    "        leaf_values = torch.tensor(leaf_values, dtype=torch.float32)\n",
    "\n",
    "        indices = leaf_ids - (self.buf_len - 1)\n",
    "        assert 0 <= indices.min()\n",
    "        assert indices.max() < self.buf_len\n",
    "\n",
    "        prob_ary = leaf_values / self.tree[beg:end].min()\n",
    "        weights = torch.pow(prob_ary, -per_beta)\n",
    "        return indices, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentBase:\n",
    "    \"\"\"\n",
    "    The basic agent of ElegantRL\n",
    "\n",
    "    net_dims: the middle layer dimension of MLP (MultiLayer Perceptron)\n",
    "    state_dim: the dimension of state (the number of state vector)\n",
    "    action_dim: the dimension of action (or the number of discrete action)\n",
    "    gpu_id: the gpu_id of the training device. Use CPU when cuda is not available.\n",
    "    args: the arguments for agent training. `args = Config()`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.gamma = args.gamma  # discount factor of future rewards\n",
    "        self.num_envs = args.num_envs  # the number of sub envs in vectorized env. `num_envs=1` in single env.\n",
    "        self.batch_size = args.batch_size  # num of transitions sampled from replay buffer.\n",
    "        self.repeat_times = args.repeat_times  # repeatedly update network using ReplayBuffer\n",
    "        self.reward_scale = args.reward_scale  # an approximate target reward usually be closed to 256\n",
    "        self.learning_rate = args.learning_rate  # the learning rate for network updating\n",
    "        self.if_off_policy = args.if_off_policy  # whether off-policy or on-policy of DRL algorithm\n",
    "        self.clip_grad_norm = args.clip_grad_norm  # clip the gradient after normalization\n",
    "        self.soft_update_tau = args.soft_update_tau  # the tau of soft target update `net = (1-tau)*net + net1`\n",
    "        self.state_value_tau = args.state_value_tau  # the tau of normalize for value and state\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.last_state = None  # last state of the trajectory for training. last_state.shape == (num_envs, state_dim)\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "\n",
    "        '''network'''\n",
    "        act_class = getattr(self, \"act_class\", None)\n",
    "        cri_class = getattr(self, \"cri_class\", None)\n",
    "        self.act = self.act_target = act_class(net_dims, state_dim, action_dim).to(self.device)\n",
    "        self.cri = self.cri_target = cri_class(net_dims, state_dim, action_dim).to(self.device) \\\n",
    "            if cri_class else self.act\n",
    "\n",
    "        '''optimizer'''\n",
    "        self.act_optimizer = torch.optim.AdamW(self.act.parameters(), self.learning_rate)\n",
    "        self.cri_optimizer = torch.optim.AdamW(self.cri.parameters(), self.learning_rate) \\\n",
    "            if cri_class else self.act_optimizer\n",
    "        from types import MethodType  # built-in package of Python3\n",
    "        self.act_optimizer.parameters = MethodType(get_optim_param, self.act_optimizer)\n",
    "        self.cri_optimizer.parameters = MethodType(get_optim_param, self.cri_optimizer)\n",
    "\n",
    "        \"\"\"attribute\"\"\"\n",
    "        if self.num_envs == 1:\n",
    "            self.explore_env = self.explore_one_env\n",
    "        else:\n",
    "            self.explore_env = self.explore_vec_env\n",
    "\n",
    "        self.if_use_per = getattr(args, 'if_use_per', None)  # use PER (Prioritized Experience Replay)\n",
    "        if self.if_use_per:\n",
    "            self.criterion = torch.nn.SmoothL1Loss(reduction=\"none\")\n",
    "            self.get_obj_critic = self.get_obj_critic_per\n",
    "        else:\n",
    "            self.criterion = torch.nn.SmoothL1Loss(reduction=\"mean\")\n",
    "            self.get_obj_critic = self.get_obj_critic_raw\n",
    "\n",
    "        \"\"\"save and load\"\"\"\n",
    "        self.save_attr_names = {'act', 'act_target', 'act_optimizer', 'cri', 'cri_target', 'cri_optimizer'}\n",
    "\n",
    "    def explore_one_env(self, env, horizon_len: int, if_random: bool = False) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Collect trajectories through the actor-environment interaction for a **single** environment instance.\n",
    "\n",
    "        env: RL training environment. env.reset() env.step(). It should be a vector env.\n",
    "        horizon_len: collect horizon_len step while exploring to update networks\n",
    "        if_random: uses random action for warn-up exploration\n",
    "        return: `(states, actions, rewards, undones)` for off-policy\n",
    "            num_envs == 1\n",
    "            states.shape == (horizon_len, num_envs, state_dim)\n",
    "            actions.shape == (horizon_len, num_envs, action_dim)\n",
    "            rewards.shape == (horizon_len, num_envs)\n",
    "            undones.shape == (horizon_len, num_envs)\n",
    "        \"\"\"\n",
    "        states = torch.zeros((horizon_len, self.num_envs, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len, self.num_envs, self.action_dim), dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.zeros((horizon_len, self.num_envs), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.zeros((horizon_len, self.num_envs), dtype=torch.bool).to(self.device)\n",
    "\n",
    "        state = self.last_state  # state.shape == (1, state_dim) for a single env.\n",
    "\n",
    "        get_action = self.act.get_action\n",
    "        for t in range(horizon_len):\n",
    "            action = torch.rand(1, self.action_dim) * 2 - 1.0 if if_random else get_action(state)\n",
    "            states[t] = state\n",
    "\n",
    "            ary_action = action[0].detach().cpu().numpy()\n",
    "            ary_state, reward, done, _ = env.step(ary_action)  # next_state\n",
    "            ary_state = env.reset() if done else ary_state  # ary_state.shape == (state_dim, )\n",
    "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            actions[t] = action\n",
    "            rewards[t] = reward\n",
    "            dones[t] = done\n",
    "\n",
    "        self.last_state = state  # state.shape == (1, state_dim) for a single env.\n",
    "\n",
    "        rewards *= self.reward_scale\n",
    "        undones = 1.0 - dones.type(torch.float32)\n",
    "        return states, actions, rewards, undones\n",
    "\n",
    "    def explore_vec_env(self, env, horizon_len: int, if_random: bool = False) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Collect trajectories through the actor-environment interaction for a **vectorized** environment instance.\n",
    "\n",
    "        env: RL training environment. env.reset() env.step(). It should be a vector env.\n",
    "        horizon_len: collect horizon_len step while exploring to update networks\n",
    "        if_random: uses random action for warn-up exploration\n",
    "        return: `(states, actions, rewards, undones)` for off-policy\n",
    "            states.shape == (horizon_len, num_envs, state_dim)\n",
    "            actions.shape == (horizon_len, num_envs, action_dim)\n",
    "            rewards.shape == (horizon_len, num_envs)\n",
    "            undones.shape == (horizon_len, num_envs)\n",
    "        \"\"\"\n",
    "        states = torch.zeros((horizon_len, self.num_envs, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len, self.num_envs, self.action_dim), dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.zeros((horizon_len, self.num_envs), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.zeros((horizon_len, self.num_envs), dtype=torch.bool).to(self.device)\n",
    "\n",
    "        state = self.last_state  # last_state.shape == (num_envs, state_dim)\n",
    "        get_action = self.act.get_action\n",
    "        for t in range(horizon_len):\n",
    "            action = torch.rand(self.num_envs, self.action_dim) * 2 - 1.0 if if_random \\\n",
    "                else get_action(state).detach()\n",
    "            states[t] = state  # state.shape == (num_envs, state_dim)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)  # next_state\n",
    "            actions[t] = action\n",
    "            rewards[t] = reward\n",
    "            dones[t] = done\n",
    "\n",
    "        self.last_state = state\n",
    "\n",
    "        rewards *= self.reward_scale\n",
    "        undones = 1.0 - dones.type(torch.float32)\n",
    "        return states, actions, rewards, undones\n",
    "\n",
    "    def update_net(self, buffer: Union[ReplayBuffer, tuple]) -> Tuple[float, ...]:\n",
    "        obj_critic = 0.0  # criterion(q_value, q_label).mean().item()\n",
    "        obj_actor = 0.0  # q_value.mean().item()\n",
    "        assert isinstance(buffer, ReplayBuffer) or isinstance(buffer, tuple)\n",
    "        assert isinstance(self.batch_size, int)\n",
    "        assert isinstance(self.repeat_times, int)\n",
    "        assert isinstance(self.reward_scale, float)\n",
    "        return obj_critic, obj_actor\n",
    "\n",
    "    def get_obj_critic_raw(self, buffer: ReplayBuffer, batch_size: int) -> Tuple[Tensor, Tensor]:\n",
    "        with torch.no_grad():\n",
    "            states, actions, rewards, undones, next_ss = buffer.sample(batch_size)  # next_ss: next states\n",
    "            next_as = self.act_target(next_ss)  # next actions\n",
    "            next_qs = self.cri_target(next_ss, next_as)  # next q values\n",
    "            q_labels = rewards + undones * self.gamma * next_qs\n",
    "\n",
    "        q_values = self.cri(states, actions)\n",
    "        obj_critic = self.criterion(q_values, q_labels)\n",
    "        return obj_critic, states\n",
    "\n",
    "    def get_obj_critic_per(self, buffer: ReplayBuffer, batch_size: int) -> Tuple[Tensor, Tensor]:\n",
    "        with torch.no_grad():\n",
    "            states, actions, rewards, undones, next_ss, is_weights, is_indices = buffer.sample_for_per(batch_size)\n",
    "            # is_weights, is_indices: important sampling `weights, indices` by Prioritized Experience Replay (PER)\n",
    "\n",
    "            next_as = self.act_target(next_ss)\n",
    "            next_qs = self.cri_target(next_ss, next_as)\n",
    "            q_labels = rewards + undones * self.gamma * next_qs\n",
    "\n",
    "        q_values = self.cri(states, actions)\n",
    "        td_errors = self.criterion(q_values, q_labels)\n",
    "        obj_critic = (td_errors * is_weights).mean()\n",
    "\n",
    "        buffer.td_error_update_for_per(is_indices.detach(), td_errors.detach())\n",
    "        return obj_critic, states\n",
    "\n",
    "    def get_cumulative_rewards(self, rewards: Tensor, undones: Tensor) -> Tensor:\n",
    "        returns = torch.empty_like(rewards)\n",
    "\n",
    "        masks = undones * self.gamma\n",
    "        horizon_len = rewards.shape[0]\n",
    "\n",
    "        last_state = self.last_state\n",
    "        next_action = self.act_target(last_state)\n",
    "        next_value = self.cri_target(last_state, next_action).detach()\n",
    "        for t in range(horizon_len - 1, -1, -1):\n",
    "            returns[t] = next_value = rewards[t] + masks[t] * next_value\n",
    "        return returns\n",
    "\n",
    "    def optimizer_update(self, optimizer: torch.optim, objective: Tensor):\n",
    "        \"\"\"minimize the optimization objective via update the network parameters\n",
    "\n",
    "        optimizer: `optimizer = torch.optim.SGD(net.parameters(), learning_rate)`\n",
    "        objective: `objective = net(...)` the optimization objective, sometimes is a loss function.\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        objective.backward()\n",
    "        clip_grad_norm_(parameters=optimizer.param_groups[0][\"params\"], max_norm=self.clip_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "    def optimizer_update_amp(self, optimizer: torch.optim, objective: Tensor):  # automatic mixed precision\n",
    "        \"\"\"minimize the optimization objective via update the network parameters\n",
    "\n",
    "        amp: Automatic Mixed Precision\n",
    "\n",
    "        optimizer: `optimizer = torch.optim.SGD(net.parameters(), learning_rate)`\n",
    "        objective: `objective = net(...)` the optimization objective, sometimes is a loss function.\n",
    "        \"\"\"\n",
    "        amp_scale = torch.cuda.amp.GradScaler()  # write in __init__()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        amp_scale.scale(objective).backward()  # loss.backward()\n",
    "        amp_scale.unscale_(optimizer)  # amp\n",
    "\n",
    "        # from torch.nn.utils import clip_grad_norm_\n",
    "        clip_grad_norm_(parameters=optimizer.param_groups[0][\"params\"], max_norm=self.clip_grad_norm)\n",
    "        amp_scale.step(optimizer)  # optimizer.step()\n",
    "        amp_scale.update()  # optimizer.step()\n",
    "\n",
    "    def update_avg_std_for_normalization(self, states: Tensor, returns: Tensor):\n",
    "        tau = self.state_value_tau\n",
    "        if tau == 0:\n",
    "            return\n",
    "\n",
    "        state_avg = states.mean(dim=0, keepdim=True)\n",
    "        state_std = states.std(dim=0, keepdim=True)\n",
    "        self.act.state_avg[:] = self.act.state_avg * (1 - tau) + state_avg * tau\n",
    "        self.act.state_std[:] = self.cri.state_std * (1 - tau) + state_std * tau + 1e-4\n",
    "        self.cri.state_avg[:] = self.act.state_avg\n",
    "        self.cri.state_std[:] = self.act.state_std\n",
    "\n",
    "        returns_avg = returns.mean(dim=0)\n",
    "        returns_std = returns.std(dim=0)\n",
    "        self.cri.value_avg[:] = self.cri.value_avg * (1 - tau) + returns_avg * tau\n",
    "        self.cri.value_std[:] = self.cri.value_std * (1 - tau) + returns_std * tau + 1e-4\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(target_net: torch.nn.Module, current_net: torch.nn.Module, tau: float):\n",
    "        \"\"\"soft update target network via current network\n",
    "\n",
    "        target_net: update target network via current network to make training more stable.\n",
    "        current_net: current network update via an optimizer\n",
    "        tau: tau of soft target update: `target_net = target_net * (1-tau) + current_net * tau`\n",
    "        \"\"\"\n",
    "        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n",
    "            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))\n",
    "\n",
    "    def save_or_load_agent(self, cwd: str, if_save: bool):\n",
    "        \"\"\"save or load training files for Agent\n",
    "\n",
    "        cwd: Current Working Directory. ElegantRL save training files in CWD.\n",
    "        if_save: True: save files. False: load files.\n",
    "        \"\"\"\n",
    "        assert self.save_attr_names.issuperset({'act', 'act_target', 'act_optimizer'})\n",
    "\n",
    "        for attr_name in self.save_attr_names:\n",
    "            file_path = f\"{cwd}/{attr_name}.pth\"\n",
    "            if if_save:\n",
    "                torch.save(getattr(self, attr_name), file_path)\n",
    "            elif os.path.isfile(file_path):\n",
    "                setattr(self, attr_name, torch.load(file_path, map_location=self.device))\n",
    "\n",
    "\n",
    "def get_optim_param(optimizer: torch.optim) -> list:  # backup\n",
    "    params_list = []\n",
    "    for params_dict in optimizer.state_dict()[\"state\"].values():\n",
    "        params_list.extend([t for t in params_dict.values() if isinstance(t, torch.Tensor)])\n",
    "    return params_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(dims: [int], activation: nn = None, if_raw_out: bool = True) -> nn.Sequential:\n",
    "    \"\"\"\n",
    "    build MLP (MultiLayer Perceptron)\n",
    "\n",
    "    dims: the middle dimension, `dims[-1]` is the output dimension of this network\n",
    "    activation: the activation function\n",
    "    if_remove_out_layer: if remove the activation function of the output layer.\n",
    "    \"\"\"\n",
    "    if activation is None:\n",
    "        activation = nn.ReLU\n",
    "    net_list = []\n",
    "    for i in range(len(dims) - 1):\n",
    "        net_list.extend([nn.Linear(dims[i], dims[i + 1]), activation()])\n",
    "    if if_raw_out:\n",
    "        del net_list[-1]  # delete the activation function of the output layer to keep raw output\n",
    "    return nn.Sequential(*net_list)\n",
    "\n",
    "\n",
    "def layer_init_with_orthogonal(layer, std=1.0, bias_const=1e-6):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "\n",
    "class QNetBase(nn.Module):  # nn.Module is a standard PyTorch Network\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        self.explore_rate = 0.125\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.net = None  # build_mlp(dims=[state_dim + action_dim, *dims, 1])\n",
    "\n",
    "        self.state_avg = nn.Parameter(torch.zeros((state_dim,)), requires_grad=False)\n",
    "        self.state_std = nn.Parameter(torch.ones((state_dim,)), requires_grad=False)\n",
    "        self.value_avg = nn.Parameter(torch.zeros((1,)), requires_grad=False)\n",
    "        self.value_std = nn.Parameter(torch.ones((1,)), requires_grad=False)\n",
    "\n",
    "    def state_norm(self, state: Tensor) -> Tensor:\n",
    "        return (state - self.state_avg) / self.state_std\n",
    "\n",
    "    def value_re_norm(self, value: Tensor) -> Tensor:\n",
    "        return value * self.value_std + self.value_avg\n",
    "\n",
    "\n",
    "class QNet(QNetBase):\n",
    "    def __init__(self, dims: [int], state_dim: int, action_dim: int):\n",
    "        super().__init__(state_dim=state_dim, action_dim=action_dim)\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, action_dim])\n",
    "        layer_init_with_orthogonal(self.net[-1], std=0.1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self.state_norm(state)\n",
    "        value = self.net(state)\n",
    "        value = self.value_re_norm(value)\n",
    "        return value  # Q values for multiple actions\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = self.state_norm(state)\n",
    "        if self.explore_rate < torch.rand(1):\n",
    "            action = self.net(state).argmax(dim=1, keepdim=True)\n",
    "        else:\n",
    "            action = torch.randint(self.action_dim, size=(state.shape[0], 1))\n",
    "        return action\n",
    "\n",
    "\n",
    "class QNetDuel(QNetBase):  # Dueling DQN\n",
    "    def __init__(self, dims: [int], state_dim: int, action_dim: int):\n",
    "        super().__init__(state_dim=state_dim, action_dim=action_dim)\n",
    "        self.net_state = build_mlp(dims=[state_dim, *dims])\n",
    "        self.net_adv = build_mlp(dims=[dims[-1], 1])  # advantage value\n",
    "        self.net_val = build_mlp(dims=[dims[-1], action_dim])  # Q value\n",
    "\n",
    "        layer_init_with_orthogonal(self.net_adv[-1], std=0.1)\n",
    "        layer_init_with_orthogonal(self.net_val[-1], std=0.1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self.state_norm(state)\n",
    "        s_enc = self.net_state(state)  # encoded state\n",
    "        q_val = self.net_val(s_enc)  # q value\n",
    "        q_adv = self.net_adv(s_enc)  # advantage value\n",
    "        value = q_val - q_val.mean(dim=1, keepdim=True) + q_adv  # dueling Q value\n",
    "        value = self.value_re_norm(value)\n",
    "        return value\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = self.state_norm(state)\n",
    "        if self.explore_rate < torch.rand(1):\n",
    "            s_enc = self.net_state(state)  # encoded state\n",
    "            q_val = self.net_val(s_enc)  # q value\n",
    "            action = q_val.argmax(dim=1, keepdim=True)\n",
    "        else:\n",
    "            action = torch.randint(self.action_dim, size=(state.shape[0], 1))\n",
    "        return action\n",
    "\n",
    "\n",
    "class QNetTwin(QNetBase):  # Double DQN\n",
    "    def __init__(self, dims: [int], state_dim: int, action_dim: int):\n",
    "        super().__init__(state_dim=state_dim, action_dim=action_dim)\n",
    "        self.net_state = build_mlp(dims=[state_dim, *dims])\n",
    "        self.net_val1 = build_mlp(dims=[dims[-1], action_dim])  # Q value 1\n",
    "        self.net_val2 = build_mlp(dims=[dims[-1], action_dim])  # Q value 2\n",
    "        self.soft_max = nn.Softmax(dim=1)\n",
    "\n",
    "        layer_init_with_orthogonal(self.net_val1[-1], std=0.1)\n",
    "        layer_init_with_orthogonal(self.net_val2[-1], std=0.1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self.state_norm(state)\n",
    "        s_enc = self.net_state(state)  # encoded state\n",
    "        q_val = self.net_val1(s_enc)  # q value\n",
    "        return q_val  # one group of Q values\n",
    "\n",
    "    def get_q1_q2(self, state):\n",
    "        state = self.state_norm(state)\n",
    "        s_enc = self.net_state(state)  # encoded state\n",
    "        q_val1 = self.net_val1(s_enc)  # q value 1\n",
    "        q_val1 = self.value_re_norm(q_val1)\n",
    "        q_val2 = self.net_val2(s_enc)  # q value 2\n",
    "        q_val2 = self.value_re_norm(q_val2)\n",
    "        return q_val1, q_val2  # two groups of Q values\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = self.state_norm(state)\n",
    "        s_enc = self.net_state(state)  # encoded state\n",
    "        q_val = self.net_val1(s_enc)  # q value\n",
    "        if self.explore_rate < torch.rand(1):\n",
    "            action = q_val.argmax(dim=1, keepdim=True)\n",
    "        else:\n",
    "            a_prob = self.soft_max(q_val)\n",
    "            action = torch.multinomial(a_prob, num_samples=1)\n",
    "        return action\n",
    "\n",
    "\n",
    "class QNetTwinDuel(QNetBase):  # D3QN: Dueling Double DQN\n",
    "    def __init__(self, dims: [int], state_dim: int, action_dim: int):\n",
    "        super().__init__(state_dim=state_dim, action_dim=action_dim)\n",
    "        self.net_state = build_mlp(dims=[state_dim, *dims])\n",
    "        self.net_adv1 = build_mlp(dims=[dims[-1], 1])  # advantage value 1\n",
    "        self.net_val1 = build_mlp(dims=[dims[-1], action_dim])  # Q value 1\n",
    "        self.net_adv2 = build_mlp(dims=[dims[-1], 1])  # advantage value 2\n",
    "        self.net_val2 = build_mlp(dims=[dims[-1], action_dim])  # Q value 2\n",
    "        self.soft_max = nn.Softmax(dim=1)\n",
    "\n",
    "        layer_init_with_orthogonal(self.net_adv1[-1], std=0.1)\n",
    "        layer_init_with_orthogonal(self.net_val1[-1], std=0.1)\n",
    "        layer_init_with_orthogonal(self.net_adv2[-1], std=0.1)\n",
    "        layer_init_with_orthogonal(self.net_val2[-1], std=0.1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = self.state_norm(state)\n",
    "        s_enc = self.net_state(state)  # encoded state\n",
    "        q_val = self.net_val1(s_enc)  # q value\n",
    "        q_adv = self.net_adv1(s_enc)  # advantage value\n",
    "        value = q_val - q_val.mean(dim=1, keepdim=True) + q_adv  # one dueling Q value\n",
    "        value = self.value_re_norm(value)\n",
    "        return value\n",
    "\n",
    "    def get_q1_q2(self, state):\n",
    "        state = self.state_norm(state)\n",
    "        s_enc = self.net_state(state)  # encoded state\n",
    "\n",
    "        q_val1 = self.net_val1(s_enc)  # q value 1\n",
    "        q_adv1 = self.net_adv1(s_enc)  # advantage value 1\n",
    "        q_duel1 = q_val1 - q_val1.mean(dim=1, keepdim=True) + q_adv1\n",
    "        q_duel1 = self.value_re_norm(q_duel1)\n",
    "\n",
    "        q_val2 = self.net_val2(s_enc)  # q value 2\n",
    "        q_adv2 = self.net_adv2(s_enc)  # advantage value 2\n",
    "        q_duel2 = q_val2 - q_val2.mean(dim=1, keepdim=True) + q_adv2\n",
    "        q_duel2 = self.value_re_norm(q_duel2)\n",
    "        return q_duel1, q_duel2  # two dueling Q values\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = self.state_norm(state)\n",
    "        s_enc = self.net_state(state)  # encoded state\n",
    "        q_val = self.net_val1(s_enc)  # q value\n",
    "        if self.explore_rate < torch.rand(1):\n",
    "            action = q_val.argmax(dim=1, keepdim=True)\n",
    "        else:\n",
    "            a_prob = self.soft_max(q_val)\n",
    "            action = torch.multinomial(a_prob, num_samples=1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDQN(AgentBase):\n",
    "    \"\"\"\n",
    "    Deep Q-Network algorithm. “Human-Level Control Through Deep Reinforcement Learning”. Mnih V. et al.. 2015.\n",
    "\n",
    "    net_dims: the middle layer dimension of MLP (MultiLayer Perceptron)\n",
    "    state_dim: the dimension of state (the number of state vector)\n",
    "    action_dim: the dimension of action (or the number of discrete action)\n",
    "    gpu_id: the gpu_id of the training device. Use CPU when cuda is not available.\n",
    "    args: the arguments for agent training. `args = Config()`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.act_class = getattr(self, \"act_class\", QNet)\n",
    "        self.cri_class = None  # means `self.cri = self.act`\n",
    "        super().__init__(net_dims=net_dims, state_dim=state_dim, action_dim=action_dim, gpu_id=gpu_id, args=args)\n",
    "        self.act_target = self.cri_target = deepcopy(self.act)\n",
    "\n",
    "        self.act.explore_rate = getattr(args, \"explore_rate\", 0.25)\n",
    "        # Using ϵ-greedy to select uniformly random actions for exploration with `explore_rate` probability.\n",
    "\n",
    "    def explore_one_env(self, env, horizon_len: int, if_random: bool = False) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Collect trajectories through the actor-environment interaction for a **single** environment instance.\n",
    "\n",
    "        env: RL training environment. env.reset() env.step(). It should be a vector env.\n",
    "        horizon_len: collect horizon_len step while exploring to update networks\n",
    "        if_random: uses random action for warn-up exploration\n",
    "        return: `(states, actions, rewards, undones)` for off-policy\n",
    "            num_envs == 1\n",
    "            states.shape == (horizon_len, num_envs, state_dim)\n",
    "            actions.shape == (horizon_len, num_envs, action_dim)\n",
    "            rewards.shape == (horizon_len, num_envs)\n",
    "            undones.shape == (horizon_len, num_envs)\n",
    "        \"\"\"\n",
    "        states = torch.zeros((horizon_len, self.num_envs, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len, self.num_envs, 1), dtype=torch.int32).to(self.device)  # different\n",
    "        rewards = torch.zeros((horizon_len, self.num_envs), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.zeros((horizon_len, self.num_envs), dtype=torch.bool).to(self.device)\n",
    "\n",
    "        state = self.last_state  # state.shape == (1, state_dim) for a single env.\n",
    "\n",
    "        get_action = self.act.get_action\n",
    "        for t in range(horizon_len):\n",
    "            action = torch.randint(self.action_dim, size=(1, 1)) if if_random else get_action(state)  # different\n",
    "            states[t] = state\n",
    "\n",
    "            ary_action = action[0, 0].detach().cpu().numpy()\n",
    "            ary_state, reward, done, _ = env.step(ary_action)  # next_state\n",
    "            ary_state = env.reset() if done else ary_state  # ary_state.shape == (state_dim, )\n",
    "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            actions[t] = action\n",
    "            rewards[t] = reward\n",
    "            dones[t] = done\n",
    "\n",
    "        self.last_state = state  # state.shape == (1, state_dim) for a single env.\n",
    "\n",
    "        rewards *= self.reward_scale\n",
    "        undones = 1.0 - dones.type(torch.float32)\n",
    "        return states, actions, rewards, undones\n",
    "\n",
    "    def explore_vec_env(self, env, horizon_len: int, if_random: bool = False) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Collect trajectories through the actor-environment interaction for a **vectorized** environment instance.\n",
    "\n",
    "        env: RL training environment. env.reset() env.step(). It should be a vector env.\n",
    "        horizon_len: collect horizon_len step while exploring to update networks\n",
    "        if_random: uses random action for warn-up exploration\n",
    "        return: `(states, actions, rewards, undones)` for off-policy\n",
    "            states.shape == (horizon_len, num_envs, state_dim)\n",
    "            actions.shape == (horizon_len, num_envs, action_dim)\n",
    "            rewards.shape == (horizon_len, num_envs)\n",
    "            undones.shape == (horizon_len, num_envs)\n",
    "        \"\"\"\n",
    "        states = torch.zeros((horizon_len, self.num_envs, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len, self.num_envs, 1), dtype=torch.int32).to(self.device)  # different\n",
    "        rewards = torch.zeros((horizon_len, self.num_envs), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.zeros((horizon_len, self.num_envs), dtype=torch.bool).to(self.device)\n",
    "\n",
    "        state = self.last_state  # last_state.shape = (num_envs, state_dim) for a vectorized env.\n",
    "\n",
    "        get_action = self.act.get_action\n",
    "        for t in range(horizon_len):\n",
    "            action = torch.randint(self.action_dim, size=(self.num_envs, 1)) if if_random \\\n",
    "                else get_action(state).detach()  # different\n",
    "            states[t] = state\n",
    "\n",
    "            state, reward, done, _ = env.step(action)  # next_state\n",
    "            actions[t] = action\n",
    "            rewards[t] = reward\n",
    "            dones[t] = done\n",
    "\n",
    "        self.last_state = state\n",
    "\n",
    "        rewards *= self.reward_scale\n",
    "        undones = 1.0 - dones.type(torch.float32)\n",
    "        return states, actions, rewards, undones\n",
    "\n",
    "    def update_net(self, buffer: ReplayBuffer) -> Tuple[float, ...]:\n",
    "        with torch.no_grad():\n",
    "            states, actions, rewards, undones = buffer.add_item\n",
    "            self.update_avg_std_for_normalization(\n",
    "                states=states.reshape((-1, self.state_dim)),\n",
    "                returns=self.get_cumulative_rewards(rewards=rewards, undones=undones).reshape((-1,))\n",
    "            )\n",
    "\n",
    "        '''update network'''\n",
    "        obj_critics = 0.0\n",
    "        obj_actors = 0.0\n",
    "\n",
    "        update_times = int(buffer.add_size * self.repeat_times)\n",
    "        assert update_times >= 1\n",
    "        for _ in range(update_times):\n",
    "            obj_critic, q_value = self.get_obj_critic(buffer, self.batch_size)\n",
    "            obj_critics += obj_critic.item()\n",
    "            obj_actors += q_value.mean().item()\n",
    "            self.optimizer_update(self.cri_optimizer, obj_critic)\n",
    "            self.soft_update(self.cri_target, self.cri, self.soft_update_tau)\n",
    "        return obj_critics / update_times, obj_actors / update_times\n",
    "\n",
    "    def get_obj_critic_raw(self, buffer: ReplayBuffer, batch_size: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate the loss of the network and predict Q values with **uniform sampling**.\n",
    "\n",
    "        :param buffer: the ReplayBuffer instance that stores the trajectories.\n",
    "        :param batch_size: the size of batch data for Stochastic Gradient Descent (SGD).\n",
    "        :return: the loss of the network and Q values.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            states, actions, rewards, undones, next_ss = buffer.sample(batch_size)  # next_ss: next states\n",
    "            next_qs = self.cri_target(next_ss).max(dim=1, keepdim=True)[0].squeeze(1)  # next q_values\n",
    "            q_labels = rewards + undones * self.gamma * next_qs\n",
    "\n",
    "        q_values = self.cri(states).gather(1, actions.long()).squeeze(1)\n",
    "        obj_critic = self.criterion(q_values, q_labels)\n",
    "        return obj_critic, q_values\n",
    "\n",
    "    def get_obj_critic_per(self, buffer: ReplayBuffer, batch_size: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate the loss of the network and predict Q values with **Prioritized Experience Replay (PER)**.\n",
    "\n",
    "        :param buffer: the ReplayBuffer instance that stores the trajectories.\n",
    "        :param batch_size: the size of batch data for Stochastic Gradient Descent (SGD).\n",
    "        :return: the loss of the network and Q values.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            states, actions, rewards, undones, next_ss, is_weights, is_indices = buffer.sample_for_per(batch_size)\n",
    "            # is_weights, is_indices: important sampling `weights, indices` by Prioritized Experience Replay (PER)\n",
    "\n",
    "            next_qs = self.cri_target(next_ss).max(dim=1, keepdim=True)[0].squeeze(1)  # q values in next step\n",
    "            q_labels = rewards + undones * self.gamma * next_qs\n",
    "\n",
    "        q_values = self.cri(states).gather(1, actions.long()).squeeze(1)\n",
    "        td_errors = self.criterion(q_values, q_labels)  # or td_error = (q_value - q_label).abs()\n",
    "        obj_critic = (td_errors * is_weights).mean()\n",
    "\n",
    "        buffer.td_error_update_for_per(is_indices.detach(), td_errors.detach())\n",
    "        return obj_critic, q_values\n",
    "\n",
    "    def get_cumulative_rewards(self, rewards: Tensor, undones: Tensor) -> Tensor:\n",
    "        returns = torch.empty_like(rewards)\n",
    "\n",
    "        masks = undones * self.gamma\n",
    "        horizon_len = rewards.shape[0]\n",
    "\n",
    "        last_state = self.last_state\n",
    "        next_value = self.act_target(last_state).argmax(dim=1).detach()  # actor is Q Network in DQN style\n",
    "        for t in range(horizon_len - 1, -1, -1):\n",
    "            returns[t] = next_value = rewards[t] + masks[t] * next_value\n",
    "        return returns\n",
    "\n",
    "\n",
    "class AgentDoubleDQN(AgentDQN):\n",
    "    \"\"\"\n",
    "    Double Deep Q-Network algorithm. “Deep Reinforcement Learning with Double Q-learning”. H. V. Hasselt et al.. 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.act_class = getattr(self, \"act_class\", QNetTwin)\n",
    "        self.cri_class = getattr(self, \"cri_class\", None)  # means `self.cri = self.act`\n",
    "        super().__init__(net_dims=net_dims, state_dim=state_dim, action_dim=action_dim, gpu_id=gpu_id, args=args)\n",
    "\n",
    "    def get_obj_critic_raw(self, buffer: ReplayBuffer, batch_size: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate the loss of the network and predict Q values with **uniform sampling**.\n",
    "\n",
    "        :param buffer: the ReplayBuffer instance that stores the trajectories.\n",
    "        :param batch_size: the size of batch data for Stochastic Gradient Descent (SGD).\n",
    "        :return: the loss of the network and Q values.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            states, actions, rewards, undones, next_ss = buffer.sample(batch_size)\n",
    "\n",
    "            next_qs = torch.min(*self.cri_target.get_q1_q2(next_ss)).max(dim=1, keepdim=True)[0].squeeze(1)\n",
    "            q_labels = rewards + undones * self.gamma * next_qs\n",
    "\n",
    "        q1, q2 = [qs.gather(1, actions.long()).squeeze(1) for qs in self.act.get_q1_q2(states)]\n",
    "        obj_critic = self.criterion(q1, q_labels) + self.criterion(q2, q_labels)\n",
    "        return obj_critic, q1\n",
    "\n",
    "    def get_obj_critic_per(self, buffer: ReplayBuffer, batch_size: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate the loss of the network and predict Q values with **Prioritized Experience Replay (PER)**.\n",
    "\n",
    "        :param buffer: the ReplayBuffer instance that stores the trajectories.\n",
    "        :param batch_size: the size of batch data for Stochastic Gradient Descent (SGD).\n",
    "        :return: the loss of the network and Q values.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            states, actions, rewards, undones, next_ss, is_weights, is_indices = buffer.sample_for_per(batch_size)\n",
    "\n",
    "            next_qs = torch.min(*self.cri_target.get_q1_q2(next_ss)).max(dim=1, keepdim=True)[0].squeeze(1)\n",
    "            q_labels = rewards + undones * self.gamma * next_qs\n",
    "\n",
    "        q1, q2 = [qs.gather(1, actions.long()).squeeze(1) for qs in self.act.get_q1_q2(states)]\n",
    "        td_errors = self.criterion(q1, q_labels) + self.criterion(q2, q_labels)\n",
    "        obj_critic = (td_errors * is_weights).mean()\n",
    "\n",
    "        buffer.td_error_update_for_per(is_indices.detach(), td_errors.detach())\n",
    "        return obj_critic, q1\n",
    "\n",
    "\n",
    "'''add dueling q network'''\n",
    "\n",
    "\n",
    "class AgentDuelingDQN(AgentDQN):\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.act_class = getattr(self, \"act_class\", QNetDuel)\n",
    "        self.cri_class = getattr(self, \"cri_class\", None)  # means `self.cri = self.act`\n",
    "        super().__init__(net_dims=net_dims, state_dim=state_dim, action_dim=action_dim, gpu_id=gpu_id, args=args)\n",
    "\n",
    "\n",
    "class AgentD3QN(AgentDoubleDQN):  # Dueling Double Deep Q Network. (D3QN)\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.act_class = getattr(self, \"act_class\", QNetTwinDuel)\n",
    "        self.cri_class = getattr(self, \"cri_class\", None)  # means `self.cri = self.act`\n",
    "        super().__init__(net_dims=net_dims, state_dim=state_dim, action_dim=action_dim, gpu_id=gpu_id, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing(object):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.env_name = 'cyborg'\n",
    "        self.num_envs = 1\n",
    "        self.max_step = 100\n",
    "        self.if_discrete = True\n",
    "\n",
    "    def reset(self):\n",
    "        if self.env_name == 'cyborg':\n",
    "            obs = self.env.reset()\n",
    "        else:\n",
    "            obs, info = self.env.reset()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.env_name == 'cyborg':\n",
    "           obs, reward, done, info = self.env.step(action) \n",
    "        else:\n",
    "            obs, reward, done, _, info = self.env.step(action)\n",
    "        return obs, reward, done, info\n",
    "\n",
    "\n",
    "def build_env(env_class=None, env_args: dict = None, gpu_id: int = -1):\n",
    "    # return preprocessing(gym.make('CartPole-v1'))\n",
    "    return preprocessing(ChallengeWrapper(env=CybORG(path,'sim', agents={'Red': RedMeanderAgent}), agent_name=\"Blue\", max_steps=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, cwd: str, env, args: Config, if_tensorboard: bool = False):\n",
    "        self.cwd = cwd  # current working directory to save model\n",
    "        self.env = env  # the env for Evaluator, `eval_env = env` in default\n",
    "        self.agent_id = args.gpu_id\n",
    "        self.total_step = 0  # the total training step\n",
    "        self.start_time = time.time()  # `used_time = time.time() - self.start_time`\n",
    "        self.eval_times = args.eval_times  # number of times that get episodic cumulative return\n",
    "        self.eval_per_step = args.eval_per_step  # evaluate the agent per training steps\n",
    "        self.eval_step_counter = -self.eval_per_step  # `self.total_step > self.eval_step_counter + self.eval_per_step`\n",
    "\n",
    "        self.save_gap = args.save_gap\n",
    "        self.save_counter = 0\n",
    "        self.if_keep_save = args.if_keep_save\n",
    "        self.if_over_write = args.if_over_write\n",
    "\n",
    "        self.recorder_path = f'{cwd}/recorder.npy'\n",
    "        self.recorder = []  # total_step, r_avg, r_std, obj_c, ...\n",
    "        self.max_r = -np.inf\n",
    "        print(\"| Evaluator:\"\n",
    "              \"\\n| `step`: Number of samples, or total training steps, or running times of `env.step()`.\"\n",
    "              \"\\n| `time`: Time spent from the start of training to this moment.\"\n",
    "              \"\\n| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              \"\\n| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              \"\\n| `avgS`: Average of steps in an episode.\"\n",
    "              \"\\n| `objC`: Objective of Critic network. Or call it loss function of critic network.\"\n",
    "              \"\\n| `objA`: Objective of Actor network. It is the average Q value of the critic network.\"\n",
    "              f\"\\n{'#' * 80}\\n\"\n",
    "              f\"{'ID':<3}{'Step':>8}{'Time':>8} |\"\n",
    "              f\"{'avgR':>8}{'stdR':>7}{'avgS':>7}{'stdS':>6} |\"\n",
    "              f\"{'expR':>8}{'objC':>7}{'objA':>7}{'etc.':>7}\")\n",
    "\n",
    "        if getattr(env, 'num_envs', 1) == 1:  # get attribute\n",
    "            self.get_cumulative_rewards_and_step = self.get_cumulative_rewards_and_step_single_env\n",
    "        else:  # vectorized environment\n",
    "            self.get_cumulative_rewards_and_step = self.get_cumulative_rewards_and_step_vectorized_env\n",
    "\n",
    "        if if_tensorboard:\n",
    "            from torch.utils.tensorboard import SummaryWriter\n",
    "            self.tensorboard = SummaryWriter(f\"{cwd}/tensorboard\")\n",
    "        else:\n",
    "            self.tensorboard = None\n",
    "\n",
    "    def evaluate_and_save(self, actor: torch.nn, steps: int, exp_r: float, logging_tuple: tuple):\n",
    "        self.total_step += steps  # update total training steps\n",
    "        if self.total_step < self.eval_step_counter + self.eval_per_step:\n",
    "            return\n",
    "\n",
    "        self.eval_step_counter = self.total_step\n",
    "\n",
    "        rewards_step_ten = self.get_cumulative_rewards_and_step(actor)\n",
    "        returns = rewards_step_ten[:, 0]  # episodic cumulative returns of an\n",
    "        steps = rewards_step_ten[:, 1]  # episodic step number\n",
    "        avg_r = returns.mean().item()\n",
    "        std_r = returns.std().item()\n",
    "        avg_s = steps.mean().item()\n",
    "        std_s = steps.std().item()\n",
    "\n",
    "        train_time = int(time.time() - self.start_time)\n",
    "\n",
    "        '''record the training information'''\n",
    "        self.recorder.append((self.total_step, avg_r, std_r, exp_r, *logging_tuple))  # update recorder\n",
    "        if self.tensorboard:\n",
    "            self.tensorboard.add_scalar(\"info/critic_loss_sample\", logging_tuple[0], self.total_step)\n",
    "            self.tensorboard.add_scalar(\"info/actor_obj_sample\", -1 * logging_tuple[1], self.total_step)\n",
    "            self.tensorboard.add_scalar(\"reward/avg_reward_sample\", avg_r, self.total_step)\n",
    "            self.tensorboard.add_scalar(\"reward/std_reward_sample\", std_r, self.total_step)\n",
    "            self.tensorboard.add_scalar(\"reward/exp_reward_sample\", exp_r, self.total_step)\n",
    "\n",
    "            self.tensorboard.add_scalar(\"info/critic_loss_time\", logging_tuple[0], train_time)\n",
    "            self.tensorboard.add_scalar(\"info/actor_obj_time\", -1 * logging_tuple[1], train_time)\n",
    "            self.tensorboard.add_scalar(\"reward/avg_reward_time\", avg_r, train_time)\n",
    "            self.tensorboard.add_scalar(\"reward/std_reward_time\", std_r, train_time)\n",
    "            self.tensorboard.add_scalar(\"reward/exp_reward_time\", exp_r, train_time)\n",
    "\n",
    "        '''print some information to Terminal'''\n",
    "        prev_max_r = self.max_r\n",
    "        self.max_r = max(self.max_r, avg_r)  # update max average cumulative rewards\n",
    "        print(f\"{self.agent_id:<3}{self.total_step:8.2e}{train_time:8.0f} |\"\n",
    "              f\"{avg_r:8.2f}{std_r:7.1f}{avg_s:7.0f}{std_s:6.0f} |\"\n",
    "              f\"{exp_r:8.2f}{''.join(f'{n:7.2f}' for n in logging_tuple)}\")\n",
    "\n",
    "        if_save = avg_r > prev_max_r\n",
    "        if if_save:\n",
    "            self.save_training_curve_jpg()\n",
    "        if not self.if_keep_save:\n",
    "            return\n",
    "\n",
    "        self.save_counter += 1\n",
    "        actor_path = None\n",
    "        if if_save:  # save checkpoint with the highest episode return\n",
    "            if self.if_over_write:\n",
    "                actor_path = f\"{self.cwd}/actor.pt\"\n",
    "            else:\n",
    "                actor_path = f\"{self.cwd}/actor__{self.total_step:012}_{self.max_r:09.3f}.pt\"\n",
    "\n",
    "        elif self.save_counter == self.save_gap:\n",
    "            self.save_counter = 0\n",
    "            if self.if_over_write:\n",
    "                actor_path = f\"{self.cwd}/actor.pt\"\n",
    "            else:\n",
    "                actor_path = f\"{self.cwd}/actor__{self.total_step:012}.pt\"\n",
    "\n",
    "        if actor_path:\n",
    "            torch.save(actor, actor_path)  # save policy network in *.pt\n",
    "\n",
    "    def save_or_load_recoder(self, if_save: bool):\n",
    "        if if_save:\n",
    "            np.save(self.recorder_path, self.recorder)\n",
    "        elif os.path.exists(self.recorder_path):\n",
    "            recorder = np.load(self.recorder_path)\n",
    "            self.recorder = [tuple(i) for i in recorder]  # convert numpy to list\n",
    "            self.total_step = self.recorder[-1][0]\n",
    "\n",
    "    def get_cumulative_rewards_and_step_single_env(self, actor) -> Tensor:\n",
    "        rewards_steps_list = [get_cumulative_rewards_and_steps(self.env, actor) for _ in range(self.eval_times)]\n",
    "        rewards_steps_ten = torch.tensor(rewards_steps_list, dtype=torch.float32)\n",
    "        return rewards_steps_ten  # rewards_steps_ten.shape[1] == 2\n",
    "\n",
    "    def get_cumulative_rewards_and_step_vectorized_env(self, actor) -> Tensor:\n",
    "        rewards_step_list = [get_cumulative_rewards_and_step_from_vec_env(self.env, actor)\n",
    "                             for _ in range(max(1, self.eval_times // self.env.num_envs))]\n",
    "        rewards_step_list = sum(rewards_step_list, [])\n",
    "        rewards_step_ten = torch.tensor(rewards_step_list)\n",
    "        return rewards_step_ten  # rewards_steps_ten.shape[1] == 2\n",
    "\n",
    "    def save_training_curve_jpg(self):\n",
    "        recorder = np.array(self.recorder)\n",
    "\n",
    "        train_time = int(time.time() - self.start_time)\n",
    "        total_step = int(self.recorder[-1][0])\n",
    "        fig_title = f\"step_time_maxR_{int(total_step)}_{int(train_time)}_{self.max_r:.3f}\"\n",
    "\n",
    "        draw_learning_curve(recorder=recorder, fig_title=fig_title, save_path=f\"{self.cwd}/LearningCurve.jpg\")\n",
    "        np.save(self.recorder_path, recorder)  # save self.recorder for `draw_learning_curve()`\n",
    "\n",
    "def draw_learning_curve(recorder: np.ndarray = None,\n",
    "                        fig_title: str = 'learning_curve',\n",
    "                        save_path: str = 'learning_curve.jpg'):\n",
    "    steps = recorder[:, 0]  # x-axis is training steps\n",
    "    r_avg = recorder[:, 1]\n",
    "    r_std = recorder[:, 2]\n",
    "    r_exp = recorder[:, 3]\n",
    "    obj_c = recorder[:, 4]\n",
    "    obj_a = recorder[:, 5]\n",
    "\n",
    "    '''plot subplots'''\n",
    "    import matplotlib as mpl\n",
    "    mpl.use('Agg')\n",
    "    \"\"\"Generating matplotlib graphs without a running X server [duplicate]\n",
    "    write `mpl.use('Agg')` before `import matplotlib.pyplot as plt`\n",
    "    https://stackoverflow.com/a/4935945/9293137\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    '''axs[0]'''\n",
    "    ax00 = axs[0]\n",
    "    ax00.cla()\n",
    "\n",
    "    ax01 = axs[0].twinx()\n",
    "    color01 = 'darkcyan'\n",
    "    ax01.set_ylabel('Explore AvgReward', color=color01)\n",
    "    ax01.plot(steps, r_exp, color=color01, alpha=0.5, )\n",
    "    ax01.tick_params(axis='y', labelcolor=color01)\n",
    "\n",
    "    color0 = 'lightcoral'\n",
    "    ax00.set_ylabel('Episode Return', color=color0)\n",
    "    ax00.plot(steps, r_avg, label='Episode Return', color=color0)\n",
    "    ax00.fill_between(steps, r_avg - r_std, r_avg + r_std, facecolor=color0, alpha=0.3)\n",
    "    ax00.grid()\n",
    "    '''axs[1]'''\n",
    "    ax10 = axs[1]\n",
    "    ax10.cla()\n",
    "\n",
    "    ax11 = axs[1].twinx()\n",
    "    color11 = 'darkcyan'\n",
    "    ax11.set_ylabel('objC', color=color11)\n",
    "    ax11.fill_between(steps, obj_c, facecolor=color11, alpha=0.2, )\n",
    "    ax11.tick_params(axis='y', labelcolor=color11)\n",
    "\n",
    "    color10 = 'royalblue'\n",
    "    ax10.set_xlabel('Total Steps')\n",
    "    ax10.set_ylabel('objA', color=color10)\n",
    "    ax10.plot(steps, obj_a, label='objA', color=color10)\n",
    "    ax10.tick_params(axis='y', labelcolor=color10)\n",
    "    for plot_i in range(6, recorder.shape[1]):\n",
    "        other = recorder[:, plot_i]\n",
    "        ax10.plot(steps, other, label=f'{plot_i}', color='grey', alpha=0.5)\n",
    "    ax10.legend()\n",
    "    ax10.grid()\n",
    "\n",
    "    '''plot save'''\n",
    "    plt.title(fig_title, y=2.3)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close('all')  # avoiding warning about too many open figures, rcParam `figure.max_open_warning`\n",
    "    # plt.show()  # if use `mpl.use('Agg')` to draw figures without GUI, then plt can't plt.show()\n",
    "\n",
    "\n",
    "def get_cumulative_rewards_and_steps(env, actor, if_render: bool = False) -> Tuple[float, int]:\n",
    "    \"\"\"Usage\n",
    "    eval_times = 4\n",
    "    net_dim = 2 ** 7\n",
    "    actor_path = './LunarLanderContinuous-v2_PPO_1/actor.pt'\n",
    "\n",
    "    env = build_env(env_class=env_class, env_args=env_args)\n",
    "    act = agent(net_dim, env.state_dim, env.action_dim, gpu_id=gpu_id).act\n",
    "    act.load_state_dict(torch.load(actor_path, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    r_s_ary = [get_episode_return_and_step(env, act) for _ in range(eval_times)]\n",
    "    r_s_ary = np.array(r_s_ary, dtype=np.float32)\n",
    "    r_avg, s_avg = r_s_ary.mean(axis=0)  # average of episode return and episode step\n",
    "    \"\"\"\n",
    "    max_step = env.max_step\n",
    "    if_discrete = env.if_discrete\n",
    "    device = next(actor.parameters()).device  # net.parameters() is a Python generator.\n",
    "\n",
    "    state = env.reset()\n",
    "    steps = None\n",
    "    returns = 0.0  # sum of rewards in an episode\n",
    "    for steps in range(max_step):\n",
    "        tensor_state = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        tensor_action = actor(tensor_state)\n",
    "        if if_discrete:\n",
    "            tensor_action = tensor_action.argmax(dim=1)\n",
    "        action = tensor_action.detach().cpu().numpy()[0]  # not need detach(), because using torch.no_grad() outside\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        returns += reward\n",
    "\n",
    "        if if_render:\n",
    "            env.render()\n",
    "            time.sleep(0.02)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    else:\n",
    "        print(\"| get_rewards_and_step: WARNING. max_step > 12345\")\n",
    "    returns = getattr(env, 'cumulative_returns', returns)\n",
    "    steps += 1\n",
    "    return returns, steps\n",
    "\n",
    "\n",
    "def get_cumulative_rewards_and_step_from_vec_env(env, actor) -> List[Tuple[float, int]]:\n",
    "    device = env.device\n",
    "    env_num = env.num_envs\n",
    "    max_step = env.max_step\n",
    "    if_discrete = env.if_discrete\n",
    "\n",
    "    '''get returns and dones (GPU)'''\n",
    "    returns = torch.empty((max_step, env_num), dtype=torch.float32, device=device)\n",
    "    dones = torch.empty((max_step, env_num), dtype=torch.bool, device=device)\n",
    "\n",
    "    state = env.reset()  # must reset in vectorized env\n",
    "    for t in range(max_step):\n",
    "        action = actor(state.to(device))\n",
    "        # assert action.shape == (env.env_num, env.action_dim)\n",
    "        if if_discrete:\n",
    "            action = action.argmax(dim=1, keepdim=True)\n",
    "        state, reward, done, info_dict = env.step(action)\n",
    "\n",
    "        returns[t] = reward\n",
    "        dones[t] = done\n",
    "\n",
    "    '''get cumulative returns and step'''\n",
    "    if hasattr(env, 'cumulative_returns'):  # GPU\n",
    "        returns_step_list = [(ret, env.max_step) for ret in env.cumulative_returns]\n",
    "    else:  # CPU\n",
    "        returns = returns.cpu()\n",
    "        dones = dones.cpu()\n",
    "\n",
    "        returns_step_list = []\n",
    "        for i in range(env_num):\n",
    "            dones_where = torch.where(dones[:, i] == 1)[0] + 1\n",
    "            episode_num = len(dones_where)\n",
    "            if episode_num == 0:\n",
    "                continue\n",
    "\n",
    "            j0 = 0\n",
    "            for j1 in dones_where.tolist():\n",
    "                reward_sum = returns[j0:j1, i].sum().item()  # cumulative returns of an episode\n",
    "                steps_num = j1 - j0  # step number of an episode\n",
    "                returns_step_list.append((reward_sum, steps_num))\n",
    "\n",
    "                j0 = j1\n",
    "    return returns_step_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(args: Config):\n",
    "    args.init_before_training()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    '''init environment'''\n",
    "    env = build_env(args.env_class, args.env_args, args.gpu_id)\n",
    "\n",
    "    '''init agent'''\n",
    "    agent = args.agent_class(args.net_dims, args.state_dim, args.action_dim, gpu_id=args.gpu_id, args=args)\n",
    "    agent.save_or_load_agent(args.cwd, if_save=False)\n",
    "\n",
    "    '''init agent.last_state'''\n",
    "    state = env.reset()\n",
    "    if args.num_envs == 1:\n",
    "        assert state.shape == (args.state_dim,)\n",
    "        assert isinstance(state, np.ndarray)\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=agent.device).unsqueeze(0)\n",
    "    else:\n",
    "        assert state.shape == (args.num_envs, args.state_dim)\n",
    "        assert isinstance(state, torch.Tensor)\n",
    "        state = state.to(agent.device)\n",
    "    assert state.shape == (args.num_envs, args.state_dim)\n",
    "    assert isinstance(state, torch.Tensor)\n",
    "    agent.last_state = state.detach()\n",
    "\n",
    "    '''init buffer'''\n",
    "    if args.if_off_policy:\n",
    "        buffer = ReplayBuffer(\n",
    "            gpu_id=args.gpu_id,\n",
    "            num_seqs=args.num_envs,\n",
    "            max_size=args.buffer_size,\n",
    "            state_dim=args.state_dim,\n",
    "            action_dim=1 if args.if_discrete else args.action_dim,\n",
    "            if_use_per=args.if_use_per,\n",
    "            args=args,\n",
    "        )\n",
    "        buffer_items = agent.explore_env(env, args.horizon_len * args.eval_times, if_random=True)\n",
    "        buffer.update(buffer_items)  # warm up for ReplayBuffer\n",
    "    else:\n",
    "        buffer = []\n",
    "\n",
    "    '''init evaluator'''\n",
    "    eval_env_class = args.eval_env_class if args.eval_env_class else args.env_class\n",
    "    eval_env_args = args.eval_env_args if args.eval_env_args else args.env_args\n",
    "    eval_env = build_env(eval_env_class, eval_env_args, args.gpu_id)\n",
    "    evaluator = Evaluator(cwd=args.cwd, env=eval_env, args=args, if_tensorboard=False)\n",
    "\n",
    "    '''train loop'''\n",
    "    cwd = args.cwd\n",
    "    break_step = args.break_step\n",
    "    horizon_len = args.horizon_len\n",
    "    if_off_policy = args.if_off_policy\n",
    "    if_save_buffer = args.if_save_buffer\n",
    "    del args\n",
    "\n",
    "    if_train = True\n",
    "    while if_train:\n",
    "        buffer_items = agent.explore_env(env, horizon_len)\n",
    "\n",
    "        exp_r = buffer_items[2].mean().item()\n",
    "        if if_off_policy:\n",
    "            buffer.update(buffer_items)\n",
    "        else:\n",
    "            buffer[:] = buffer_items\n",
    "\n",
    "        torch.set_grad_enabled(True)\n",
    "        logging_tuple = agent.update_net(buffer)\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        evaluator.evaluate_and_save(actor=agent.act, steps=horizon_len, exp_r=exp_r, logging_tuple=logging_tuple)\n",
    "        if_train = (evaluator.total_step <= break_step) and (not os.path.exists(f\"{cwd}/stop\"))\n",
    "\n",
    "    print(f'| UsedTime: {time.time() - evaluator.start_time:>7.0f} | SavedDir: {cwd}')\n",
    "\n",
    "    env.close() if hasattr(env, 'close') else None\n",
    "    evaluator.save_training_curve_jpg()\n",
    "    agent.save_or_load_agent(cwd, if_save=True)\n",
    "    if if_save_buffer and hasattr(buffer, 'save_or_load_history'):\n",
    "        buffer.save_or_load_history(cwd, if_save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Arguments Remove cwd: ./cyborg_D3QN_0\n",
      "| Evaluator:\n",
      "| `step`: Number of samples, or total training steps, or running times of `env.step()`.\n",
      "| `time`: Time spent from the start of training to this moment.\n",
      "| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `avgS`: Average of steps in an episode.\n",
      "| `objC`: Objective of Critic network. Or call it loss function of critic network.\n",
      "| `objA`: Objective of Actor network. It is the average Q value of the critic network.\n",
      "################################################################################\n",
      "ID     Step    Time |    avgR   stdR   avgS  stdS |    expR   objC   objA   etc.\n",
      "0  5.12e+02       4 | -649.57  397.4    100     0 |   -2.68   1.53  -2.39\n",
      "0  2.10e+04     167 |  -98.43   95.8    100     0 |   -1.84   3.18 -36.77\n",
      "0  4.15e+04     326 |  -98.83   22.2    100     0 |   -0.72   2.04 -17.47\n",
      "0  6.20e+04     484 |  -47.00    3.9    100     0 |   -0.48   1.38 -14.46\n",
      "0  8.24e+04     643 |  -63.10   38.2    100     0 |   -0.42   1.12 -12.88\n",
      "0  1.03e+05     803 |   -9.90    8.2    100     0 |   -0.62   1.01 -11.25\n",
      "0  1.23e+05     963 |  -36.83    3.2    100     0 |   -0.50   0.88  -9.99\n",
      "0  1.44e+05    1125 |  -49.10    8.4    100     0 |   -0.52   0.81  -9.37\n",
      "0  1.64e+05    1284 |  -43.50   30.7    100     0 |   -0.76   0.78  -9.02\n",
      "0  1.85e+05    1444 |  -73.87   48.9    100     0 |   -0.48   0.69  -8.62\n",
      "0  2.05e+05    1601 |  -50.40    5.1    100     0 |   -0.53   0.65  -8.47\n",
      "0  2.26e+05    1760 |  -46.23    2.4    100     0 |   -0.52   0.65  -8.19\n",
      "0  2.46e+05    1921 |  -66.33   32.4    100     0 |   -0.48   0.64  -8.23\n",
      "0  2.67e+05    2081 |  -46.57    3.5    100     0 |   -0.72   0.61  -8.16\n",
      "0  2.87e+05    2243 |  -69.73   45.9    100     0 |   -0.65   0.61  -7.96\n",
      "0  3.08e+05    2400 |  -64.20    9.5    100     0 |   -0.38   0.59  -7.95\n",
      "0  3.28e+05    2558 |  -59.60   40.1    100     0 |   -0.46   0.57  -7.82\n",
      "0  3.49e+05    2717 |  -38.37    9.4    100     0 |   -0.38   0.57  -7.72\n",
      "0  3.69e+05    2875 |  -60.23   46.5    100     0 |   -0.46   0.57  -7.73\n",
      "0  3.90e+05    3034 |  -49.50    7.9    100     0 |   -0.47   0.55  -7.63\n",
      "0  4.10e+05    3188 |  -77.13   34.3    100     0 |   -0.69   0.57  -7.61\n",
      "0  4.31e+05    3347 |  -39.10    3.4    100     0 |   -0.52   0.55  -7.68\n",
      "0  4.51e+05    3508 |  -46.23    7.8    100     0 |   -0.47   0.53  -7.55\n",
      "0  4.72e+05    3666 |  -65.03   31.4    100     0 |   -0.65   0.53  -7.67\n"
     ]
    }
   ],
   "source": [
    "env_args = {\n",
    "    'env_name': 'cyborg',  # A pole is attached by an un-actuated joint to a cart.\n",
    "    'state_dim': 52,  # (CartPosition, CartVelocity, PoleAngle, PoleAngleVelocity)\n",
    "    'action_dim': 54,  # (Push cart to the left, Push cart to the right)\n",
    "    'if_discrete': True,  # discrete action space\n",
    "}  # env_args = get_gym_env_args(env=gym.make('CartPole-v0'), if_print=True)\n",
    "\n",
    "args = Config(agent_class=AgentD3QN, env_class=None, env_args=env_args)  # see `Config` for explanation\n",
    "args.break_step = int(2e5 * 2000)  # break training if 'total_step > break_step'\n",
    "args.net_dims = (256, 256)  # the middle layer dimension of MultiLayer Perceptron\n",
    "args.gamma = 0.95  # discount factor of future rewards\n",
    "train_agent(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianshou",
   "language": "python",
   "name": "tianshou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
