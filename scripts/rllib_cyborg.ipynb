{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/zhx/word/work/DriverOrderOfflineRL/cage-challenge-1/CybORG')\n",
    "sys.path.append('/home/zhx/word/work/DriverOrderOfflineRL/cyborg-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import cyborg_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.wrappers import EnvCompatibility\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/gymnasium/wrappers/compatibility.py:60: DeprecationWarning: \u001b[33mWARN: The `gymnasium.make(..., apply_api_compatibility=...)` parameter is deprecated and will be removed in v1.0. Instead use `gymnasium.make('GymV21Environment-v0', env_name=...)` or `from shimmy import GymV21CompatibilityV0`\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from pprint import pprint\n",
    "from CybORG import CybORG\n",
    "from CybORG.Shared.Actions import *\n",
    "from CybORG.Agents import RedMeanderAgent, B_lineAgent\n",
    "from CybORG.Agents.Wrappers import *\n",
    "\n",
    "# seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "env = gym.make('cyborg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67660511ff34478fbbfb8b8d16c1ec78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "# The following three lines are for rendering purposes only.\n",
    "# They allow us to render the env frame-by-frame in-place\n",
    "# (w/o creating a huge output which we would then have to scroll through).\n",
    "out = Output()\n",
    "display.display(out)\n",
    "with out:\n",
    "\n",
    "    # Putting the Gym simple API methods together.\n",
    "    # Here is a pattern for running a bunch of episodes.\n",
    "    num_episodes = 5 # Number of episodes you want to run the agent\n",
    "    total_reward = 0.0  # Initialize reward to 0\n",
    "\n",
    "    # Loop through episodes\n",
    "    for ep in range(num_episodes):\n",
    "\n",
    "        # Reset the environment at the start of each episode\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Loop through time steps per episode\n",
    "        for i in range(100):\n",
    "            # take random action, but you can also do something more intelligent \n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # apply the action\n",
    "            new_obs, reward, done, _, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # If the epsiode is up, then start another one\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Render the env (in place).\n",
    "            out.clear_output(wait=True)\n",
    "            print(f\"episode: {ep}\")\n",
    "            print(f\"obs: {new_obs}, reward: {total_reward}, done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in this system: 72\n",
      "numpy: 1.24.3\n",
      "pandas: 2.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 11:24:12,835\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "# import commonly-used libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "print(f'Number of CPUs in this system: {os.cpu_count()}')\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "\n",
    "# import ray\n",
    "import ray\n",
    "from ray import tune, rllib, air\n",
    "from ray.tune.logger import pretty_print\n",
    "print(f\"ray: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 12:02:04,472\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking environment ...\n",
      "All checks passed. No errors found.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env\n",
    "\n",
    "# How to check you do not have any environment errors\n",
    "print(\"checking environment ...\")\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"All checks passed. No errors found.\")\n",
    "except:\n",
    "    print(\"failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "from cyborg_env.envs.cyborg_env import CyborgEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_creator = lambda config: CyborgEnv()\n",
    "# register that way to make the environment under an rllib name\n",
    "register_env('cyborg', lambda config: env_creator(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "# Default DQN config values\n",
    "# uncomment below to see the long list of specifically PPO default config values\n",
    "# print(pretty_print(DQNConfig().to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 12:02:14,403\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/simple_q/` has been deprecated. Use `rllib_contrib/simple_q/` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config type: <class 'ray.rllib.algorithms.dqn.dqn.DQNConfig'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-12-02 12:02:17,747\tINFO worker.py:1673 -- Started a local Ray instance.\n",
      "\u001b[36m(pid=3306004)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[36m(pid=3306004)\u001b[0m /home/zhx/.conda/envs/academy/lib/python3.8/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[36m(pid=3306004)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[36m(RolloutWorker pid=3306005)\u001b[0m No module named 'cyborg_env'\n",
      "\u001b[36m(RolloutWorker pid=3306005)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(RolloutWorker pid=3306005)\u001b[0m   File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 404, in deserialize_objects\n",
      "\u001b[36m(RolloutWorker pid=3306005)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(RolloutWorker pid=3306005)\u001b[0m   File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 270, in _deserialize_object\n",
      "\u001b[36m(RolloutWorker pid=3306005)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[36m(RolloutWorker pid=3306005)\u001b[0m   File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "\u001b[36m(RolloutWorker pid=3306005)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[36m(RolloutWorker pid=3306005)\u001b[0m   File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "\u001b[36m(RolloutWorker pid=3306005)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[36m(RolloutWorker pid=3306005)\u001b[0m ModuleNotFoundError: No module named 'cyborg_env'\n",
      "\u001b[36m(RolloutWorker pid=3306004)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3306004, ip=192.168.0.143, actor_id=da3829a1d0ebfb82a7fa748401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc9def6e6a0>)\n",
      "\u001b[36m(RolloutWorker pid=3306004)\u001b[0m   At least one of the input arguments for this task could not be computed:\n",
      "\u001b[36m(RolloutWorker pid=3306004)\u001b[0m ray.exceptions.RaySystemError: System error: No module named 'cyborg_env'\n",
      "\u001b[36m(RolloutWorker pid=3306004)\u001b[0m traceback: Traceback (most recent call last):\n",
      "2023-12-02 12:02:26,168\tERROR actor_manager.py:500 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3306004, ip=192.168.0.143, actor_id=da3829a1d0ebfb82a7fa748401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc9def6e6a0>)\n",
      "  At least one of the input arguments for this task could not be computed:\n",
      "ray.exceptions.RaySystemError: System error: No module named 'cyborg_env'\n",
      "traceback: Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'cyborg_env'\n",
      "2023-12-02 12:02:26,171\tERROR actor_manager.py:500 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3306005, ip=192.168.0.143, actor_id=8b5756a8a67174f01ae26f8901000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f5e631596a0>)\n",
      "  At least one of the input arguments for this task could not be computed:\n",
      "ray.exceptions.RaySystemError: System error: No module named 'cyborg_env'\n",
      "traceback: Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'cyborg_env'\n",
      "2023-12-02 12:02:26,173\tERROR actor_manager.py:500 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3306006, ip=192.168.0.143, actor_id=a3361f42a931398cb2d0b89901000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f646bee3640>)\n",
      "  At least one of the input arguments for this task could not be computed:\n",
      "ray.exceptions.RaySystemError: System error: No module named 'cyborg_env'\n",
      "traceback: Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'cyborg_env'\n",
      "2023-12-02 12:02:26,176\tERROR actor_manager.py:500 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3306008, ip=192.168.0.143, actor_id=008825291e4a93fb18d0f73701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f4c3061f6d0>)\n",
      "  At least one of the input arguments for this task could not be computed:\n",
      "ray.exceptions.RaySystemError: System error: No module named 'cyborg_env'\n",
      "traceback: Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'cyborg_env'\n",
      "2023-12-02 12:02:26,178\tERROR actor_manager.py:500 -- Ray error, taking actor 5 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3306009, ip=192.168.0.143, actor_id=6524032fe09cbea99392f8ea01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f38859ca6a0>)\n",
      "  At least one of the input arguments for this task could not be computed:\n",
      "ray.exceptions.RaySystemError: System error: No module named 'cyborg_env'\n",
      "traceback: Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'cyborg_env'\n",
      "2023-12-02 12:02:26,180\tERROR actor_manager.py:500 -- Ray error, taking actor 6 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3306012, ip=192.168.0.143, actor_id=a65bf808eb490da6599a2ad401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f53fd74a6a0>)\n",
      "  At least one of the input arguments for this task could not be computed:\n",
      "ray.exceptions.RaySystemError: System error: No module named 'cyborg_env'\n",
      "traceback: Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'cyborg_env'\n",
      "2023-12-02 12:02:26,182\tERROR actor_manager.py:500 -- Ray error, taking actor 7 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3306013, ip=192.168.0.143, actor_id=c99943677b3738647ce481b101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7fe6ab6670>)\n",
      "  At least one of the input arguments for this task could not be computed:\n",
      "ray.exceptions.RaySystemError: System error: No module named 'cyborg_env'\n",
      "traceback: Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'cyborg_env'\n"
     ]
    },
    {
     "ename": "RaySystemError",
     "evalue": "System error: No module named 'cyborg_env'\ntraceback: Traceback (most recent call last):\n  File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 404, in deserialize_objects\n    obj = self._deserialize_object(data, metadata, object_ref)\n  File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 270, in _deserialize_object\n    return self._deserialize_msgpack_data(data, metadata_fields)\n  File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 225, in _deserialize_msgpack_data\n    python_objects = self._deserialize_pickle5_data(pickle5_data)\n  File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 215, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band)\nModuleNotFoundError: No module named 'cyborg_env'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:157\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup(\n\u001b[1;32m    158\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    159\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    160\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    161\u001b[0m         local_worker\u001b[39m=\u001b[39;49mlocal_worker,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39m# constructor).\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:227\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[0;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_workers(\n\u001b[1;32m    228\u001b[0m     num_workers,\n\u001b[1;32m    229\u001b[0m     validate\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mvalidate_workers_after_construction,\n\u001b[1;32m    230\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[39m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:593\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39mok:\n\u001b[0;32m--> 593\u001b[0m     \u001b[39mraise\u001b[39;00m result\u001b[39m.\u001b[39mget()\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py:481\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.__fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 481\u001b[0m     result \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(r)\n\u001b[1;32m    482\u001b[0m     remote_results\u001b[39m.\u001b[39madd_result(actor_id, ResultOrError(result\u001b[39m=\u001b[39mresult), tag)\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/worker.py:2565\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2564\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2565\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m   2567\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3306004, ip=192.168.0.143, actor_id=da3829a1d0ebfb82a7fa748401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc9def6e6a0>)\n  At least one of the input arguments for this task could not be computed:\nray.exceptions.RaySystemError: System error: No module named 'cyborg_env'\ntraceback: Traceback (most recent call last):\nModuleNotFoundError: No module named 'cyborg_env'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRaySystemError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/zhx/word/work/DriverOrderOfflineRL/scripts/rllib_cyborg.ipynb 单元格 11\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bimmu1/home/zhx/word/work/DriverOrderOfflineRL/scripts/rllib_cyborg.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConfig type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(dqn_config)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bimmu1/home/zhx/word/work/DriverOrderOfflineRL/scripts/rllib_cyborg.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# Use the config object's `build()` method for instantiating\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bimmu1/home/zhx/word/work/DriverOrderOfflineRL/scripts/rllib_cyborg.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# an RLlib Algorithm instance that we can then train.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bimmu1/home/zhx/word/work/DriverOrderOfflineRL/scripts/rllib_cyborg.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# Note if using Tune, don't need algo object, but this is still a good debugging step.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bimmu1/home/zhx/word/work/DriverOrderOfflineRL/scripts/rllib_cyborg.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m dqn_algo \u001b[39m=\u001b[39m dqn_config\u001b[39m.\u001b[39;49mbuild()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bimmu1/home/zhx/word/work/DriverOrderOfflineRL/scripts/rllib_cyborg.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAlgorithm type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(dqn_algo)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bimmu1/home/zhx/word/work/DriverOrderOfflineRL/scripts/rllib_cyborg.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm_config.py:1100\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1098\u001b[0m     algo_class \u001b[39m=\u001b[39m get_trainable_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class)\n\u001b[0;32m-> 1100\u001b[0m \u001b[39mreturn\u001b[39;00m algo_class(\n\u001b[1;32m   1101\u001b[0m     config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m use_copy \u001b[39melse\u001b[39;49;00m copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m),\n\u001b[1;32m   1102\u001b[0m     logger_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger_creator,\n\u001b[1;32m   1103\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/rllib/utils/deprecation.py:106\u001b[0m, in \u001b[0;36mDeprecated.<locals>._inner.<locals>.patched_init\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m log_once(old \u001b[39mor\u001b[39;00m obj\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m):\n\u001b[1;32m    100\u001b[0m     deprecation_warning(\n\u001b[1;32m    101\u001b[0m         old\u001b[39m=\u001b[39mold \u001b[39mor\u001b[39;00m obj\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[1;32m    102\u001b[0m         new\u001b[39m=\u001b[39mnew,\n\u001b[1;32m    103\u001b[0m         help\u001b[39m=\u001b[39mhelp,\n\u001b[1;32m    104\u001b[0m         error\u001b[39m=\u001b[39merror,\n\u001b[1;32m    105\u001b[0m     )\n\u001b[0;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m obj_init(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:517\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    504\u001b[0m     \u001b[39m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     },\n\u001b[1;32m    515\u001b[0m }\n\u001b[0;32m--> 517\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    518\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    519\u001b[0m     logger_creator\u001b[39m=\u001b[39;49mlogger_creator,\n\u001b[1;32m    520\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    521\u001b[0m )\n\u001b[1;32m    523\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/tune/trainable/trainable.py:161\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    157\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStorageContext on the TRAINABLE:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mstorage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 161\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    162\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:639\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[39mif\u001b[39;00m _init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    638\u001b[0m     \u001b[39m# Create a set of env runner actors via a WorkerSet.\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    640\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    641\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    642\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    643\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    644\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rollout_workers,\n\u001b[1;32m    645\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    646\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    649\u001b[0m     \u001b[39m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m    652\u001b[0m         \u001b[39m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/academy/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:179\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    167\u001b[0m     \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[39m# errors.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mactor_init_failed:\n\u001b[1;32m    172\u001b[0m         \u001b[39m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[1;32m    173\u001b[0m         \u001b[39m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[39m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    178\u001b[0m         \u001b[39m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m2\u001b[39m]\n\u001b[1;32m    180\u001b[0m     \u001b[39m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[0;31mRaySystemError\u001b[0m: System error: No module named 'cyborg_env'\ntraceback: Traceback (most recent call last):\n  File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 404, in deserialize_objects\n    obj = self._deserialize_object(data, metadata, object_ref)\n  File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 270, in _deserialize_object\n    return self._deserialize_msgpack_data(data, metadata_fields)\n  File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 225, in _deserialize_msgpack_data\n    python_objects = self._deserialize_pickle5_data(pickle5_data)\n  File \"/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/ray/_private/serialization.py\", line 215, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band)\nModuleNotFoundError: No module named 'cyborg_env'\n"
     ]
    }
   ],
   "source": [
    "# Create a DQNConfig object\n",
    "dqn_config = DQNConfig()\n",
    "\n",
    "# Only for Colab. Specify 1 gpu\n",
    "# dqn_config.num_gpus=1\n",
    "\n",
    "# Setup our config object to use our environment\n",
    "dqn_config.environment(env='cyborg')\n",
    "\n",
    "# Decide if you want torch or tensorflow DL framework.  Default is \"tf\"\n",
    "dqn_config.framework(framework=\"torch\")\n",
    "\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "dqn_config.debugging(seed=415, log_level=\"ERROR\")\n",
    "\n",
    "# Setup evaluation\n",
    "dqn_config.evaluation(\n",
    "    \n",
    "    # Minimum number of training iterations between evaluations.\n",
    "    # Evaluations are blocking operations (if evaluation_parallel_to_training=False) \n",
    "    # set `evaluation_interval` larger for faster runtime.\n",
    "    evaluation_interval=10, \n",
    "\n",
    "    # Minimum number of evaluation iterations.\n",
    "    # If using multiple evaluation workers, we will run at least \n",
    "    # this many episodes * num_evalworkers total.\n",
    "    evaluation_duration=5,      \n",
    "\n",
    "    # Number of parallel evaluation workers. \n",
    "    # Zero by default, which means evaluation will run on the training resources. \n",
    "    # If you increase this, it will increase total Ray resource usage\n",
    "    # since evaluation workers are created separately from rollout workers \n",
    "    # Note: these show up on Ray Dashboard as extra \"RolloutWorker\"s\n",
    "    evaluation_num_workers=7,  #0 for Colab\n",
    "\n",
    "    # Use the parallel evaluation workers in parallel with training workers\n",
    "    evaluation_parallel_to_training=True,  #False for Colab\n",
    "    \n",
    "    evaluation_config = dict(\n",
    "        # Explicitly set \"explore\"=False to override default True\n",
    "        # Best practice value is False unless environment is stochastic\n",
    "        explore=False,\n",
    "        \n",
    "        # Number of parallel Training workers\n",
    "        # Override the num_workers from the training config \n",
    "        # Note: DQN only allows 1 Trainer worker, see documentation\n",
    "        num_workers=1,  #any number here will be reset = 1 for DQN\n",
    "    ),\n",
    ")\n",
    "\n",
    "# # Override default training parameters\n",
    "# dqn_config.training(target_network_update_freq=5000, \n",
    "#                     model=dict(\"fcnet_hiddens\" : [32, 32])\n",
    "#                    )\n",
    "\n",
    "# Setup sampling rollout workers for streaming the data \n",
    "dqn_config.rollouts(\n",
    "    num_rollout_workers=7,  #1 for Colab\n",
    "    \n",
    "    # for small environments this can be >1 based on size of your processor\n",
    "    num_envs_per_worker=1,)\n",
    "\n",
    "print(f\"Config type: {type(dqn_config)}\")\n",
    "\n",
    "# Use the config object's `build()` method for instantiating\n",
    "# an RLlib Algorithm instance that we can then train.\n",
    "# Note if using Tune, don't need algo object, but this is still a good debugging step.\n",
    "dqn_algo = dqn_config.build()\n",
    "print(f\"Algorithm type: {type(dqn_algo)}\")\n",
    "\n",
    "print()\n",
    "print(\"DQN MODEL ARCHITECTURE:\")\n",
    "# print(result['config']['model'])\n",
    "# # tf print keras model summary\n",
    "# print(dqn_algo.get_policy().model.base_model.summary())\n",
    "# # torch\n",
    "# from torchinfo import summary\n",
    "# summary(dqn_algo.get_policy().model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/zhx/word/work/DriverOrderOfflineRL/cage-challenge-1/CybORG')\n",
    "\n",
    "import ray\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.wrappers import EnvCompatibility\n",
    "\n",
    "from typing import Optional, Tuple, Union\n",
    "from typing import Any, Dict, Optional, Protocol, runtime_checkable\n",
    "from gymnasium.core import ObsType\n",
    "\n",
    "import inspect\n",
    "from pprint import pprint\n",
    "from CybORG import CybORG\n",
    "from CybORG.Shared.Actions import *\n",
    "from CybORG.Agents import RedMeanderAgent, B_lineAgent\n",
    "from CybORG.Agents.Wrappers import *\n",
    "path = str(inspect.getfile(CybORG))\n",
    "path = path[:-10] + '/Shared/Scenarios/Scenario1b.yaml'\n",
    "\n",
    "# seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# env\n",
    "CYBORG = CybORG(path,'sim', agents={'Red': RedMeanderAgent})\n",
    "class Wrapper(gym.Env):\n",
    "    def __init__(self, env:ChallengeWrapper) -> None:\n",
    "        self.env = env\n",
    "        self.action_space = spaces.Discrete(54)\n",
    "        self.observation_space = spaces.Box(low=-1.0, high=3.0, shape=((52,)), dtype=np.float32)\n",
    "    \n",
    "    def step(self, action=None):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "class CyborgEnv(gym.Env):\n",
    "    def __init__(self, config=None) -> None:\n",
    "        self.env = EnvCompatibility(Wrapper(ChallengeWrapper(env=CybORG(path,'sim', agents={'Red': RedMeanderAgent}), agent_name=\"Blue\", max_steps=100)))\n",
    "        self.action_space = spaces.Discrete(54)\n",
    "        self.observation_space = spaces.Box(low=-1.0, high=3.0, shape=((52,)), dtype=np.float32)\n",
    "\n",
    "    def step(self, action: Any) -> Tuple[Any, float, bool, bool, Dict]:\n",
    "        return self.env.step(action=action)\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, options: Optional[dict] = None\n",
    "    ) -> Tuple[ObsType, dict]:\n",
    "        return self.env.reset(seed=seed, options=options)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academy",
   "language": "python",
   "name": "academy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
