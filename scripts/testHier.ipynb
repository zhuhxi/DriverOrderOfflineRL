{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/zhx/word/DriverOrderOfflineRL/cage-challenge-1/CybORG')\n",
    "sys.path.append('/home/zhx/word/DriverOrderOfflineRL/tianshou')\n",
    "sys.path.append('/home/zhx/word/DriverOrderOfflineRL/tianshou/examples/atari')\n",
    "sys.path.append('/home/zhx/word/DriverOrderOfflineRL/gym')\n",
    "sys.path.append('/home/zhx/word/DriverOrderOfflineRL')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import wandb\n",
    "\n",
    "class ActorPPO(nn.Module):\n",
    "    def __init__(self, dims: [int], state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        state = torch.flatten(state, start_dim=1)\n",
    "        n = torch.tanh(self.l1(state))\n",
    "        n = torch.tanh(self.l2(n))\n",
    "        return n\n",
    "\n",
    "    def get_action(self, state: Tensor) -> (Tensor, Tensor):  # for exploration\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi(state, softmax_dim=1)\n",
    "            m = Categorical(pi)\n",
    "            a = m.sample().item()\n",
    "            pi_a = torch.log(pi[0][a]).item()\n",
    "            return a, pi_a\n",
    "    \n",
    "    def pi(self, state, softmax_dim = 1):\n",
    "        n = self.forward(state)\n",
    "        prob = F.softmax(self.l3(n), dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "    def get_logprob_entropy(self, state: Tensor, action: Tensor) -> (Tensor, Tensor):\n",
    "        prob = self.pi(state, softmax_dim=1)\n",
    "        entropy = Categorical(prob).entropy()\n",
    "        prob_action = prob.gather(1, action.view(-1, 1))\n",
    "        logprob = torch.log(prob_action)\n",
    "        return logprob.squeeze(1), entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_action_for_env(action: Tensor) -> Tensor:\n",
    "        return action.tanh()\n",
    "\n",
    "\n",
    "class CriticPPO(nn.Module):\n",
    "    def __init__(self, dims: [int], state_dim: int, _action_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, 1])\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        state = torch.flatten(state, start_dim=1)\n",
    "        return self.net(state)  # advantage value\n",
    "\n",
    "\n",
    "def build_mlp(dims: [int]) -> nn.Sequential:  # MLP (MultiLayer Perceptron)\n",
    "    net_list = []\n",
    "    for i in range(len(dims) - 1):\n",
    "        net_list.extend([nn.Linear(dims[i], dims[i + 1]), nn.ReLU()])\n",
    "    del net_list[-1]  # remove the activation of output layer\n",
    "    return nn.Sequential(*net_list)\n",
    "\n",
    "import inspect\n",
    "from pprint import pprint\n",
    "from CybORG import CybORG\n",
    "from CybORG.Shared.Actions import *\n",
    "from CybORG.Agents import RedMeanderAgent, B_lineAgent\n",
    "from CybORG.Agents.Wrappers import *\n",
    "\n",
    "path = str(inspect.getfile(CybORG))\n",
    "path = path[:-10] + '/Shared/Scenarios/Scenario1b.yaml'\n",
    "\n",
    "class Config:  # for on-policy\n",
    "    def __init__(self, agent_class=None, env_class=None, env_args=None):\n",
    "        self.agent_class = agent_class  # agent = agent_class(...)\n",
    "        self.if_off_policy = False  # whether off-policy or on-policy of DRL algorithm\n",
    "\n",
    "        self.env_class = env_class  # env = env_class(**env_args)\n",
    "        self.env_args = env_args  # env = env_class(**env_args)\n",
    "        if env_args is None:  # dummy env_args\n",
    "            env_args = {'env_name': None, 'state_dim': None, 'action_dim': None, 'if_discrete': None}\n",
    "        self.env_name = env_args['env_name']  # the name of environment. Be used to set 'cwd'.\n",
    "        self.state_dim = env_args['state_dim']  # vector dimension (feature number) of state\n",
    "        self.action_dim = env_args['action_dim']  # vector dimension (feature number) of action\n",
    "        self.if_discrete = env_args['if_discrete']  # discrete or continuous action space\n",
    "\n",
    "        '''Arguments for reward shaping'''\n",
    "        self.gamma = 0.99  # discount factor of future rewards\n",
    "        self.reward_scale = 1.0  # an approximate target reward usually be closed to 256\n",
    "\n",
    "        '''Arguments for training'''\n",
    "        self.net_dims = (256, 256)  # the middle layer dimension of MLP (MultiLayer Perceptron)\n",
    "        self.learning_rate = 6e-5  # 2 ** -14 ~= 6e-5\n",
    "        self.soft_update_tau = 5e-3  # 2 ** -8 ~= 5e-3\n",
    "        self.batch_size = int(1024)  # num of transitions sampled from replay buffer.\n",
    "        self.horizon_len = int(2000)  # collect horizon_len step while exploring, then update network\n",
    "        self.buffer_size = None  # ReplayBuffer size. Empty the ReplayBuffer for on-policy.\n",
    "        self.repeat_times = 8.0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
    "\n",
    "        '''Arguments for device'''\n",
    "        self.gpu_id = int(0)  # `int` means the ID of single GPU, -1 means CPU\n",
    "        self.thread_num = int(8)  # cpu_num for pytorch, `torch.set_num_threads(self.num_threads)`\n",
    "        self.random_seed = int(0)  # initialize random seed in self.init_before_training()\n",
    "\n",
    "        '''Arguments for evaluate'''\n",
    "        self.cwd = None  # current working directory to save model. None means set automatically\n",
    "        self.if_remove = True  # remove the cwd folder? (True, False, None:ask me)\n",
    "        self.break_step = +np.inf  # break training if 'total_step > break_step'\n",
    "\n",
    "        self.eval_times = int(32)  # number of times that get episodic cumulative return\n",
    "        self.eval_per_step = int(2e4)  # evaluate the agent per training steps\n",
    "\n",
    "    def init_before_training(self):\n",
    "        if self.cwd is None:  # set cwd (current working directory) for saving model\n",
    "            self.cwd = f'./{self.env_name}_{self.agent_class.__name__[5:]}'\n",
    "        os.makedirs(self.cwd, exist_ok=True)\n",
    "\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class HierEnv(gym.Env):\n",
    "    # Env parameters\n",
    "    max_steps = 100 # Careful! There are two other envs!\n",
    "    mem_len = 4\n",
    "\n",
    "    \"\"\"The CybORGAgent env\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.RMenv = ChallengeWrapper(env=CybORG(path,'sim', agents={'Red': RedMeanderAgent}), agent_name=\"Blue\", max_steps=100)\n",
    "        self.BLenv = ChallengeWrapper(env=CybORG(path,'sim', agents={'Red': B_lineAgent}), agent_name=\"Blue\", max_steps=100)\n",
    "\n",
    "        self.BL_def = \n",
    "        self.RM_def = \n",
    "\n",
    "        self.steps = 0\n",
    "        self.agent_name = 'BlueHier'\n",
    "\n",
    "        #action space is 2 for each trained agent to select from\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # observations for controller is a sliding window of 4 observations\n",
    "        self.observation_space = spaces.Box(-1.0,1.0,(self.mem_len,52), dtype=float)\n",
    "\n",
    "        #defuault observation is 4 lots of nothing\n",
    "        self.observation = np.zeros((self.mem_len,52))\n",
    "\n",
    "        self.action = None\n",
    "        self.env = self.BLenv\n",
    "\n",
    "    # reset doesnt reset the sliding window of the agent so it can differentiate between\n",
    "    # agents across episode boundaries\n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        #rest the environments of each attacker\n",
    "        self.BLenv.reset()\n",
    "        self.RMenv.reset()\n",
    "        if random.choice([0,1]) == 0:\n",
    "            self.env = self.BLenv\n",
    "        else:\n",
    "            self.env = self.RMenv\n",
    "        return np.zeros((self.mem_len,52))\n",
    "\n",
    "    def step(self, action=None):\n",
    "        # select agent\n",
    "        if action == 0:\n",
    "            # get action from agent trained against the B_lineAgent\n",
    "            agent_action = self.BL_def.compute_single_action(self.observation[-1:])\n",
    "        elif action == 1:\n",
    "            # get action from agent trained against the RedMeanderAgent\n",
    "            agent_action = self.RM_def.compute_single_action(self.observation[-1:])\n",
    "        else:\n",
    "            print('something went terribly wrong, old sport')\n",
    "        observation, reward, done, info = self.env.step(agent_action)\n",
    "\n",
    "        # update sliding window\n",
    "        self.observation = np.roll(self.observation, -1, 0) # Shift left by one to bring the oldest timestep on the rightmost position\n",
    "        self.observation[self.mem_len-1] = observation      # Replace what's on the rightmost position\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps == self.max_steps:\n",
    "            return self.observation, reward, True, info\n",
    "        assert(self.steps <= self.max_steps)\n",
    "        result = self.observation, reward, done, info\n",
    "        return result\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)\n",
    "\n",
    "def get_gym_env_args(env, if_print: bool) -> dict:\n",
    "    \"\"\"Get a dict ``env_args`` about a standard OpenAI gym env information.\n",
    "\n",
    "    param env: a standard OpenAI gym env\n",
    "    param if_print: [bool] print the dict about env information.\n",
    "    return: env_args [dict]\n",
    "\n",
    "    env_args = {\n",
    "        'env_name': env_name,       # [str] the environment name, such as XxxXxx-v0\n",
    "        'state_dim': state_dim,     # [int] the dimension of state\n",
    "        'action_dim': action_dim,   # [int] the dimension of action or the number of discrete action\n",
    "        'if_discrete': if_discrete, # [bool] action space is discrete or continuous\n",
    "    }\n",
    "    \"\"\"\n",
    "    state_shape = env.observation_space.shape\n",
    "\n",
    "    if_discrete = True\n",
    "    if if_discrete:  # make sure it is discrete action space\n",
    "        action_dim = env.action_space.n\n",
    "\n",
    "    env_name = 'cyborg.discrete.ppo'\n",
    "    state_dim = 52  # sometimes state_dim is a list\n",
    "    action_dim = 54\n",
    "    if_discrete = True\n",
    "\n",
    "    env_args = {'env_name': env_name,\n",
    "                'state_dim': state_dim,\n",
    "                'action_dim': action_dim,\n",
    "                'if_discrete': if_discrete, }\n",
    "    if if_print:\n",
    "        env_args_str = repr(env_args).replace(',', f\",\\n{'':11}\")\n",
    "        print(f\"env_args = {env_args_str}\")\n",
    "    return env_args\n",
    "\n",
    "\n",
    "def kwargs_filter(function, kwargs: dict) -> dict:\n",
    "    import inspect\n",
    "    sign = inspect.signature(function).parameters.values()\n",
    "    sign = {val.name for val in sign}\n",
    "    common_args = sign.intersection(kwargs.keys())\n",
    "    return {key: kwargs[key] for key in common_args}  # filtered kwargs\n",
    "\n",
    "\n",
    "def build_env(env_class=None, env_args=None):\n",
    "    return ChallengeWrapper(env=CybORG(path,'sim', agents={'Red': RedMeanderAgent}), agent_name=\"Blue\", max_steps=100)\n",
    "\n",
    "class AgentBase:\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.gamma = args.gamma\n",
    "        self.batch_size = args.batch_size\n",
    "        self.repeat_times = args.repeat_times\n",
    "        self.reward_scale = args.reward_scale\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.if_off_policy = args.if_off_policy\n",
    "        self.soft_update_tau = args.soft_update_tau\n",
    "\n",
    "        self.last_state = None  # save the last state of the trajectory for training. `last_state.shape == (state_dim)`\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "\n",
    "        act_class = getattr(self, \"act_class\", None)\n",
    "        cri_class = getattr(self, \"cri_class\", None)\n",
    "        self.act = self.act_target = act_class(net_dims, state_dim, action_dim).to(self.device)\n",
    "        self.cri = self.cri_target = cri_class(net_dims, state_dim, action_dim).to(self.device) \\\n",
    "            if cri_class else self.act\n",
    "\n",
    "        self.act_optimizer = torch.optim.Adam(self.act.parameters(), self.learning_rate)\n",
    "        self.cri_optimizer = torch.optim.Adam(self.cri.parameters(), self.learning_rate) \\\n",
    "            if cri_class else self.act_optimizer\n",
    "\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    @staticmethod\n",
    "    def optimizer_update(optimizer, objective: Tensor):\n",
    "        optimizer.zero_grad()\n",
    "        objective.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(target_net: torch.nn.Module, current_net: torch.nn.Module, tau: float):\n",
    "        # assert target_net is not current_net\n",
    "        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n",
    "            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))\n",
    "\n",
    "\n",
    "class AgentPPO(AgentBase):\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.if_off_policy = False\n",
    "        self.act_class = getattr(self, \"act_class\", ActorPPO)\n",
    "        self.cri_class = getattr(self, \"cri_class\", CriticPPO)\n",
    "        AgentBase.__init__(self, net_dims, state_dim, action_dim, gpu_id, args)\n",
    "\n",
    "        self.ratio_clip = getattr(args, \"ratio_clip\", 0.25)  # `ratio.clamp(1 - clip, 1 + clip)`\n",
    "        self.lambda_gae_adv = getattr(args, \"lambda_gae_adv\", 0.95)  # could be 0.80~0.99\n",
    "        self.lambda_entropy = getattr(args, \"lambda_entropy\", 0.01)  # could be 0.00~0.10\n",
    "        self.lambda_entropy = torch.tensor(self.lambda_entropy, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def explore_env(self, env, horizon_len: int) -> [Tensor]:\n",
    "        states = torch.zeros((horizon_len, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros(horizon_len, dtype=torch.int64).to(self.device)\n",
    "        logprobs = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.zeros(horizon_len, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        ary_state = self.last_state\n",
    "\n",
    "        get_action = self.act.get_action\n",
    "        for i in range(horizon_len):\n",
    "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device)\n",
    "            action, logprob = [t for t in get_action(state.unsqueeze(0))[:2]]\n",
    "\n",
    "            ary_action = action\n",
    "            ary_state, reward, done, info = env.step(ary_action)\n",
    "            if done:\n",
    "                ary_state = env.reset()\n",
    "\n",
    "            states[i] = state\n",
    "            actions[i] = action\n",
    "            logprobs[i] = logprob\n",
    "            rewards[i] = reward\n",
    "            dones[i] = done\n",
    "\n",
    "        self.last_state = ary_state\n",
    "        rewards = (rewards * self.reward_scale).unsqueeze(1)\n",
    "        undones = (1 - dones.type(torch.float32)).unsqueeze(1)\n",
    "        return states, actions, logprobs, rewards, undones\n",
    "\n",
    "    def update_net(self, buffer) -> [float]:\n",
    "        with torch.no_grad():\n",
    "            states, actions, logprobs, rewards, undones = buffer\n",
    "            buffer_size = states.shape[0]\n",
    "\n",
    "            '''get advantages reward_sums'''\n",
    "            bs = 2 ** 10  # set a smaller 'batch_size' when out of GPU memory.\n",
    "            values = [self.cri(states[i:i + bs]) for i in range(0, buffer_size, bs)]\n",
    "            values = torch.cat(values, dim=0).squeeze(1)  # values.shape == (buffer_size, )\n",
    "\n",
    "            advantages = self.get_advantages(rewards, undones, values)  # advantages.shape == (buffer_size, )\n",
    "            reward_sums = advantages + values  # reward_sums.shape == (buffer_size, )\n",
    "            del rewards, undones, values\n",
    "\n",
    "            # advantages = (advantages - advantages.mean()) / (advantages.std(dim=0) + 1e-5)\n",
    "        assert logprobs.shape == advantages.shape == reward_sums.shape == (buffer_size,)\n",
    "\n",
    "        '''update network'''\n",
    "        obj_critics = 0.0\n",
    "        obj_actors = 0.0\n",
    "\n",
    "        update_times = int(buffer_size * self.repeat_times / self.batch_size)\n",
    "        assert update_times >= 1\n",
    "        for _ in range(update_times):\n",
    "            indices = torch.randint(buffer_size, size=(self.batch_size,), requires_grad=False)\n",
    "            state = states[indices]\n",
    "            action = actions[indices]\n",
    "            logprob = logprobs[indices]\n",
    "            advantage = advantages[indices]\n",
    "            reward_sum = reward_sums[indices]\n",
    "\n",
    "            value = self.cri(state).squeeze(1)  # critic network predicts the reward_sum (Q value) of state\n",
    "            obj_critic = self.criterion(value, reward_sum)\n",
    "            self.optimizer_update(self.cri_optimizer, obj_critic)\n",
    "\n",
    "            new_logprob, obj_entropy = self.act.get_logprob_entropy(state, action)\n",
    "            ratio = (new_logprob - logprob.detach()).exp()\n",
    "            surrogate1 = advantage * ratio\n",
    "            surrogate2 = advantage * ratio.clamp(1 - self.ratio_clip, 1 + self.ratio_clip)\n",
    "            obj_surrogate = torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            obj_actor = obj_surrogate + obj_entropy.mean() * self.lambda_entropy\n",
    "            self.optimizer_update(self.act_optimizer, -obj_actor)\n",
    "\n",
    "            obj_critics += obj_critic.item()\n",
    "            obj_actors += obj_actor.item()\n",
    "        a_std_log = getattr(self.act, 'a_std_log', torch.zeros(1)).mean()\n",
    "        return obj_critics / update_times, obj_actors / update_times, a_std_log.item()\n",
    "\n",
    "    def get_advantages(self, rewards: Tensor, undones: Tensor, values: Tensor) -> Tensor:\n",
    "        advantages = torch.empty_like(values)  # advantage value\n",
    "\n",
    "        masks = undones * self.gamma\n",
    "        horizon_len = rewards.shape[0]\n",
    "\n",
    "        next_state = torch.tensor(self.last_state, dtype=torch.float32).to(self.device)\n",
    "        next_value = self.cri(next_state.unsqueeze(0)).detach().squeeze(1).squeeze(0)\n",
    "\n",
    "        advantage = 0  # last_gae_lambda\n",
    "        for t in range(horizon_len - 1, -1, -1):\n",
    "            delta = rewards[t] + masks[t] * next_value - values[t]\n",
    "            advantages[t] = advantage = delta + masks[t] * self.lambda_gae_adv * advantage\n",
    "            next_value = values[t]\n",
    "        return advantages\n",
    "\n",
    "\n",
    "def train_agent(args: Config):\n",
    "    args.init_before_training()\n",
    "\n",
    "    env = build_env(args.env_class, args.env_args)\n",
    "    agent = args.agent_class(args.net_dims, args.state_dim, args.action_dim, gpu_id=args.gpu_id, args=args)\n",
    "    agent.last_state = env.reset()\n",
    "\n",
    "    evaluator = Evaluator(eval_env=build_env(args.env_class, args.env_args),\n",
    "                          eval_per_step=args.eval_per_step,\n",
    "                          eval_times=args.eval_times,\n",
    "                          cwd=args.cwd)\n",
    "    with wandb.init(project=\"elegentrl.ppo.cyborg\", name=\"11-29_elegentrl.ppo.cyborg.3layerMlp\", dir=\"/home/zhx/word/DriverOrderOfflineRL/scripts/wandb\"):\n",
    "        wandb.watch(agent.act, log=\"gradients\", log_freq=10)\n",
    "        wandb.watch(agent.cri, log=\"gradients\", log_freq=10)\n",
    "    # for i in range(100):\n",
    "        while True:  # start training\n",
    "            buffer_items = agent.explore_env(env, args.horizon_len)\n",
    "\n",
    "            logging_tuple = agent.update_net(buffer_items)\n",
    "\n",
    "            # print(f\"100 len return: {buffer_items[3].sum() / 20}\")\n",
    "\n",
    "            evaluator.evaluate_and_save(agent.act, args.horizon_len, logging_tuple)\n",
    "            if (evaluator.total_step > args.break_step) or os.path.exists(f\"{args.cwd}/stop\"):\n",
    "                break  # stop training when reach `break_step` or `mkdir cwd/stop`\n",
    "\n",
    "\n",
    "def render_agent(env_class, env_args: dict, net_dims: [int], agent_class, actor_path: str, render_times: int = 8):\n",
    "    env = build_env(env_class, env_args)\n",
    "\n",
    "    state_dim = env_args['state_dim']\n",
    "    action_dim = env_args['action_dim']\n",
    "    agent = agent_class(net_dims, state_dim, action_dim, gpu_id=-1)\n",
    "    actor = agent.act\n",
    "\n",
    "    print(f\"| render and load actor from: {actor_path}\")\n",
    "    actor.load_state_dict(torch.load(actor_path, map_location=lambda storage, loc: storage))\n",
    "    for i in range(render_times):\n",
    "        cumulative_reward, episode_step = get_rewards_and_steps(env, actor, if_render=True)\n",
    "        print(f\"|{i:4}  cumulative_reward {cumulative_reward:9.3f}  episode_step {episode_step:5.0f}\")\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, eval_env, eval_per_step: int = 1e4, eval_times: int = 8, cwd: str = '.'):\n",
    "        self.cwd = cwd\n",
    "        self.env_eval = eval_env\n",
    "        self.eval_step = 0\n",
    "        self.total_step = 0\n",
    "        self.start_time = time.time()\n",
    "        self.eval_times = eval_times  # number of times that get episodic cumulative return\n",
    "        self.eval_per_step = eval_per_step  # evaluate the agent per training steps\n",
    "\n",
    "        self.recorder = []\n",
    "        print(f\"\\n| `step`: Number of samples, or total training steps, or running times of `env.step()`.\"\n",
    "              f\"\\n| `time`: Time spent from the start of training to this moment.\"\n",
    "              f\"\\n| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              f\"\\n| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              f\"\\n| `avgS`: Average of steps in an episode.\"\n",
    "              f\"\\n| `objC`: Objective of Critic network. Or call it loss function of critic network.\"\n",
    "              f\"\\n| `objA`: Objective of Actor network. It is the average Q value of the critic network.\"\n",
    "              f\"\\n| {'step':>8}  {'time':>8}  | {'avgR':>8}  {'stdR':>6}  {'avgS':>6}  | {'objC':>8}  {'objA':>8}\")\n",
    "\n",
    "    def evaluate_and_save(self, actor, horizon_len: int, logging_tuple: tuple):\n",
    "        self.total_step += horizon_len\n",
    "        # if self.eval_step + self.eval_per_step > self.total_step:\n",
    "        #     return\n",
    "        self.eval_step = self.total_step\n",
    "\n",
    "        rewards_steps_ary = [get_rewards_and_steps(self.env_eval, actor) for _ in range(self.eval_times)]\n",
    "        rewards_steps_ary = np.array(rewards_steps_ary, dtype=np.float32)\n",
    "        avg_r = rewards_steps_ary[:, 0].mean()  # average of cumulative rewards\n",
    "        std_r = rewards_steps_ary[:, 0].std()  # std of cumulative rewards\n",
    "        avg_s = rewards_steps_ary[:, 1].mean()  # average of steps in an episode\n",
    "\n",
    "        used_time = time.time() - self.start_time\n",
    "        self.recorder.append((self.total_step, used_time, avg_r))\n",
    "\n",
    "        print(\"test_result: \", avg_r)\n",
    "        print(f\"|step: {self.total_step:8.2e}  used_time:{used_time:8.0f}  \"\n",
    "              f\"| avg_r:{avg_r:8.2f}  std_r:{std_r:6.2f}  avg_s:{avg_s:6.0f}  \"\n",
    "              f\"| objC:{logging_tuple[0]:8.2f}  objA:{logging_tuple[1]:8.2f}\")\n",
    "        wandb.log({\n",
    "            \"totoal_step\": self.total_step,\n",
    "            \"used_time\": used_time,\n",
    "            \"avg_r\": avg_r,\n",
    "            \"std_r\": std_r,\n",
    "            \"avg_s\": avg_s,\n",
    "            \"objC\": logging_tuple[0],\n",
    "            \"objA\": logging_tuple[1]})\n",
    "\n",
    "\n",
    "def get_rewards_and_steps(env, actor, if_render: bool = False) -> (float, int):  # cumulative_rewards and episode_steps\n",
    "    device = next(actor.parameters()).device  # net.parameters() is a Python generator.\n",
    "\n",
    "    state = env.reset()\n",
    "    episode_steps = 0\n",
    "    cumulative_returns = 0.0  # sum of rewards in an episode\n",
    "    for episode_steps in range(500):\n",
    "        tensor_state = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        tensor_action, _ = actor.get_action(tensor_state)\n",
    "        action = tensor_action  # not need detach(), because using torch.no_grad() outside\n",
    "        state, reward, done, info = env.step(action)\n",
    "        cumulative_returns += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return cumulative_returns, episode_steps + 1\n",
    "\n",
    "\n",
    "\n",
    "def train_ppo():\n",
    "    agent_class = AgentPPO  # DRL algorithm name\n",
    "    # env = ChallengeWrapper(env=CybORG(path,'sim', agents={'Red': RedMeanderAgent}), agent_name=\"Blue\", max_steps=100)\n",
    "    env = ChallengeWrapper(env=CybORG(path,'sim', agents={'Red': RedMeanderAgent}), agent_name=\"Blue\", max_steps=100)\n",
    "    env_class = None  # run a custom env: PendulumEnv, which based on OpenAI pendulum\n",
    "    env_args = {\n",
    "        'env_name': 'cyborg',  # Apply torque on the free end to swing a pendulum into an upright position\n",
    "        'state_dim': 52,  # the x-y coordinates of the pendulum's free end and its angular velocity.\n",
    "        'action_dim': 54,  # the torque applied to free end of the pendulum\n",
    "        'if_discrete': True  # continuous action space, symbols → direction, value → force\n",
    "    }\n",
    "    \n",
    "    # env\n",
    "    get_gym_env_args(env=env, if_print=True)  # return env_args\n",
    "\n",
    "    args = Config(agent_class, env_class, env_args)  # see `config.py Arguments()` for hyperparameter explanation\n",
    "    args.env = env\n",
    "    args.break_step = int(2e5 * 2000)  # break training if 'total_step > break_step'\n",
    "    args.net_dims = (256, 256)  # the middle layer dimension of MultiLayer Perceptron\n",
    "    args.gamma = 0.97  # discount factor of future rewards\n",
    "    args.repeat_times = 4  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
    "\n",
    "    # seed\n",
    "    np.random.seed(args.random_seed)\n",
    "    torch.manual_seed(args.random_seed)\n",
    "    train_agent(args)\n",
    "\n",
    "def train_hier():\n",
    "    agent_class = AgentPPO  # DRL algorithm name\n",
    "    env = HierEnv()\n",
    "    env_class = None\n",
    "    env_args = {\n",
    "        'env_name': 'cyborg.hierContorller',  # Apply torque on the free end to swing a pendulum into an upright position\n",
    "        'state_dim': 52 * 4,  # the x-y coordinates of the pendulum's free end and its angular velocity.\n",
    "        'action_dim': 54,  # the torque applied to free end of the pendulum\n",
    "        'if_discrete': True  # continuous action space, symbols → direction, value → force\n",
    "    }\n",
    "    # env\n",
    "    args = Config(agent_class, env_class, env_args)  # see `config.py Arguments()` for hyperparameter explanation\n",
    "    args.env = env\n",
    "    args.break_step = int(2e5 * 2000)  # break training if 'total_step > break_step'\n",
    "    args.net_dims = (256, 256)  # the middle layer dimension of MultiLayer Perceptron\n",
    "    args.gamma = 0.97  # discount factor of future rewards\n",
    "    args.repeat_times = 4  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
    "\n",
    "    # seed\n",
    "    np.random.seed(args.random_seed)\n",
    "    torch.manual_seed(args.random_seed)\n",
    "    train_agent(args) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_class = AgentPPO  # DRL algorithm name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HierEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_class = None\n",
    "env_args = {\n",
    "    'env_name': 'cyborg.hierContorller',  # Apply torque on the free end to swing a pendulum into an upright position\n",
    "    'state_dim': 52 * 4,  # the x-y coordinates of the pendulum's free end and its angular velocity.\n",
    "    'action_dim': 54,  # the torque applied to free end of the pendulum\n",
    "    'if_discrete': True  # continuous action space, symbols → direction, value → force\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f23d84727b0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env\n",
    "args = Config(agent_class, env_class, env_args)  # see `config.py Arguments()` for hyperparameter explanation\n",
    "args.env = env\n",
    "args.break_step = int(2e5 * 2000)  # break training if 'total_step > break_step'\n",
    "args.net_dims = (256, 256)  # the middle layer dimension of MultiLayer Perceptron\n",
    "args.gamma = 0.97  # discount factor of future rewards\n",
    "args.repeat_times = 4  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
    "\n",
    "# seed\n",
    "np.random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| `step`: Number of samples, or total training steps, or running times of `env.step()`.\n",
      "| `time`: Time spent from the start of training to this moment.\n",
      "| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `avgS`: Average of steps in an episode.\n",
      "| `objC`: Objective of Critic network. Or call it loss function of critic network.\n",
      "| `objA`: Objective of Actor network. It is the average Q value of the critic network.\n",
      "|     step      time  |     avgR    stdR    avgS  |     objC      objA\n",
      "something went terribly wrong, old sport\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'agent_action' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb 单元格 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:  \u001b[39m# start training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         buffer_items \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mexplore_env(env, args\u001b[39m.\u001b[39;49mhorizon_len)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         logging_tuple \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mupdate_net(buffer_items)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m# print(f\"100 len return: {buffer_items[3].sum() / 20}\")\u001b[39;00m\n",
      "\u001b[1;32m/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb 单元格 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=311'>312</a>\u001b[0m action, logprob \u001b[39m=\u001b[39m [t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m get_action(state\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))[:\u001b[39m2\u001b[39m]]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=313'>314</a>\u001b[0m ary_action \u001b[39m=\u001b[39m action\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=314'>315</a>\u001b[0m ary_state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(ary_action)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=315'>316</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=316'>317</a>\u001b[0m     ary_state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n",
      "\u001b[1;32m/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb 单元格 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=182'>183</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=183'>184</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msomething went terribly wrong, old sport\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=184'>185</a>\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(agent_action)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=186'>187</a>\u001b[0m \u001b[39m# update sliding window\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/testHier.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=187'>188</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mroll(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m# Shift left by one to bring the oldest timestep on the rightmost position\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'agent_action' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "args.init_before_training()\n",
    "\n",
    "env = args.env\n",
    "agent = args.agent_class(args.net_dims, args.state_dim, args.action_dim, gpu_id=args.gpu_id, args=args)\n",
    "agent.last_state = env.reset()\n",
    "\n",
    "evaluator = Evaluator(eval_env=HierEnv(),\n",
    "                        eval_per_step=args.eval_per_step,\n",
    "                        eval_times=args.eval_times,\n",
    "                        cwd=args.cwd)\n",
    "# with wandb.init(project=\"elegentrl.ppo.cyborg\", name=\"11-29_elegentrl.ppo.cyborg.3layerMlp\", dir=\"/home/zhx/word/DriverOrderOfflineRL/scripts/wandb\"):\n",
    "#     wandb.watch(agent.act, log=\"gradients\", log_freq=10)\n",
    "#     wandb.watch(agent.cri, log=\"gradients\", log_freq=10)\n",
    "for i in range(100):\n",
    "    while True:  # start training\n",
    "        buffer_items = agent.explore_env(env, args.horizon_len)\n",
    "\n",
    "        logging_tuple = agent.update_net(buffer_items)\n",
    "\n",
    "        # print(f\"100 len return: {buffer_items[3].sum() / 20}\")\n",
    "\n",
    "        evaluator.evaluate_and_save(agent.act, args.horizon_len, logging_tuple)\n",
    "        # if (evaluator.total_step > args.break_step) or os.path.exists(f\"{args.cwd}/stop\"):\n",
    "        #     break  # stop training when reach `break_step` or `mkdir cwd/stop`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianshou",
   "language": "python",
   "name": "tianshou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
