{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence, Callable\n",
    "from typing import Any, cast, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/zhx/word/work/DriverOrderOfflineRL/cage-challenge-1/CybORG')\n",
    "sys.path.append('/home/zhx/word/work/DriverOrderOfflineRL/tianshou')\n",
    "sys.path.append('/home/zhx/word/work/DriverOrderOfflineRL/tianshou/examples/atari')\n",
    "sys.path.append('/tmp-data/zhx/DriverOrderOfflineRL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3dc8e2dc50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "import torch\n",
    "from pprint import pprint\n",
    "from CybORG import CybORG\n",
    "from CybORG.Shared.Actions import *\n",
    "from CybORG.Agents import RedMeanderAgent, B_lineAgent\n",
    "from CybORG.Agents.Wrappers import *\n",
    "path = str(inspect.getfile(CybORG))\n",
    "path = path[:-10] + '/Shared/Scenarios/Scenario1b.yaml'\n",
    "\n",
    "# seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/zhx/.conda/envs/academy/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| `step`: Number of samples, or total training steps, or running times of `env.step()`.\n",
      "| `time`: Time spent from the start of training to this moment.\n",
      "| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\n",
      "| `avgS`: Average of steps in an episode.\n",
      "| `objC`: Objective of Critic network. Or call it loss function of critic network.\n",
      "| `objA`: Objective of Actor network. It is the average Q value of the critic network.\n",
      "|     step      time  |     avgR    stdR    avgS  |     objC      objA\n",
      "| 2.05e+04        64  |    10.00    0.00      10  |     0.33     20.69\n",
      "| 4.10e+04       171  |    10.00    0.00      10  |     0.17     19.77\n",
      "| 6.14e+04       320  |    10.00    0.00      10  |     0.14     19.71\n",
      "| 8.19e+04       509  |    10.00    0.00      10  |     0.11     19.53\n",
      "| 1.02e+05       740  |    10.00    0.00      10  |     0.09     19.34\n",
      "| 1.23e+05       999  |    10.00    0.00      10  |     0.08     19.26\n",
      "| 1.43e+05      1311  |    10.00    0.00      10  |     0.07     19.21\n",
      "| 1.64e+05      1656  |    10.00    0.00      10  |     0.07     19.25\n",
      "| 1.84e+05      2049  |    10.00    0.00      10  |     0.06     19.39\n",
      "| 2.05e+05      2487  |    10.00    0.00      10  |     0.06     19.44\n",
      "| 2.25e+05      2961  |    10.00    0.00      10  |     0.06     19.24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Config:  # for off-policy\n",
    "    def __init__(self, agent_class=None, env_class=None, env_args=None):\n",
    "        self.agent_class = agent_class  # agent = agent_class(...)\n",
    "        self.if_off_policy = True  # whether off-policy or on-policy of DRL algorithm\n",
    "\n",
    "        self.env_class = env_class  # env = env_class(**env_args)\n",
    "        self.env_args = env_args  # env = env_class(**env_args)\n",
    "        if env_args is None:  # dummy env_args\n",
    "            env_args = {'env_name': None, 'state_dim': None, 'action_dim': None, 'if_discrete': None}\n",
    "        self.env_name = env_args['env_name']  # the name of environment. Be used to set 'cwd'.\n",
    "        self.state_dim = env_args['state_dim']  # vector dimension (feature number) of state\n",
    "        self.action_dim = env_args['action_dim']  # vector dimension (feature number) of action\n",
    "        self.if_discrete = env_args['if_discrete']  # discrete or continuous action space\n",
    "\n",
    "        '''Arguments for reward shaping'''\n",
    "        self.gamma = 0.99  # discount factor of future rewards\n",
    "        self.reward_scale = 1.0  # an approximate target reward usually be closed to 256\n",
    "\n",
    "        '''Arguments for training'''\n",
    "        self.net_dims = (256, 256)  # the middle layer dimension of MLP (MultiLayer Perceptron)\n",
    "        self.learning_rate = 6e-5  # 2 ** -14 ~= 6e-5\n",
    "        self.soft_update_tau = 5e-3  # 2 ** -8 ~= 5e-3\n",
    "        self.batch_size = int(64)  # num of transitions sampled from replay buffer.\n",
    "        self.horizon_len = int(512)  # collect horizon_len step while exploring, then update network\n",
    "        self.buffer_size = int(1e6)  # ReplayBuffer size. First in first out for off-policy.\n",
    "        self.repeat_times = 1.0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
    "\n",
    "        '''Arguments for device'''\n",
    "        self.gpu_id = int(1)  # `int` means the ID of single GPU, -1 means CPU\n",
    "        self.thread_num = int(8)  # cpu_num for pytorch, `torch.set_num_threads(self.num_threads)`\n",
    "        self.random_seed = int(0)  # initialize random seed in self.init_before_training()\n",
    "\n",
    "        '''Arguments for evaluate'''\n",
    "        self.cwd = None  # current working directory to save model. None means set automatically\n",
    "        self.if_remove = True  # remove the cwd folder? (True, False, None:ask me)\n",
    "        self.break_step = +np.inf  # break training if 'total_step > break_step'\n",
    "\n",
    "        self.eval_times = int(32)  # number of times that get episodic cumulative return\n",
    "        self.eval_per_step = int(2e4)  # evaluate the agent per training steps\n",
    "\n",
    "    def init_before_training(self):\n",
    "        if self.cwd is None:  # set cwd (current working directory) for saving model\n",
    "            self.cwd = f'./{self.env_name}_{self.agent_class.__name__[5:]}'\n",
    "        os.makedirs(self.cwd, exist_ok=True)\n",
    "\n",
    "\n",
    "class QNet(nn.Module):  # `nn.Module` is a PyTorch module for neural network\n",
    "    def __init__(self, dims: [int], state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, action_dim])\n",
    "        self.explore_rate = None\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        return self.net(state)  # Q values for multiple actions\n",
    "\n",
    "    def get_action(self, state: Tensor) -> Tensor:  # return the index [int] of discrete action for exploration\n",
    "        if self.explore_rate < torch.rand(1):\n",
    "            action = self.net(state).argmax(dim=1, keepdim=True)\n",
    "        else:\n",
    "            action = torch.randint(self.action_dim, size=(state.shape[0], 1))\n",
    "        return action\n",
    "\n",
    "\n",
    "def build_mlp(dims: [int]) -> nn.Sequential:  # MLP (MultiLayer Perceptron)\n",
    "    net_list = []\n",
    "    for i in range(len(dims) - 1):\n",
    "        net_list.extend([nn.Linear(dims[i], dims[i + 1]), nn.ReLU()])\n",
    "    del net_list[-1]  # remove the activation of output layer\n",
    "    return nn.Sequential(*net_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kwargs_filter(function, kwargs: dict) -> dict:\n",
    "    import inspect\n",
    "    sign = inspect.signature(function).parameters.values()\n",
    "    sign = {val.name for val in sign}\n",
    "    common_args = sign.intersection(kwargs.keys())\n",
    "    return {key: kwargs[key] for key in common_args}  # filtered kwargs\n",
    "\n",
    "\n",
    "def build_env(env_class=None, env_args=None):\n",
    "    return gym.make('CartPole-v1')\n",
    "    # return ChallengeWrapper(env=CybORG(path,'sim', agents={'Red': B_lineAgent}), agent_name=\"Blue\", max_steps=100)\n",
    "\n",
    "\n",
    "class AgentBase:\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.gamma = args.gamma\n",
    "        self.batch_size = args.batch_size\n",
    "        self.repeat_times = args.repeat_times\n",
    "        self.reward_scale = args.reward_scale\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.if_off_policy = args.if_off_policy\n",
    "        self.soft_update_tau = args.soft_update_tau\n",
    "\n",
    "        self.last_state = None  # save the last state of the trajectory for training. `last_state.shape == (state_dim)`\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "\n",
    "        act_class = getattr(self, \"act_class\", None)\n",
    "        cri_class = getattr(self, \"cri_class\", None)\n",
    "        self.act = self.act_target = act_class(net_dims, state_dim, action_dim).to(self.device)\n",
    "        self.cri = self.cri_target = cri_class(net_dims, state_dim, action_dim).to(self.device) \\\n",
    "            if cri_class else self.act\n",
    "\n",
    "        self.act_optimizer = torch.optim.Adam(self.act.parameters(), self.learning_rate)\n",
    "        self.cri_optimizer = torch.optim.Adam(self.cri.parameters(), self.learning_rate) \\\n",
    "            if cri_class else self.act_optimizer\n",
    "\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    @staticmethod\n",
    "    def optimizer_update(optimizer, objective: Tensor):\n",
    "        optimizer.zero_grad()\n",
    "        objective.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(target_net: torch.nn.Module, current_net: torch.nn.Module, tau: float):\n",
    "        # assert target_net is not current_net\n",
    "        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n",
    "            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))\n",
    "\n",
    "\n",
    "class AgentDQN(AgentBase):\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.act_class = getattr(self, \"act_class\", QNet)\n",
    "        self.cri_class = getattr(self, \"cri_class\", None)  # means `self.cri = self.act`\n",
    "        AgentBase.__init__(self, net_dims, state_dim, action_dim, gpu_id, args)\n",
    "        self.act_target = self.cri_target = deepcopy(self.act)\n",
    "\n",
    "        self.act.explore_rate = getattr(args, \"explore_rate\", 0.25)  # set for `self.act.get_action()`\n",
    "        # the probability of choosing action randomly in epsilon-greedy\n",
    "\n",
    "    def explore_env(self, env, horizon_len: int, if_random: bool = False) -> [Tensor]:\n",
    "        states = torch.zeros((horizon_len, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len, 1), dtype=torch.int32).to(self.device)\n",
    "        rewards = torch.ones(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.zeros(horizon_len, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        ary_state = self.last_state\n",
    "\n",
    "        get_action = self.act.get_action\n",
    "        for i in range(horizon_len):\n",
    "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device)\n",
    "            if if_random:\n",
    "                action = torch.randint(self.action_dim, size=(1,))[0]\n",
    "            else:\n",
    "                action = get_action(state.unsqueeze(0))[0, 0]\n",
    "\n",
    "            ary_action = action.detach().cpu().numpy()\n",
    "            ary_state, reward, done, _ = env.step(ary_action)\n",
    "            if done:\n",
    "                ary_state = env.reset()\n",
    "\n",
    "            states[i] = state\n",
    "            actions[i] = action\n",
    "            rewards[i] = reward\n",
    "            dones[i] = done\n",
    "\n",
    "        self.last_state = ary_state\n",
    "        rewards = (rewards * self.reward_scale).unsqueeze(1)\n",
    "        undones = (1.0 - dones.type(torch.float32)).unsqueeze(1)\n",
    "        return states, actions, rewards, undones\n",
    "\n",
    "    def update_net(self, buffer) -> [float]:\n",
    "        obj_critics = 0.0\n",
    "        q_values = 0.0\n",
    "\n",
    "        update_times = int(buffer.cur_size * self.repeat_times / self.batch_size)\n",
    "        assert update_times >= 1\n",
    "        for i in range(update_times):\n",
    "            obj_critic, q_value = self.get_obj_critic(buffer, self.batch_size)\n",
    "            self.optimizer_update(self.cri_optimizer, obj_critic)\n",
    "            self.soft_update(self.cri_target, self.cri, self.soft_update_tau)\n",
    "\n",
    "            obj_critics += obj_critic.item()\n",
    "            q_values += q_value.item()\n",
    "        return obj_critics / update_times, q_values / update_times\n",
    "\n",
    "    def get_obj_critic(self, buffer, batch_size: int) -> (Tensor, Tensor):\n",
    "        with torch.no_grad():\n",
    "            state, action, reward, undone, next_state = buffer.sample(batch_size)\n",
    "            next_q = self.cri_target(next_state).max(dim=1, keepdim=True)[0]\n",
    "            q_label = reward + undone * self.gamma * next_q\n",
    "        q_value = self.cri(state).gather(1, action.long())\n",
    "        obj_critic = self.criterion(q_value, q_label)\n",
    "        return obj_critic, q_value.mean()\n",
    "\n",
    "\n",
    "class ReplayBuffer:  # for off-policy\n",
    "    def __init__(self, max_size: int, state_dim: int, action_dim: int, gpu_id: int = 0):\n",
    "        self.p = 0  # pointer\n",
    "        self.if_full = False\n",
    "        self.cur_size = 0\n",
    "        self.max_size = max_size\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "\n",
    "        self.states = torch.empty((max_size, state_dim), dtype=torch.float32, device=self.device)\n",
    "        self.actions = torch.empty((max_size, action_dim), dtype=torch.float32, device=self.device)\n",
    "        self.rewards = torch.empty((max_size, 1), dtype=torch.float32, device=self.device)\n",
    "        self.undones = torch.empty((max_size, 1), dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def update(self, items: [Tensor]):\n",
    "        states, actions, rewards, undones = items\n",
    "        p = self.p + rewards.shape[0]  # pointer\n",
    "        if p > self.max_size:\n",
    "            self.if_full = True\n",
    "            p0 = self.p\n",
    "            p1 = self.max_size\n",
    "            p2 = self.max_size - self.p\n",
    "            p = p - self.max_size\n",
    "\n",
    "            self.states[p0:p1], self.states[0:p] = states[:p2], states[-p:]\n",
    "            self.actions[p0:p1], self.actions[0:p] = actions[:p2], actions[-p:]\n",
    "            self.rewards[p0:p1], self.rewards[0:p] = rewards[:p2], rewards[-p:]\n",
    "            self.undones[p0:p1], self.undones[0:p] = undones[:p2], undones[-p:]\n",
    "        else:\n",
    "            self.states[self.p:p] = states\n",
    "            self.actions[self.p:p] = actions\n",
    "            self.rewards[self.p:p] = rewards\n",
    "            self.undones[self.p:p] = undones\n",
    "        self.p = p\n",
    "        self.cur_size = self.max_size if self.if_full else self.p\n",
    "\n",
    "    def sample(self, batch_size: int) -> [Tensor]:\n",
    "        ids = torch.randint(self.cur_size - 1, size=(batch_size,), requires_grad=False)\n",
    "        return self.states[ids], self.actions[ids], self.rewards[ids], self.undones[ids], self.states[ids + 1]\n",
    "\n",
    "\n",
    "def train_agent(args: Config):\n",
    "    args.init_before_training()\n",
    "\n",
    "    env = build_env(args.env_class, args.env_args)\n",
    "    agent = args.agent_class(args.net_dims, args.state_dim, args.action_dim, gpu_id=0, args=args)\n",
    "    agent.last_state = env.reset()\n",
    "    buffer = ReplayBuffer(gpu_id=0, max_size=args.buffer_size,\n",
    "                          state_dim=args.state_dim, action_dim=1 if args.if_discrete else args.action_dim, )\n",
    "    buffer_items = agent.explore_env(env, args.horizon_len * args.eval_times, if_random=True)\n",
    "    buffer.update(buffer_items)  # warm up for ReplayBuffer\n",
    "\n",
    "    evaluator = Evaluator(eval_env=build_env(args.env_class, args.env_args),\n",
    "                          eval_per_step=args.eval_per_step, eval_times=args.eval_times, cwd=args.cwd)\n",
    "    torch.set_grad_enabled(False)\n",
    "    while True:  # start training\n",
    "        buffer_items = agent.explore_env(env, args.horizon_len)\n",
    "        buffer.update(buffer_items)\n",
    "\n",
    "        torch.set_grad_enabled(True)\n",
    "        logging_tuple = agent.update_net(buffer)\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        evaluator.evaluate_and_save(agent.act, args.horizon_len, logging_tuple)\n",
    "        if (evaluator.total_step > args.break_step) or os.path.exists(f\"{args.cwd}/stop\"):\n",
    "            break  # stop training when reach `break_step` or `mkdir cwd/stop`\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, eval_env, eval_per_step: int = 1e4, eval_times: int = 8, cwd: str = '.'):\n",
    "        self.cwd = cwd\n",
    "        self.env_eval = eval_env\n",
    "        self.eval_step = 0\n",
    "        self.total_step = 0\n",
    "        self.start_time = time.time()\n",
    "        self.eval_times = eval_times  # number of times that get episodic cumulative return\n",
    "        self.eval_per_step = eval_per_step  # evaluate the agent per training steps\n",
    "\n",
    "        self.recorder = []\n",
    "        print(\"\\n| `step`: Number of samples, or total training steps, or running times of `env.step()`.\"\n",
    "              \"\\n| `time`: Time spent from the start of training to this moment.\"\n",
    "              \"\\n| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              \"\\n| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              \"\\n| `avgS`: Average of steps in an episode.\"\n",
    "              \"\\n| `objC`: Objective of Critic network. Or call it loss function of critic network.\"\n",
    "              \"\\n| `objA`: Objective of Actor network. It is the average Q value of the critic network.\"\n",
    "              f\"\\n| {'step':>8}  {'time':>8}  | {'avgR':>8}  {'stdR':>6}  {'avgS':>6}  | {'objC':>8}  {'objA':>8}\")\n",
    "\n",
    "    def evaluate_and_save(self, actor, horizon_len: int, logging_tuple: tuple):\n",
    "        self.total_step += horizon_len\n",
    "        if self.eval_step + self.eval_per_step > self.total_step:\n",
    "            return\n",
    "        self.eval_step = self.total_step\n",
    "\n",
    "        rewards_steps_ary = [get_rewards_and_steps(self.env_eval, actor) for _ in range(self.eval_times)]\n",
    "        rewards_steps_ary = np.array(rewards_steps_ary, dtype=np.float32)\n",
    "        avg_r = rewards_steps_ary[:, 0].mean()  # average of cumulative rewards\n",
    "        std_r = rewards_steps_ary[:, 0].std()  # std of cumulative rewards\n",
    "        avg_s = rewards_steps_ary[:, 1].mean()  # average of steps in an episode\n",
    "\n",
    "        used_time = time.time() - self.start_time\n",
    "        self.recorder.append((self.total_step, used_time, avg_r))\n",
    "\n",
    "        print(f\"| {self.total_step:8.2e}  {used_time:8.0f}  \"\n",
    "              f\"| {avg_r:8.2f}  {std_r:6.2f}  {avg_s:6.0f}  \"\n",
    "              f\"| {logging_tuple[0]:8.2f}  {logging_tuple[1]:8.2f}\")\n",
    "\n",
    "\n",
    "def get_rewards_and_steps(env, actor, if_render: bool = False) -> (float, int):  # cumulative_rewards and episode_steps\n",
    "    device = next(actor.parameters()).device  # net.parameters() is a Python generator.\n",
    "    state = env.reset()\n",
    "    episode_steps = 0\n",
    "    cumulative_returns = 0.0  # sum of rewards in an episode\n",
    "    for episode_steps in range(10):\n",
    "        tensor_state = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        tensor_action = actor(tensor_state).argmax(dim=1)\n",
    "        action = tensor_action.detach().cpu().numpy()[0]  # not need detach(), because using torch.no_grad() outside\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        cumulative_returns += reward\n",
    "\n",
    "        if if_render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "    return cumulative_returns, episode_steps + 1\n",
    "\n",
    "\n",
    "def train_dqn():\n",
    "    # env_args = {\n",
    "    #     'env_name': 'cyborg',  # A pole is attached by an un-actuated joint to a cart.\n",
    "    #     'state_dim': 52,  # (CartPosition, CartVelocity, PoleAngle, PoleAngleVelocity)\n",
    "    #     'action_dim': 54,  # (Push cart to the left, Push cart to the right)\n",
    "    #     'if_discrete': True,  # discrete action space\n",
    "    # }  # env_args = get_gym_env_args(env=gym.make('CartPole-v0'), if_print=True)\n",
    "\n",
    "    env_args = {\n",
    "        'env_name': 'CartPole-v1',  # A pole is attached by an un-actuated joint to a cart.\n",
    "        'state_dim': 4,  # (CartPosition, CartVelocity, PoleAngle, PoleAngleVelocity)\n",
    "        'action_dim': 2,  # (Push cart to the left, Push cart to the right)\n",
    "        'if_discrete': True,  # discrete action space\n",
    "    }  # env_args = get_gym_env_args(env=gym.make('CartPole-v0'), if_print=True)\n",
    "\n",
    "    args = Config(agent_class=AgentDQN, env_class=gym.make, env_args=env_args)  # see `Config` for explanation\n",
    "    args.break_step = int(2e5 * 2000)  # break training if 'total_step > break_step'\n",
    "    args.net_dims = (256, 256)  # the middle layer dimension of MultiLayer Perceptron\n",
    "    args.gamma = 0.95  # discount factor of future rewards\n",
    "    args.cwd = '/home/zhx/word/work/DriverOrderOfflineRL/lab_result'\n",
    "\n",
    "    train_agent(args)\n",
    "\n",
    "\n",
    "train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntrinsicCuriosityModule(nn.Module):\n",
    "    \"\"\"Implementation of Intrinsic Curiosity Module. arXiv:1705.05363.\n",
    "\n",
    "    :param feature_net: a self-defined feature_net which output a\n",
    "        flattened hidden state.\n",
    "    :param feature_dim: input dimension of the feature net.\n",
    "    :param action_dim: dimension of the action space.\n",
    "    :param hidden_sizes: hidden layer sizes for forward and inverse models.\n",
    "    :param device: device for the module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_sizes: Sequence[int] = (),\n",
    "        device: str or torch.device = \"cpu\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.feature_net = feature_net\n",
    "        self.forward_model = MLP(\n",
    "            feature_dim + action_dim,\n",
    "            output_dim=feature_dim,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            device=device,\n",
    "        )\n",
    "        self.inverse_model = MLP(\n",
    "            feature_dim * 2,\n",
    "            output_dim=action_dim,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            device=device,\n",
    "        )\n",
    "        self.feature_dim = feature_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "    def forward(\n",
    "        self,\n",
    "        s1,\n",
    "        act,\n",
    "        s2,\n",
    "        **kwargs: Any,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Mapping: s1, act, s2 -> mse_loss, act_hat.\"\"\"\n",
    "        s1 = to_torch(s1, dtype=torch.float32, device=self.device)\n",
    "        s2 = to_torch(s2, dtype=torch.float32, device=self.device)\n",
    "        phi1, phi2 = self.feature_net(s1), self.feature_net(s2)\n",
    "        act = to_torch(act, dtype=torch.long, device=self.device)\n",
    "        phi2_hat = self.forward_model(\n",
    "            torch.cat([phi1, F.one_hot(act, num_classes=self.action_dim)], dim=1),\n",
    "        )\n",
    "        mse_loss = 0.5 * F.mse_loss(phi2_hat, phi2, reduction=\"none\").sum(1)\n",
    "        act_hat = self.inverse_model(torch.cat([phi1, phi2], dim=1))\n",
    "        return mse_loss, act_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academy",
   "language": "python",
   "name": "academy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
