{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "HOME = os.path.expanduser('~')\n",
    "TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "SAVEPATH = '/home/adryw/dataset/ehdqn/ckpts/'\n",
    "SAVEPATH = '../ckpts/' if not os.path.isdir(SAVEPATH) else SAVEPATH\n",
    "SAVEPATH = os.path.join(SAVEPATH, TIME)\n",
    "LOGPATH = HOME + '/dataset/ehdqn/logs/' + TIME\n",
    "#LOGPATH = '../logs/' + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "class Setting:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "sett = Setting()\n",
    "sett.device = device\n",
    "sett.HOME = HOME\n",
    "sett.SAVEPATH = SAVEPATH\n",
    "sett.LOGPATH = LOGPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_memory):\n",
    "        self.max_memory = max_memory\n",
    "        self.state = []\n",
    "        self.new_state = []\n",
    "        self.action = []\n",
    "        self.reward = []\n",
    "        self.is_terminal = []\n",
    "        self.idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.state)\n",
    "\n",
    "    def store_transition(self, s, s1, a, r, is_terminal):\n",
    "        if len(self.state) <= self.max_memory:\n",
    "            self.state.append(s)\n",
    "            self.new_state.append(s1)\n",
    "            self.action.append(a)\n",
    "            self.reward.append(r)\n",
    "            self.is_terminal.append(is_terminal)\n",
    "        else:\n",
    "            self.state[self.idx] = s\n",
    "            self.new_state[self.idx] = s1\n",
    "            self.action[self.idx] = a\n",
    "            self.reward[self.idx] = r\n",
    "            self.is_terminal[self.idx] = is_terminal\n",
    "            self.idx = (self.idx + 1) % self.max_memory\n",
    "        assert len(self.state) == len(self.new_state) == len(self.reward) == len(self.is_terminal) == len(self.action)\n",
    "\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.state[:]\n",
    "        del self.new_state[:]\n",
    "        del self.action[:]\n",
    "        del self.reward[:]\n",
    "        del self.is_terminal[:]\n",
    "\n",
    "    def sample(self, bs):\n",
    "        idx = np.random.randint(len(self.state), size=bs)\n",
    "        state, new_state, action, reward, is_terminal = [], [], [], [], []\n",
    "        for i in idx:\n",
    "            state.append(self.state[i])\n",
    "            new_state.append(self.new_state[i])\n",
    "            action.append(self.action[i])\n",
    "            reward.append(self.reward[i])\n",
    "            is_terminal.append(int(self.is_terminal[i]))\n",
    "        return state, new_state, action, reward, is_terminal\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICM_Model(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Relu()\n",
    "        )\n",
    "\n",
    "        out_shape = 256\n",
    "\n",
    "        # Forward Model\n",
    "        self.fwd = nn.Sequential(\n",
    "            nn.Linear(out_shape + 1, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_shape)\n",
    "        )\n",
    "\n",
    "        # Inverse Model\n",
    "        self.inv = nn.Sequential(\n",
    "            nn.Linear(out_shape * 2, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, action_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        obs, action = input\n",
    "        action = action.view(-1, 1)\n",
    "        phi = self.phi_state(obs)\n",
    "        x = torch.cat((phi, action.float()), -1)\n",
    "        phi_hat = self.fwd(x)\n",
    "        return phi_hat\n",
    "\n",
    "    def phi_state(self, s):\n",
    "        s = s[:, -1]\n",
    "        x = s.float().transpose(1, 3)\n",
    "        x = self.phi(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def inverse_pred(self, s, s1):\n",
    "        s = self.phi_state(s.float())\n",
    "        s1 = self.phi_state(s1.float())\n",
    "        x = torch.cat((s, s1), -1)\n",
    "        return self.inv(x)\n",
    "\n",
    "    def curiosity_rew(self, s, s1, a):\n",
    "        phi_hat = self.forward(s, a)\n",
    "        phi_s1 = self.phi_state(s1)\n",
    "        cur_rew = 1 / 2 * (torch.norm(phi_hat - phi_s1, p=2, dim=-1) ** 2)\n",
    "        return cur_rew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DDQN_Model(nn.Module):\n",
    "    def __init__(self, state_size, action_size, conv, macro=None, hidd_ch=256, conv_ch=32):\n",
    "        super(DDQN_Model, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        self.hidd_ch = hidd_ch\n",
    "        self.state_size = state_size\n",
    "        if macro is None:\n",
    "            self.backbone = nn.Sequential(\n",
    "                nn.Linear(state_size, hidd_ch),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(state_size, hidd_ch),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(hidd_ch, hidd_ch),\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(hidd_ch, self.action_size)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidd_ch, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, macro=None):\n",
    "        if obs.ndimension() == 4:\n",
    "            obs = obs[None]\n",
    "        obs = obs.float().transpose(2, 4)\n",
    "        stack = obs.shape[1]\n",
    "        backbone = self.backbone if macro is None else macro.backbone\n",
    "        x = torch.cat([self.features(backbone(obs[:, i]))[:, None] for i in range(stack)], dim=1)\n",
    "        x = x.view(x.size(0), stack, -1)\n",
    "\n",
    "        adv = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        return value + (adv - adv.mean(-1, keepdim=True))\n",
    "\n",
    "    def act(self, state, eps, backbone=None):\n",
    "        if np.random.random() > eps:\n",
    "            q = self.forward(state, backbone)\n",
    "            action = torch.argmax(q, dim=-1).cpu().data.numpy()\n",
    "        else:\n",
    "            action = np.random.randint(self.action_size, size=1 if len(state.shape) == 1 else state.shape[0])\n",
    "        return action.item() if action.shape == (1,) else list(action.astype(np.int))\n",
    "\n",
    "    def update_target(self, model):\n",
    "        self.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import mse_loss, cross_entropy, smooth_l1_loss, softmax\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "\n",
    "class EHDQN:\n",
    "    def __init__(self, state_dim, tau, action_dim, gamma, n_subpolicy, max_time, hidd_ch, lam, lr, eps,\n",
    "                 eps_decay, eps_sub, eps_sub_decay, beta, bs, target_interval, train_steps, max_memory, max_memory_sub,\n",
    "                 conv, gamma_macro, reward_rescale, n_proc, per=False, norm_input=True, logger=None):\n",
    "        \"\"\"\n",
    "        :param state_dim: Shape of the state\n",
    "        :param float tau: Weight for agent loss\n",
    "        :param gamma_macro: Discount for macro controller\n",
    "        :param int action_dim: Number of actions\n",
    "        :param float gamma: Discount for sub controller\n",
    "        :param int n_subpolicy: Number of sub policies\n",
    "        :param int max_time: Number of steps for each sub policy\n",
    "        :param int hidd_ch: Number of hidden channels\n",
    "        :param float lam: Scaler for ICM reward\n",
    "        :param float lr: Learning rate\n",
    "        :param float eps: Eps greedy chance for macro policy\n",
    "        :param float eps_decay: Epsilon decay computed as eps * (1 - eps_decay) each step\n",
    "        :param float eps_sub: Eps greedy change for sub policies\n",
    "        :param float eps_sub_decay: Epsilon decay for sub policy computed as eps * (1 - eps_decay) each step\n",
    "        :param float beta: Weight for loss of fwd net vs inv net\n",
    "        :param int bs: Batch size\n",
    "        :param int target_interval: Number of train steps between target updates\n",
    "        :param int train_steps: Number of training iterations for each call\n",
    "        :param int max_memory: Max memory\n",
    "        :param bool conv: Use or not convolutional networks\n",
    "        :param bool per: Use or not prioritized experience replay\n",
    "        :param int max_time: Maximum steps for sub policy\n",
    "        \"\"\"\n",
    "\n",
    "        # Parameters\n",
    "        self.logger = logger\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.target_interval = target_interval\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        # Macro Policy parameters\n",
    "        self.eps = eps\n",
    "        self.eps_decay = 1 - eps_decay\n",
    "        self.gamma_macro = gamma_macro\n",
    "        # Sub policy parameters\n",
    "        self.n_subpolicy = n_subpolicy\n",
    "        self.tau = tau\n",
    "        self.eps_sub = eps_sub\n",
    "        self.eps_sub_decay = 1 - eps_sub_decay\n",
    "        self.gamma = gamma\n",
    "        # ICM parameters\n",
    "        self.beta = beta\n",
    "        self.lam = lam\n",
    "\n",
    "        self.n_proc = n_proc\n",
    "        self.selected_policy = np.full((self.n_proc,), fill_value=None)\n",
    "        self.macro_state = np.full((self.n_proc,), fill_value=None)\n",
    "        self.max_time = max_time\n",
    "        self.train_steps = train_steps\n",
    "        self.reward_rescale = reward_rescale\n",
    "        self.norm_input = norm_input\n",
    "        self.per = per\n",
    "        self.curr_time = np.zeros((self.n_proc, ), dtype=np.int)\n",
    "        self.macro_reward = np.zeros((self.n_proc,), dtype=np.float)\n",
    "        self.target_count = np.zeros((self.n_subpolicy,), dtype=np.int)\n",
    "        self.counter_macro = np.zeros((self.n_subpolicy,), dtype=np.int)\n",
    "        self.counter_policies = np.zeros((self.n_subpolicy, self.action_dim), dtype=np.int)\n",
    "        self.macro_count = 0\n",
    "\n",
    "        memory = Memory\n",
    "\n",
    "        # Create Policies / ICM modules / Memories\n",
    "        self.macro = DDQN_Model(state_dim, n_subpolicy, conv=conv, hidd_ch=hidd_ch)\n",
    "        self.macro_target = DDQN_Model(state_dim, n_subpolicy, conv=conv, hidd_ch=hidd_ch)\n",
    "        self.macro_target.update_target(self.macro)\n",
    "        self.macro_memory = Memory(max_memory)\n",
    "        self.macro_opt = torch.optim.Adam(self.macro.parameters(), lr=self.lr * 4 if self.per else self.lr)\n",
    "        self.memory, self.policy, self.target, self.icm, self.policy_opt, self.icm_opt = [], [], [], [], [], []\n",
    "        for i in range(n_subpolicy):\n",
    "            # Create sub-policies\n",
    "            self.policy.append(DDQN_Model(state_dim, action_dim, conv=conv, hidd_ch=hidd_ch, macro=self.macro).to(sett.device))\n",
    "            self.target.append(DDQN_Model(state_dim, action_dim, conv=conv, hidd_ch=hidd_ch, macro=self.macro).to(sett.device))\n",
    "            self.target[-1].update_target(self.policy[-1])\n",
    "            self.memory.append(memory(max_memory_sub))\n",
    "\n",
    "            # Create ICM modules\n",
    "            self.icm.append(ICM_Model(self.state_dim, self.action_dim, conv).to(sett.device))\n",
    "\n",
    "            # Create sub optimizers\n",
    "            self.policy_opt.append(torch.optim.Adam(self.policy[i].parameters(), lr=self.lr))\n",
    "            self.icm_opt.append(torch.optim.Adam(self.icm[i].parameters(), lr=1e-3))\n",
    "\n",
    "        # Send macro to correct device\n",
    "        self.macro = self.macro.to(sett.device)\n",
    "        self.macro_target = self.macro_target.to(sett.device)\n",
    "\n",
    "    def save(self, i):\n",
    "        if not os.path.isdir(sett.SAVEPATH):\n",
    "            os.makedirs(sett.SAVEPATH)\n",
    "        torch.save(self.macro.state_dict(), os.path.join(sett.SAVEPATH, 'Macro_%s.pth' % i))\n",
    "        for sub in range(self.n_subpolicy):\n",
    "            torch.save(self.policy[sub].state_dict(), os.path.join(sett.SAVEPATH, 'Sub_%s_%s.pth' % (sub, i)))\n",
    "            torch.save(self.icm[sub].state_dict(), os.path.join(sett.SAVEPATH, 'Icm_%s_%s.pth' % (sub, i)))\n",
    "\n",
    "    def load(self, path, i):\n",
    "        self.macro.load_state_dict(torch.load(os.path.join(path, 'Macro_%s.pth' % i), map_location=sett.device))\n",
    "        for sub in range(self.n_subpolicy):\n",
    "            self.policy[sub].load_state_dict(torch.load(os.path.join(path, 'Sub_%s_%s.pth' % (sub, i)), map_location=sett.device))\n",
    "            self.icm[sub].load_state_dict(torch.load(os.path.join(path, 'Icm_%s_%s.pth' % (sub, i)), map_location=sett.device))\n",
    "\n",
    "    def act(self, obs, deterministic=False):\n",
    "        x = torch.from_numpy(obs).float().to(sett.device)\n",
    "        if self.norm_input:\n",
    "            x /= 255\n",
    "\n",
    "        for i, sel_policy, curr_time in zip(range(self.n_proc), self.selected_policy, self.curr_time):\n",
    "            if sel_policy is None or curr_time == self.max_time:\n",
    "                if sel_policy is not None and not deterministic:\n",
    "                    # Store non terminal macro transition\n",
    "                    self.macro_memory.store_transition(self.macro_state[i], obs[i], sel_policy, self.macro_reward[i], False)\n",
    "                    self.macro_reward[i] = 0\n",
    "\n",
    "                # Pick macro action\n",
    "                self.selected_policy[i] = self.pick_policy(x[i][None], deterministic=deterministic)\n",
    "                assert isinstance(self.selected_policy[i], int)\n",
    "                self.curr_time[i] = 0\n",
    "                if not deterministic:\n",
    "                    self.macro_state[i] = obs[i]\n",
    "\n",
    "                self.counter_macro[sel_policy] += 1\n",
    "\n",
    "        eps = max(0.01, self.eps_sub) if not deterministic else 0.01\n",
    "        sel_pol = np.unique(self.selected_policy)\n",
    "        sel_indices = [(self.selected_policy == i).nonzero()[0] for i in sel_pol]\n",
    "        action = -np.ones((self.n_proc,), dtype=np.int)\n",
    "        for policy_idx, indices in zip(sel_pol, sel_indices):\n",
    "            action[indices] = self.policy[policy_idx].act(x[indices], eps=eps, backbone=self.macro)\n",
    "            self.counter_policies[policy_idx, action[indices]] += 1\n",
    "\n",
    "        self.curr_time += 1  # Is a vector\n",
    "        return action\n",
    "\n",
    "    def pick_policy(self, obs, deterministic=False):\n",
    "        eps = max(0.01, self.eps) if not deterministic else 0.01\n",
    "        policy = self.macro.act(obs, eps=eps)\n",
    "        return policy\n",
    "\n",
    "    def set_mode(self, training=False):\n",
    "        for policy in self.policy:\n",
    "            policy.train(training)\n",
    "        self.macro.train(training)\n",
    "        self.selected_policy[:] = None\n",
    "        self.curr_time[:] = 0\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        # Rescale reward if a scaling is provided\n",
    "        if self.reward_rescale != 0:\n",
    "            if self.reward_rescale == 1:\n",
    "                reward = np.sign(reward)\n",
    "            elif self.reward_rescale == 2:\n",
    "                reward = np.clip(reward, -1, 1)\n",
    "            else:\n",
    "                reward *= self.reward_rescale\n",
    "        return reward\n",
    "\n",
    "    def store_transition(self, s, s1, a, reward, is_terminal):\n",
    "        reward = self.process_reward(reward)\n",
    "\n",
    "        for i, sel_policy in enumerate(self.selected_policy):\n",
    "            # Store sub policy experience\n",
    "            self.memory[sel_policy].store_transition(s[i], s1[i], a[i], reward[i], is_terminal[i])\n",
    "            self.macro_reward[i] += reward[i]\n",
    "\n",
    "            # Store terminal macro transition\n",
    "            if is_terminal[i]:\n",
    "                self.macro_memory.store_transition(self.macro_state[i], s1[i], sel_policy, self.macro_reward[i], is_terminal[i])\n",
    "                self.macro_reward[i] = 0\n",
    "                self.selected_policy[i] = None\n",
    "\n",
    "    def update(self):\n",
    "        for i in range(self.train_steps):\n",
    "            self._update()\n",
    "            if self.logger is not None:\n",
    "                self.logger.step += 1\n",
    "\n",
    "    def _update(self):\n",
    "        # First train each sub policy\n",
    "        self.macro_opt.zero_grad()  # To allow cumulative gradients on backbone part\n",
    "\n",
    "        for i in range(self.n_subpolicy):\n",
    "            memory = self.memory[i]\n",
    "            if len(memory) < self.bs * 100:\n",
    "                continue\n",
    "\n",
    "            policy = self.policy[i]\n",
    "            target = self.target[i]\n",
    "            icm = self.icm[i]\n",
    "            policy_opt = self.policy_opt[i]\n",
    "            icm_opt = self.icm_opt[i]\n",
    "\n",
    "            if self.per:\n",
    "                state, new_state, action, reward, is_terminal, idxs, w_is = memory.sample(self.bs)\n",
    "                reduction = 'none'\n",
    "                self.logger.log_scalar(tag='Beta PER %i' % i, value=memory.beta)\n",
    "            else:\n",
    "                state, new_state, action, reward, is_terminal = memory.sample(self.bs)\n",
    "                reduction = 'mean'\n",
    "\n",
    "            if self.norm_input:\n",
    "                state = np.array(state, dtype=np.float) / 255\n",
    "                new_state = np.array(new_state, dtype=np.float) / 255\n",
    "\n",
    "            state = torch.tensor(state, dtype=torch.float).detach().to(sett.device)\n",
    "            new_state = torch.tensor(new_state, dtype=torch.float).detach().to(sett.device)\n",
    "            action = torch.tensor(action).detach().to(sett.device)\n",
    "            reward = torch.tensor(reward, dtype=torch.float).detach().to(sett.device)\n",
    "            is_terminal = 1. - torch.tensor(is_terminal, dtype=torch.float).detach().to(sett.device)\n",
    "\n",
    "            # Augment rewards with curiosity\n",
    "            curiosity_rewards = icm.curiosity_rew(state, new_state, action)\n",
    "            reward = (1 - 0.01) * reward + 0.01 * self.lam * curiosity_rewards\n",
    "\n",
    "            # Policy loss\n",
    "            q = policy.forward(state, macro=self.macro)[torch.arange(self.bs), action]\n",
    "            max_action = torch.argmax(policy.forward(new_state, macro=self.macro), dim=1)\n",
    "            y = reward + self.gamma * target.forward(new_state, macro=self.macro)[torch.arange(self.bs), max_action] * is_terminal\n",
    "            policy_loss = smooth_l1_loss(input=q, target=y.detach(), reduction=reduction).mean(-1)\n",
    "\n",
    "            # ICM Loss\n",
    "            phi_hat = icm.forward(state, action)\n",
    "            phi_true = icm.phi_state(new_state)\n",
    "            fwd_loss = mse_loss(input=phi_hat, target=phi_true.detach(), reduction=reduction).mean(-1)\n",
    "            a_hat = icm.inverse_pred(state, new_state)\n",
    "            inv_loss = cross_entropy(input=a_hat, target=action.detach(), reduction=reduction)\n",
    "\n",
    "            # Total loss\n",
    "            inv_loss = (1 - self.beta) * inv_loss\n",
    "            fwd_loss = self.beta * fwd_loss * 288\n",
    "            loss = self.tau * policy_loss + inv_loss + fwd_loss\n",
    "\n",
    "            if self.per:\n",
    "                error = np.clip((torch.abs(q - y)).cpu().data.numpy(), 0, 0.8)\n",
    "                inv_prob = (1 - softmax(a_hat, dim=1)[torch.arange(self.bs), action]) / 5\n",
    "                curiosity_error = torch.abs(inv_prob).cpu().data.numpy()\n",
    "                total_error = error + curiosity_error\n",
    "\n",
    "                # update priorities\n",
    "                for k in range(self.bs):\n",
    "                    memory.update(idxs[k], total_error[k])\n",
    "\n",
    "                loss = (loss * torch.FloatTensor(w_is).to(sett.device)).mean()\n",
    "\n",
    "            policy_opt.zero_grad()\n",
    "            icm_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in policy.parameters():\n",
    "                param.grad.data.clamp(-1, 1)\n",
    "            policy_opt.step()\n",
    "            icm_opt.step()\n",
    "\n",
    "            self.target_count[i] += 1\n",
    "            if self.target_count[i] == self.target_interval:\n",
    "                self.target_count[i] = 0\n",
    "                self.target[i].update_target(self.policy[i])\n",
    "\n",
    "            if self.logger is not None:\n",
    "                self.logger.log_scalar(tag='Policy Loss %i' % i, value=policy_loss.mean().cpu().data.numpy())\n",
    "                self.logger.log_scalar(tag='ICM Fwd Loss %i' % i, value=fwd_loss.mean().cpu().data.numpy())\n",
    "                self.logger.log_scalar(tag='ICM Inv Loss %i' % i, value=inv_loss.mean().cpu().data.numpy())\n",
    "                self.logger.log_scalar(tag='Total Policy Loss %i' % i, value=loss.mean().cpu().data.numpy())\n",
    "                self.logger.log_scalar(tag='Mean Curiosity Reward %i' % i, value=curiosity_rewards.mean().cpu().data.numpy())\n",
    "                self.logger.log_scalar(tag='Q values %i' % i, value=q.mean().cpu().data.numpy())\n",
    "                self.logger.log_scalar(tag='Target Boltz %i' % i, value=y.mean().cpu().data.numpy())\n",
    "                actions = self.counter_policies[i] / max(1, self.counter_policies[i].sum())\n",
    "                self.logger.log_text(tag='Policy actions %i Text' %i, value=[str(v) for v in actions],\n",
    "                                     step=self.logger.step)\n",
    "                if self.per:\n",
    "                    self.logger.log_scalar(tag='PER Error %i' % i, value=total_error.mean())\n",
    "                    self.logger.log_scalar(tag='PER Error Policy %i' % i, value=error.mean())\n",
    "                    self.logger.log_scalar(tag='PER Error Curiosity %i' % i, value=curiosity_error.mean())\n",
    "\n",
    "        # Reduce sub eps\n",
    "        self.eps_sub = self.eps_sub * self.eps_sub_decay\n",
    "\n",
    "        # Train Macro policy\n",
    "        if len(self.macro_memory) < self.bs * 100:\n",
    "            return\n",
    "\n",
    "        # Reduce eps\n",
    "        self.eps = self.eps * self.eps_decay\n",
    "\n",
    "        state, new_state, action, reward, is_terminal = self.macro_memory.sample(self.bs)\n",
    "        if self.norm_input:\n",
    "            state = np.array(state, dtype=np.float) / 255\n",
    "            new_state = np.array(new_state, dtype=np.float) / 255\n",
    "\n",
    "        state = torch.tensor(state, dtype=torch.float).detach().to(sett.device)\n",
    "        new_state = torch.tensor(new_state, dtype=torch.float).detach().to(sett.device)\n",
    "        action = torch.tensor(action).detach().to(sett.device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float).detach().to(sett.device)\n",
    "        is_terminal = 1. - torch.tensor(is_terminal, dtype=torch.float).detach().to(sett.device)\n",
    "\n",
    "        q = self.macro.forward(state)[torch.arange(self.bs), action]\n",
    "        max_action = torch.argmax(self.macro.forward(new_state), dim=1)\n",
    "        y = reward + self.gamma_macro * self.macro_target.forward(new_state)[torch.arange(self.bs), max_action] * is_terminal\n",
    "        loss = smooth_l1_loss(input=q, target=y.detach())\n",
    "\n",
    "        loss.backward()\n",
    "        for param in self.macro.parameters():\n",
    "            param.grad.data.clamp(-1, 1)\n",
    "        self.macro_opt.step()\n",
    "\n",
    "        self.macro_count += 1\n",
    "        if self.macro_count == self.target_interval:\n",
    "            self.macro_count = 0\n",
    "            self.macro_target.update_target(self.macro)\n",
    "\n",
    "        if self.logger is not None:\n",
    "            self.logger.log_scalar(tag='Macro Loss', value=loss.cpu().detach().numpy())\n",
    "            self.logger.log_scalar(tag='Sub Eps', value=self.eps_sub)\n",
    "            self.logger.log_scalar(tag='Macro Eps', value=self.eps)\n",
    "            values = self.counter_macro / max(1, sum(self.counter_macro))\n",
    "            self.logger.log_text(tag='Macro Policy Actions Text', value=[str(v) for v in values],\n",
    "                                 step=self.logger.step)\n",
    "            self.logger.log_histogram(tag='Macro Policy Actions Hist', values=values,\n",
    "                                      step=self.logger.step, bins=self.n_subpolicy)\n",
    "            self.logger.log_scalar(tag='Macro Q values', value=q.cpu().detach().numpy().mean())\n",
    "            self.logger.log_scalar(tag='Marcro Target Boltz', value=y.cpu().detach().numpy().mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academy",
   "language": "python",
   "name": "academy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
