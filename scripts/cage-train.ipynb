{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/zhx/word/DriverOrderOfflineRL/tianshou')\n",
    "sys.path.append('/home/zhx/word/DriverOrderOfflineRL/cage-challenge-1/CybORG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from pprint import pprint\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import *\n",
    "from CybORG.Shared.Actions import *\n",
    "from CybORG.Agents.Wrappers import *\n",
    "import os\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "path = str(inspect.getfile(CybORG))\n",
    "path = path[:-10] + '/Shared/Scenarios/Scenario1b.yaml'\n",
    "\n",
    "cyborg = CybORG(path,'sim', agents={'Red':B_lineAgent})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChallengeWrapper(env=cyborg, agent_name=\"Blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhx/miniconda3/envs/tianshou/lib/python3.11/site-packages/wandb/sdk/launch/builder/build.py:11: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "from tianshou.env import DummyVectorEnv\n",
    "train_envs = DummyVectorEnv([lambda: ChallengeWrapper(env=CybORG(path,'sim', agents={'Red': B_lineAgent}), agent_name=\"Blue\") for _ in range(1)])\n",
    "test_envs = DummyVectorEnv([lambda: ChallengeWrapper(env=CybORG(path,'sim', agents={'Red': B_lineAgent}), agent_name=\"Blue\") for _ in range(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.policy import DQNPolicy\n",
    "from tianshou.utils.net.common import Net\n",
    "from torch import nn\n",
    "import argparse\n",
    "import wandb\n",
    "from typing import Any\n",
    "from collections.abc import Callable, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Reference: Human-level control through deep reinforcement learning.\n",
    "\n",
    "    For advanced usage (how to customize the network), please refer to\n",
    "    :ref:`build_the_network`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_shape: Sequence[int],\n",
    "        action_shape: Sequence[int],\n",
    "        device: str | int | torch.device = \"cpu\",\n",
    "        features_only: bool = False,\n",
    "        output_dim: int | None = None,\n",
    "        layer_init: Callable[[nn.Module], nn.Module] = lambda x: x,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.net = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_shape, 256)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            layer_init(nn.Linear(256, 256)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            layer_init(nn.Linear(256, action_shape)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.output_dim = action_shape\n",
    "        if not features_only:\n",
    "            self.net = nn.Sequential(\n",
    "                self.net,\n",
    "                layer_init(nn.Linear(self.output_dim, 256)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                layer_init(nn.Linear(256, int(np.prod(action_shape)))),\n",
    "            )\n",
    "            self.output_dim = np.prod(action_shape)\n",
    "        elif output_dim is not None:\n",
    "            self.net = nn.Sequential(\n",
    "                self.net,\n",
    "                layer_init(nn.Linear(self.output_dim, output_dim)),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "            self.output_dim = output_dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        obs: np.ndarray | torch.Tensor,\n",
    "        state: Any | None = None,\n",
    "        info: dict[str, Any] | None = None,\n",
    "    ) -> tuple[torch.Tensor, Any]:\n",
    "        r\"\"\"Mapping: s -> Q(s, \\*).\"\"\"\n",
    "        if info is None:\n",
    "            info = {}\n",
    "        obs = torch.as_tensor(obs, device=self.device, dtype=torch.float32)\n",
    "        return self.net(obs), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--task\", type=str, default=\"CybORG\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    parser.add_argument(\"--scale-obs\", type=int, default=0)\n",
    "    parser.add_argument(\"--eps-test\", type=float, default=0.005)\n",
    "    parser.add_argument(\"--eps-train\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--eps-train-final\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--buffer-size\", type=int, default=5000)\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0001)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--n-step\", type=int, default=3)\n",
    "    parser.add_argument(\"--target-update-freq\", type=int, default=500)\n",
    "    parser.add_argument(\"--epoch\", type=int, default=100)\n",
    "    parser.add_argument(\"--step-per-epoch\", type=int, default=10000)\n",
    "    parser.add_argument(\"--step-per-collect\", type=int, default=10)\n",
    "    parser.add_argument(\"--update-per-step\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=32)\n",
    "    parser.add_argument(\"--training-num\", type=int, default=10)\n",
    "    parser.add_argument(\"--test-num\", type=int, default=10)\n",
    "    parser.add_argument(\"--logdir\", type=str, default=\"log\")\n",
    "    parser.add_argument(\"--render\", type=float, default=0.0)\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        type=str,\n",
    "        default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    parser.add_argument(\"--frames-stack\", type=int, default=1)\n",
    "    parser.add_argument(\"--resume-path\", type=str, default=None)\n",
    "    parser.add_argument(\"--resume-id\", type=str, default=None)\n",
    "    parser.add_argument(\n",
    "        \"--logger\",\n",
    "        type=str,\n",
    "        default=\"wandb\",\n",
    "        choices=[\"tensorboard\", \"wandb\"],\n",
    "    )\n",
    "    parser.add_argument(\"--wandb-project\", type=str, default=\"cyborg.dqn\")\n",
    "    parser.add_argument(\n",
    "        \"--watch\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"watch the play of pre-trained policy only\",\n",
    "    )\n",
    "    parser.add_argument(\"--save-buffer-name\", type=str, default=None)\n",
    "    parser.add_argument(\n",
    "        \"--icm-lr-scale\",\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\"use intrinsic curiosity module with this lr scale\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--icm-reward-scale\",\n",
    "        type=float,\n",
    "        default=0.01,\n",
    "        help=\"scaling factor for intrinsic curiosity reward\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--icm-forward-loss-weight\",\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help=\"weight for the forward model loss in ICM\",\n",
    "    )\n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.policy import DQNPolicy\n",
    "from tianshou.policy.modelbased.icm import ICMPolicy\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from tianshou.utils import TensorboardLogger, WandbLogger\n",
    "from tianshou.utils.net.discrete import IntrinsicCuriosityModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dqn(args=get_args()):\n",
    "    args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "    # should be N_FRAMES x H x W\n",
    "    print(\"Observations shape:\", args.state_shape)\n",
    "    print(\"Actions shape:\", args.action_shape)\n",
    "    # seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    # define model\n",
    "    net = DQN(*args.state_shape, args.action_shape, args.device).to(args.device)\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "    # define policy\n",
    "    policy = DQNPolicy(\n",
    "        model=net,\n",
    "        optim=optim,\n",
    "        action_space=env.action_space,\n",
    "        discount_factor=args.gamma,\n",
    "        estimation_step=args.n_step,\n",
    "        target_update_freq=args.target_update_freq,\n",
    "    ).to(args.device)\n",
    "    if args.icm_lr_scale > 0:\n",
    "        feature_net = DQN(*args.state_shape, args.action_shape, args.device, features_only=True)\n",
    "        action_dim = np.prod(args.action_shape)\n",
    "        feature_dim = feature_net.output_dim\n",
    "        icm_net = IntrinsicCuriosityModule(\n",
    "            feature_net.net,\n",
    "            feature_dim,\n",
    "            action_dim,\n",
    "            hidden_sizes=[256],\n",
    "            device=args.device,\n",
    "        )\n",
    "        icm_optim = torch.optim.Adam(icm_net.parameters(), lr=args.lr)\n",
    "        policy = ICMPolicy(\n",
    "            policy=policy,\n",
    "            model=icm_net,\n",
    "            optim=icm_optim,\n",
    "            action_space=env.action_space,\n",
    "            lr_scale=args.icm_lr_scale,\n",
    "            reward_scale=args.icm_reward_scale,\n",
    "            forward_loss_weight=args.icm_forward_loss_weight,\n",
    "        ).to(args.device)\n",
    "    # load a previous policy\n",
    "    if args.resume_path:\n",
    "        policy.load_state_dict(torch.load(args.resume_path, map_location=args.device))\n",
    "        print(\"Loaded agent from: \", args.resume_path)\n",
    "    # replay buffer: `save_last_obs` and `stack_num` can be removed together\n",
    "    # when you have enough RAM\n",
    "    buffer = VectorReplayBuffer(\n",
    "        args.buffer_size,\n",
    "        buffer_num=len(train_envs),\n",
    "        ignore_obs_next=True,\n",
    "        save_only_last_obs=True,\n",
    "        stack_num=args.frames_stack,\n",
    "    )\n",
    "    # collector\n",
    "    train_collector = Collector(policy, train_envs, buffer, exploration_noise=True)\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "\n",
    "    # log\n",
    "    now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "    args.algo_name = \"dqn_icm\" if args.icm_lr_scale > 0 else \"dqn\"\n",
    "    log_name = os.path.join(args.task, args.algo_name, str(args.seed), now)\n",
    "    log_path = os.path.join(args.logdir, log_name)\n",
    "\n",
    "    # logger\n",
    "    if args.logger == \"wandb\":\n",
    "        logger = WandbLogger(\n",
    "            save_interval=1,\n",
    "            name=log_name.replace(os.path.sep, \"__\"),\n",
    "            run_id=args.resume_id,\n",
    "            config=args,\n",
    "            project=args.wandb_project,\n",
    "        )\n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    if args.logger == \"tensorboard\":\n",
    "        logger = TensorboardLogger(writer)\n",
    "    else:  # wandb\n",
    "        logger.load(writer)\n",
    "\n",
    "    def save_best_fn(policy):\n",
    "        torch.save(policy.state_dict(), os.path.join(log_path, \"policy.pth\"))\n",
    "\n",
    "    def stop_fn(mean_rewards: float) -> bool:\n",
    "        if env.spec.reward_threshold:\n",
    "            return mean_rewards >= env.spec.reward_threshold\n",
    "        if \"Pong\" in args.task:\n",
    "            return mean_rewards >= 20\n",
    "        return False\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        # nature DQN setting, linear decay in the first 1M steps\n",
    "        if env_step <= 1e6:\n",
    "            eps = args.eps_train - env_step / 1e6 * (args.eps_train - args.eps_train_final)\n",
    "        else:\n",
    "            eps = args.eps_train_final\n",
    "        policy.set_eps(eps)\n",
    "        if env_step % 1000 == 0:\n",
    "            logger.write(\"train/env_step\", env_step, {\"train/eps\": eps})\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        policy.set_eps(args.eps_test)\n",
    "\n",
    "    def save_checkpoint_fn(epoch, env_step, gradient_step):\n",
    "        # see also: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        ckpt_path = os.path.join(log_path, f\"checkpoint_{epoch}.pth\")\n",
    "        torch.save({\"model\": policy.state_dict()}, ckpt_path)\n",
    "        return ckpt_path\n",
    "\n",
    "    # watch agent's performance\n",
    "    def watch():\n",
    "        print(\"Setup test envs ...\")\n",
    "        policy.eval()\n",
    "        policy.set_eps(args.eps_test)\n",
    "        test_envs.seed(args.seed)\n",
    "        if args.save_buffer_name:\n",
    "            print(f\"Generate buffer with size {args.buffer_size}\")\n",
    "            buffer = VectorReplayBuffer(\n",
    "                args.buffer_size,\n",
    "                buffer_num=len(test_envs),\n",
    "                ignore_obs_next=True,\n",
    "                save_only_last_obs=True,\n",
    "                stack_num=args.frames_stack,\n",
    "            )\n",
    "            collector = Collector(policy, test_envs, buffer, exploration_noise=True)\n",
    "            result = collector.collect(n_step=args.buffer_size)\n",
    "            print(f\"Save buffer into {args.save_buffer_name}\")\n",
    "            # Unfortunately, pickle will cause oom with 1M buffer size\n",
    "            buffer.save_hdf5(args.save_buffer_name)\n",
    "        else:\n",
    "            print(\"Testing agent ...\")\n",
    "            test_collector.reset()\n",
    "            result = test_collector.collect(n_episode=args.test_num, render=args.render)\n",
    "        rew = result[\"rews\"].mean()\n",
    "        print(f\"Mean reward (over {result['n/ep']} episodes): {rew}\")\n",
    "\n",
    "    if args.watch:\n",
    "        watch()\n",
    "        sys.exit(0)\n",
    "\n",
    "    # test train_collector and start filling replay buffer\n",
    "    train_collector.collect(n_step=args.batch_size * args.training_num)\n",
    "    # trainer\n",
    "    result = OffpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,\n",
    "        max_epoch=args.epoch,\n",
    "        step_per_epoch=args.step_per_epoch,\n",
    "        step_per_collect=args.step_per_collect,\n",
    "        episode_per_test=args.test_num,\n",
    "        batch_size=args.batch_size,\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        logger=logger,\n",
    "        update_per_step=args.update_per_step,\n",
    "        test_in_train=False,\n",
    "        resume_from_log=args.resume_id is not None,\n",
    "        save_checkpoint_fn=save_checkpoint_fn,\n",
    "    ).run()\n",
    "\n",
    "    pprint.pprint(result)\n",
    "    watch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations shape: (52,)\n",
      "Actions shape: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhongxi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/zhx/miniconda3/envs/tianshou/lib/python3.11/site-packages/wandb/sdk/lib/ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/zhx/word/DriverOrderOfflineRL/scripts/wandb/run-20231112_192907-48wchw6r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hongxi/cyborg.dqn/runs/48wchw6r' target=\"_blank\">CybORG__dqn__0__231112-192905</a></strong> to <a href='https://wandb.ai/hongxi/cyborg.dqn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hongxi/cyborg.dqn' target=\"_blank\">https://wandb.ai/hongxi/cyborg.dqn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hongxi/cyborg.dqn/runs/48wchw6r' target=\"_blank\">https://wandb.ai/hongxi/cyborg.dqn/runs/48wchw6r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb 单元格 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m test_dqn(get_args())\n",
      "\u001b[1;32m/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb 单元格 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m train_collector\u001b[39m.\u001b[39mcollect(n_step\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mbatch_size \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mtraining_num)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39m# trainer\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m result \u001b[39m=\u001b[39m OffpolicyTrainer(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m     policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m     train_collector\u001b[39m=\u001b[39;49mtrain_collector,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m     test_collector\u001b[39m=\u001b[39;49mtest_collector,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m     max_epoch\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mepoch,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m     step_per_epoch\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mstep_per_epoch,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m     step_per_collect\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mstep_per_collect,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m     episode_per_test\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mtest_num,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=152'>153</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=153'>154</a>\u001b[0m     train_fn\u001b[39m=\u001b[39;49mtrain_fn,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=154'>155</a>\u001b[0m     test_fn\u001b[39m=\u001b[39;49mtest_fn,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=155'>156</a>\u001b[0m     stop_fn\u001b[39m=\u001b[39;49mstop_fn,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=156'>157</a>\u001b[0m     save_best_fn\u001b[39m=\u001b[39;49msave_best_fn,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=157'>158</a>\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m     update_per_step\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mupdate_per_step,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=159'>160</a>\u001b[0m     test_in_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=160'>161</a>\u001b[0m     resume_from_log\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mresume_id \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=161'>162</a>\u001b[0m     save_checkpoint_fn\u001b[39m=\u001b[39;49msave_checkpoint_fn,\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m )\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m pprint\u001b[39m.\u001b[39mpprint(result)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22506368227d/home/zhx/word/DriverOrderOfflineRL/scripts/cage-train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m watch()\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/tianshou/tianshou/trainer/base.py:418\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m     deque(\u001b[39mself\u001b[39;49m, maxlen\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     info \u001b[39m=\u001b[39m gather_info(\n\u001b[1;32m    420\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector,\n\u001b[1;32m    421\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std\n\u001b[1;32m    422\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/tianshou/tianshou/trainer/base.py:240\u001b[0m, in \u001b[0;36mBaseTrainer.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/tianshou/tianshou/trainer/base.py:225\u001b[0m, in \u001b[0;36mBaseTrainer.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode_per_test \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector\u001b[39m.\u001b[39mreset_stat()\n\u001b[0;32m--> 225\u001b[0m test_result \u001b[39m=\u001b[39m test_episode(\n\u001b[1;32m    226\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_collector, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_fn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_epoch,\n\u001b[1;32m    227\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepisode_per_test, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_metric\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_epoch\n\u001b[1;32m    230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std \u001b[39m=\u001b[39m \\\n\u001b[1;32m    231\u001b[0m     test_result[\u001b[39m\"\u001b[39m\u001b[39mrew\u001b[39m\u001b[39m\"\u001b[39m], test_result[\u001b[39m\"\u001b[39m\u001b[39mrew_std\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/tianshou/tianshou/trainer/utils.py:27\u001b[0m, in \u001b[0;36mtest_episode\u001b[0;34m(policy, collector, test_fn, epoch, n_episode, logger, global_step, reward_metric)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m test_fn:\n\u001b[1;32m     26\u001b[0m     test_fn(epoch, global_step)\n\u001b[0;32m---> 27\u001b[0m result \u001b[39m=\u001b[39m collector\u001b[39m.\u001b[39;49mcollect(n_episode\u001b[39m=\u001b[39;49mn_episode)\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m reward_metric:\n\u001b[1;32m     29\u001b[0m     rew \u001b[39m=\u001b[39m reward_metric(result[\u001b[39m\"\u001b[39m\u001b[39mrews\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/tianshou/tianshou/data/collector.py:253\u001b[0m, in \u001b[0;36mCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad)\u001b[0m\n\u001b[1;32m    251\u001b[0m action_remap \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mmap_action(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mact)\n\u001b[1;32m    252\u001b[0m \u001b[39m# step in env\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action_remap, ready_env_ids)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    254\u001b[0m obs_next, rew, done, info \u001b[39m=\u001b[39m result\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mupdate(obs_next\u001b[39m=\u001b[39mobs_next, rew\u001b[39m=\u001b[39mrew, done\u001b[39m=\u001b[39mdone, info\u001b[39m=\u001b[39minfo)\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/tianshou/tianshou/env/venvs.py:248\u001b[0m, in \u001b[0;36mBaseVectorEnv.step\u001b[0;34m(self, action, id)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(action) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mid\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[39mfor\u001b[39;00m i, j \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mid\u001b[39m):\n\u001b[0;32m--> 248\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworkers[j]\u001b[39m.\u001b[39;49msend(action[i])\n\u001b[1;32m    249\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[1;32m    250\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mid\u001b[39m:\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/tianshou/tianshou/env/worker/dummy.py:36\u001b[0m, in \u001b[0;36mDummyEnvWorker.send\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/cage-challenge-1/CybORG/CybORG/Agents/Wrappers/ChallengeWrapper.py:31\u001b[0m, in \u001b[0;36mChallengeWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m,action\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 31\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action\u001b[39m=\u001b[39;49maction)\n\u001b[1;32m     33\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_steps:\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/cage-challenge-1/CybORG/CybORG/Agents/Wrappers/OpenAIGymWrapper.py:27\u001b[0m, in \u001b[0;36mOpenAIGymWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: Union[\u001b[39mint\u001b[39m, List[\u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (\u001b[39mobject\u001b[39m, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m):\n\u001b[1;32m     26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction \u001b[39m=\u001b[39m action\n\u001b[0;32m---> 27\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent_name, action)\n\u001b[1;32m     28\u001b[0m     result\u001b[39m.\u001b[39mobservation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_change(result\u001b[39m.\u001b[39mobservation)\n\u001b[1;32m     29\u001b[0m     result\u001b[39m.\u001b[39maction_space \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space_change(result\u001b[39m.\u001b[39maction_space)\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/cage-challenge-1/CybORG/CybORG/Agents/Wrappers/EnumActionWrapper.py:20\u001b[0m, in \u001b[0;36mEnumActionWrapper.step\u001b[0;34m(self, agent, action)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m action \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpossible_actions[action]\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(agent, action)\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/cage-challenge-1/CybORG/CybORG/Agents/Wrappers/BaseWrapper.py:18\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, agent, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(agent, action)\n\u001b[1;32m     17\u001b[0m result\u001b[39m.\u001b[39mobservation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_change(result\u001b[39m.\u001b[39mobservation)\n\u001b[0;32m---> 18\u001b[0m result\u001b[39m.\u001b[39maction_space \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_space_change(result\u001b[39m.\u001b[39;49maction_space)\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/word/DriverOrderOfflineRL/cage-challenge-1/CybORG/CybORG/Agents/Wrappers/EnumActionWrapper.py:48\u001b[0m, in \u001b[0;36mEnumActionWrapper.action_space_change\u001b[0;34m(self, action_space)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m action_space[p]\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     47\u001b[0m                 p_dict[p] \u001b[39m=\u001b[39m key\n\u001b[0;32m---> 48\u001b[0m                 new_param_list\u001b[39m.\u001b[39mappend({key: value \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m p_dict\u001b[39m.\u001b[39mitems()})\n\u001b[1;32m     49\u001b[0m         param_list \u001b[39m=\u001b[39m new_param_list\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m p_dict \u001b[39min\u001b[39;00m param_list:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_dqn(get_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations shape: (52,)\n",
      "Actions shape: 54\n"
     ]
    }
   ],
   "source": [
    "args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "args.action_shape = env.action_space.shape or env.action_space.n\n",
    "# should be N_FRAMES x H x W\n",
    "print(\"Observations shape:\", args.state_shape)\n",
    "print(\"Actions shape:\", args.action_shape)\n",
    "# seed\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "# define model\n",
    "net = DQN(*args.state_shape, args.action_shape, args.device).to(args.device)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "# define policy\n",
    "policy = DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    action_space=env.action_space,\n",
    "    discount_factor=args.gamma,\n",
    "    estimation_step=args.n_step,\n",
    "    target_update_freq=args.target_update_freq,\n",
    ").to(args.device)\n",
    "if args.icm_lr_scale > 0:\n",
    "    feature_net = DQN(*args.state_shape, args.action_shape, args.device, features_only=True)\n",
    "    action_dim = np.prod(args.action_shape)\n",
    "    feature_dim = feature_net.output_dim\n",
    "    icm_net = IntrinsicCuriosityModule(\n",
    "        feature_net.net,\n",
    "        feature_dim,\n",
    "        action_dim,\n",
    "        hidden_sizes=[256],\n",
    "        device=args.device,\n",
    "    )\n",
    "    icm_optim = torch.optim.Adam(icm_net.parameters(), lr=args.lr)\n",
    "    policy = ICMPolicy(\n",
    "        policy=policy,\n",
    "        model=icm_net,\n",
    "        optim=icm_optim,\n",
    "        action_space=env.action_space,\n",
    "        lr_scale=args.icm_lr_scale,\n",
    "        reward_scale=args.icm_reward_scale,\n",
    "        forward_loss_weight=args.icm_forward_loss_weight,\n",
    "    ).to(args.device)\n",
    "# load a previous policy\n",
    "if args.resume_path:\n",
    "    policy.load_state_dict(torch.load(args.resume_path, map_location=args.device))\n",
    "    print(\"Loaded agent from: \", args.resume_path)\n",
    "# replay buffer: `save_last_obs` and `stack_num` can be removed together\n",
    "# when you have enough RAM\n",
    "buffer = VectorReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    buffer_num=len(train_envs),\n",
    "    ignore_obs_next=False,\n",
    "    save_only_last_obs=False,\n",
    "    stack_num=args.frames_stack,\n",
    ")\n",
    "# collector\n",
    "train_collector = Collector(policy, train_envs, buffer, exploration_noise=True)\n",
    "test_collector = Collector(policy, test_envs, exploration_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n/ep': 0,\n",
       " 'n/st': 320,\n",
       " 'rews': array([], dtype=float64),\n",
       " 'lens': array([], dtype=int64),\n",
       " 'idxs': array([], dtype=int64),\n",
       " 'rew': 0,\n",
       " 'len': 0,\n",
       " 'rew_std': 0,\n",
       " 'len_std': 0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "args.algo_name = \"dqn_icm\" if args.icm_lr_scale > 0 else \"dqn\"\n",
    "log_name = os.path.join(args.task, args.algo_name, str(args.seed), now)\n",
    "log_path = os.path.join(args.logdir, log_name)\n",
    "\n",
    "# logger\n",
    "if args.logger == \"wandb\":\n",
    "    logger = WandbLogger(\n",
    "        save_interval=1,\n",
    "        name=log_name.replace(os.path.sep, \"__\"),\n",
    "        run_id=args.resume_id,\n",
    "        config=args,\n",
    "        project=args.wandb_project,\n",
    "    )\n",
    "writer = SummaryWriter(log_path)\n",
    "writer.add_text(\"args\", str(args))\n",
    "if args.logger == \"tensorboard\":\n",
    "    logger = TensorboardLogger(writer)\n",
    "else:  # wandb\n",
    "    logger.load(writer)\n",
    "\n",
    "def save_best_fn(policy):\n",
    "    torch.save(policy.state_dict(), os.path.join(log_path, \"policy.pth\"))\n",
    "\n",
    "def stop_fn(mean_rewards: float) -> bool:\n",
    "    if env.spec.reward_threshold:\n",
    "        return mean_rewards >= env.spec.reward_threshold\n",
    "    if \"Pong\" in args.task:\n",
    "        return mean_rewards >= 20\n",
    "    return False\n",
    "\n",
    "def train_fn(epoch, env_step):\n",
    "    # nature DQN setting, linear decay in the first 1M steps\n",
    "    if env_step <= 1e6:\n",
    "        eps = args.eps_train - env_step / 1e6 * (args.eps_train - args.eps_train_final)\n",
    "    else:\n",
    "        eps = args.eps_train_final\n",
    "    policy.set_eps(eps)\n",
    "    if env_step % 1000 == 0:\n",
    "        logger.write(\"train/env_step\", env_step, {\"train/eps\": eps})\n",
    "\n",
    "def test_fn(epoch, env_step):\n",
    "    policy.set_eps(args.eps_test)\n",
    "\n",
    "def save_checkpoint_fn(epoch, env_step, gradient_step):\n",
    "    # see also: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "    ckpt_path = os.path.join(log_path, f\"checkpoint_{epoch}.pth\")\n",
    "    torch.save({\"model\": policy.state_dict()}, ckpt_path)\n",
    "    return ckpt_path\n",
    "\n",
    "# watch agent's performance\n",
    "def watch():\n",
    "    print(\"Setup test envs ...\")\n",
    "    policy.eval()\n",
    "    policy.set_eps(args.eps_test)\n",
    "    test_envs.seed(args.seed)\n",
    "    if args.save_buffer_name:\n",
    "        print(f\"Generate buffer with size {args.buffer_size}\")\n",
    "        buffer = VectorReplayBuffer(\n",
    "            args.buffer_size,\n",
    "            buffer_num=len(test_envs),\n",
    "            ignore_obs_next=True,\n",
    "            save_only_last_obs=True,\n",
    "            stack_num=args.frames_stack,\n",
    "        )\n",
    "        collector = Collector(policy, test_envs, buffer, exploration_noise=True)\n",
    "        result = collector.collect(n_step=args.buffer_size)\n",
    "        print(f\"Save buffer into {args.save_buffer_name}\")\n",
    "        # Unfortunately, pickle will cause oom with 1M buffer size\n",
    "        buffer.save_hdf5(args.save_buffer_name)\n",
    "    else:\n",
    "        print(\"Testing agent ...\")\n",
    "        test_collector.reset()\n",
    "        result = test_collector.collect(n_episode=args.test_num, render=args.render)\n",
    "    rew = result[\"rews\"].mean()\n",
    "    print(f\"Mean reward (over {result['n/ep']} episodes): {rew}\")\n",
    "\n",
    "if args.watch:\n",
    "    watch()\n",
    "    sys.exit(0)\n",
    "\n",
    "# test train_collector and start filling replay buffer\n",
    "train_collector.collect(n_step=args.batch_size * args.training_num, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [00:30, 332.17it/s, env_step=10000, len=0, loss=1.981, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #2: 10001it [00:30, 330.20it/s, env_step=20000, len=0, loss=2.857, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #3: 10001it [00:31, 318.49it/s, env_step=30000, len=0, loss=5.320, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #4: 10001it [00:31, 312.92it/s, env_step=40000, len=0, loss=6.081, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #5: 10001it [00:32, 311.81it/s, env_step=50000, len=0, loss=7.084, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #6: 10001it [00:32, 307.39it/s, env_step=60000, len=0, loss=8.278, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #7: 10001it [00:32, 305.59it/s, env_step=70000, len=0, loss=8.718, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #8: 10001it [00:33, 297.24it/s, env_step=80000, len=0, loss=9.715, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #9: 10001it [00:33, 298.69it/s, env_step=90000, len=0, loss=11.576, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #10: 10001it [00:34, 293.82it/s, env_step=100000, len=0, loss=14.252, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #11: 10001it [00:35, 283.39it/s, env_step=110000, len=0, loss=10.802, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #12: 10001it [00:35, 284.36it/s, env_step=120000, len=0, loss=12.354, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #13: 10001it [00:34, 286.86it/s, env_step=130000, len=0, loss=11.793, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #14: 10001it [00:35, 279.98it/s, env_step=140000, len=0, loss=10.012, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #15: 10001it [00:35, 282.71it/s, env_step=150000, len=0, loss=12.523, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #16: 10001it [00:36, 270.99it/s, env_step=160000, len=0, loss=14.187, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #17: 10001it [00:37, 266.64it/s, env_step=170000, len=0, loss=15.029, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #18: 10001it [00:36, 272.13it/s, env_step=180000, len=0, loss=11.744, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #19: 10001it [00:37, 269.26it/s, env_step=190000, len=0, loss=16.255, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #20: 10001it [00:39, 251.66it/s, env_step=200000, len=0, loss=13.513, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #21: 10001it [00:38, 260.13it/s, env_step=210000, len=0, loss=13.234, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #22: 10001it [00:37, 263.95it/s, env_step=220000, len=0, loss=11.536, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #23: 10001it [00:39, 255.40it/s, env_step=230000, len=0, loss=11.941, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #24: 10001it [00:41, 242.01it/s, env_step=240000, len=0, loss=10.399, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #25: 10001it [00:39, 254.69it/s, env_step=250000, len=0, loss=9.916, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #26: 10001it [00:41, 243.48it/s, env_step=260000, len=0, loss=9.698, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #27: 10001it [00:39, 251.05it/s, env_step=270000, len=0, loss=8.457, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #28: 10001it [00:43, 229.42it/s, env_step=280000, len=0, loss=6.984, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #29: 10001it [00:42, 236.17it/s, env_step=290000, len=0, loss=8.774, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #30: 10001it [00:43, 231.69it/s, env_step=300000, len=0, loss=10.948, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #31: 10001it [00:40, 245.58it/s, env_step=310000, len=0, loss=10.329, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #32: 10001it [00:40, 249.13it/s, env_step=320000, len=0, loss=7.990, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #33: 10001it [00:41, 238.50it/s, env_step=330000, len=0, loss=8.376, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #34: 10001it [00:47, 212.67it/s, env_step=340000, len=0, loss=8.311, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #35: 10001it [00:43, 229.06it/s, env_step=350000, len=0, loss=6.742, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #36: 10001it [00:47, 210.49it/s, env_step=360000, len=0, loss=7.380, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #37: 10001it [00:43, 231.41it/s, env_step=370000, len=0, loss=10.701, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #38: 10001it [00:52, 190.65it/s, env_step=380000, len=0, loss=8.745, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #39: 10001it [00:43, 229.72it/s, env_step=390000, len=0, loss=8.955, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #40: 10001it [00:42, 233.14it/s, env_step=400000, len=0, loss=8.240, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #41: 10001it [00:48, 204.30it/s, env_step=410000, len=0, loss=5.363, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #42: 10001it [00:45, 219.24it/s, env_step=420000, len=0, loss=8.538, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #43: 10001it [00:50, 197.27it/s, env_step=430000, len=0, loss=6.921, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #44: 10001it [00:46, 216.92it/s, env_step=440000, len=0, loss=7.313, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #45: 10001it [00:50, 197.04it/s, env_step=450000, len=0, loss=5.126, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #46: 10001it [00:45, 220.48it/s, env_step=460000, len=0, loss=5.155, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #47: 10001it [00:47, 210.28it/s, env_step=470000, len=0, loss=5.061, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #48: 10001it [00:47, 211.69it/s, env_step=480000, len=0, loss=4.557, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #49: 10001it [00:45, 219.61it/s, env_step=490000, len=0, loss=3.481, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #50: 10001it [00:49, 201.93it/s, env_step=500000, len=0, loss=3.567, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #51: 10001it [00:45, 220.52it/s, env_step=510000, len=0, loss=2.614, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #52: 10001it [00:48, 204.74it/s, env_step=520000, len=0, loss=3.413, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #53: 10001it [00:50, 199.08it/s, env_step=530000, len=0, loss=4.504, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #54: 10001it [01:06, 151.12it/s, env_step=540000, len=0, loss=4.246, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #55: 10001it [00:52, 191.19it/s, env_step=550000, len=0, loss=3.700, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #56: 10001it [00:47, 212.66it/s, env_step=560000, len=0, loss=3.608, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #57: 10001it [01:24, 118.71it/s, env_step=570000, len=0, loss=2.364, n/ep=0, n/st=10, rew=0.00]                          \n",
      "Epoch #58: 10001it [00:53, 187.11it/s, env_step=580000, len=0, loss=3.539, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #59: 10001it [00:49, 202.58it/s, env_step=590000, len=0, loss=3.302, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #60: 10001it [00:47, 209.04it/s, env_step=600000, len=0, loss=2.860, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #61: 10001it [01:26, 115.70it/s, env_step=610000, len=0, loss=2.595, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #62: 10001it [01:25, 116.75it/s, env_step=620000, len=0, loss=2.247, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #63: 10001it [01:27, 114.69it/s, env_step=630000, len=0, loss=2.694, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #64: 10001it [02:14, 74.36it/s, env_step=640000, len=0, loss=2.562, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #65: 10001it [00:57, 172.50it/s, env_step=650000, len=0, loss=2.238, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #66: 10001it [00:47, 209.83it/s, env_step=660000, len=0, loss=4.746, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #67: 10001it [01:46, 93.66it/s, env_step=670000, len=0, loss=2.281, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #68: 10001it [01:45, 94.76it/s, env_step=680000, len=0, loss=1.925, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #69: 10001it [02:51, 58.16it/s, env_step=690000, len=0, loss=2.566, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #70: 10001it [01:37, 103.05it/s, env_step=700000, len=0, loss=2.359, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #71: 10001it [02:04, 80.24it/s, env_step=710000, len=0, loss=1.926, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #72: 10001it [02:00, 83.26it/s, env_step=720000, len=0, loss=2.168, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #73: 10001it [04:43, 35.22it/s, env_step=730000, len=0, loss=2.722, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #74: 10001it [03:05, 53.80it/s, env_step=740000, len=0, loss=3.100, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #75: 10001it [02:54, 57.45it/s, env_step=750000, len=0, loss=3.642, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #76: 10001it [02:02, 81.91it/s, env_step=760000, len=0, loss=1.982, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #77: 10001it [01:19, 125.81it/s, env_step=770000, len=0, loss=2.357, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #78: 10001it [00:50, 198.05it/s, env_step=780000, len=0, loss=2.162, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #79: 10001it [01:46, 93.87it/s, env_step=790000, len=0, loss=3.604, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #80: 10001it [05:08, 32.40it/s, env_step=800000, len=0, loss=2.132, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #81: 10001it [03:05, 53.78it/s, env_step=810000, len=0, loss=2.070, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #82: 10001it [01:12, 137.31it/s, env_step=820000, len=0, loss=2.387, n/ep=0, n/st=10, rew=0.00]                          \n",
      "Epoch #83: 10001it [04:57, 33.60it/s, env_step=830000, len=0, loss=3.083, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #84: 10001it [00:49, 203.55it/s, env_step=840000, len=0, loss=2.457, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #85: 10001it [01:19, 125.15it/s, env_step=850000, len=0, loss=1.847, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #86: 10001it [00:51, 193.51it/s, env_step=860000, len=0, loss=2.040, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #87: 10001it [01:05, 152.58it/s, env_step=870000, len=0, loss=2.794, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #88: 10001it [01:12, 138.30it/s, env_step=880000, len=0, loss=1.705, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #89: 10001it [01:30, 110.22it/s, env_step=890000, len=0, loss=1.856, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #90: 10001it [01:15, 132.23it/s, env_step=900000, len=0, loss=2.192, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #91: 10001it [01:54, 87.10it/s, env_step=910000, len=0, loss=2.909, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #92: 10001it [05:59, 27.83it/s, env_step=920000, len=0, loss=2.957, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #93: 10001it [01:16, 131.00it/s, env_step=930000, len=0, loss=2.586, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #94: 10001it [01:11, 140.74it/s, env_step=940000, len=0, loss=1.775, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #95: 10001it [01:36, 103.46it/s, env_step=950000, len=0, loss=3.070, n/ep=0, n/st=10, rew=0.00]                          \n",
      "Epoch #96: 10001it [10:48, 15.43it/s, env_step=960000, len=0, loss=2.415, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #97: 10001it [04:37, 36.07it/s, env_step=970000, len=0, loss=4.935, n/ep=0, n/st=10, rew=0.00]                            \n",
      "Epoch #98: 10001it [00:56, 176.33it/s, env_step=980000, len=0, loss=3.470, n/ep=0, n/st=10, rew=0.00]                           \n",
      "Epoch #99: 10001it [01:51, 89.81it/s, env_step=990000, len=0, loss=3.591, n/ep=0, n/st=10, rew=0.00]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duration': '7985.64s', 'train_time/model': '533.92s', 'train_step': 990000, 'train_episode': 0, 'train_time/collector': '7451.73s', 'train_speed': '123.97 step/s'}\n"
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "result = OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    # test_collector=test_collector,\n",
    "    test_collector=None,\n",
    "    max_epoch=args.epoch,\n",
    "    step_per_epoch=args.step_per_epoch,\n",
    "    step_per_collect=args.step_per_collect,\n",
    "    episode_per_test=args.test_num,\n",
    "    batch_size=args.batch_size,\n",
    "    train_fn=train_fn,\n",
    "    test_fn=test_fn,\n",
    "    stop_fn=stop_fn,\n",
    "    save_best_fn=save_best_fn,\n",
    "    logger=logger,\n",
    "    update_per_step=args.update_per_step,\n",
    "    test_in_train=False,\n",
    "    resume_from_log=args.resume_id is not None,\n",
    "    save_checkpoint_fn=save_checkpoint_fn,\n",
    ").run()\n",
    "\n",
    "print(result)\n",
    "# watch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianshou",
   "language": "python",
   "name": "tianshou"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
